{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0736dbf-54c6-4475-a833-a3043414be16",
   "metadata": {},
   "source": [
    "# 1. Neural Graph Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04b823-c673-4699-b05a-77dce606c5e8",
   "metadata": {},
   "source": [
    "## 1.1. NGCF class construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "764a7171-38d9-4ba7-825a-b46e5bdf327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, args):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.device = args.device\n",
    "        self.emb_size = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.node_dropout = args.node_dropout[0]\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.norm_adj = norm_adj\n",
    "\n",
    "        self.layers = eval(args.layer_size)\n",
    "        self.decay = eval(args.regs)[0]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Init the weight of user-item.\n",
    "        \"\"\"\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Get sparse adj.\n",
    "        \"\"\"\n",
    "        self.sparse_norm_adj = self._convert_sp_mat_to_sp_tensor(self.norm_adj).to(self.device)\n",
    "\n",
    "    def init_weight(self):\n",
    "        # xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layers\n",
    "        for k in range(len(self.layers)):\n",
    "            weight_dict.update({'W_gc_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_gc_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "            weight_dict.update({'W_bi_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_bi_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "\n",
    "    def sparse_dropout(self, x, rate, noise_shape):\n",
    "        random_tensor = 1 - rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "\n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "\n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        # cul regularizer\n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = self.decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss, mf_loss, emb_loss\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items, drop_flag=True):\n",
    "\n",
    "        A_hat = self.sparse_dropout(self.sparse_norm_adj,\n",
    "                                    self.node_dropout,\n",
    "                                    self.sparse_norm_adj._nnz()) if drop_flag else self.sparse_norm_adj\n",
    "\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'],\n",
    "                                    self.embedding_dict['item_emb']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(len(self.layers)):\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) \\\n",
    "                                             + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            # element-wise product\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) \\\n",
    "                                            + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "\n",
    "            # message dropout.\n",
    "            ego_embeddings = nn.Dropout(self.mess_dropout[k])(ego_embeddings)\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        look up.\n",
    "        \"\"\"\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103f64f-1fb6-4d17-8b61-5394d18ad257",
   "metadata": {},
   "source": [
    "## 1.2. Data object construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17a63c87-f02a-43bb-a1c3-fa468e373a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_train += len(items)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_test += len(items)\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        with open(train_file) as f_train:\n",
    "            with open(test_file) as f_test:\n",
    "                for l in f_train.readlines():\n",
    "                    if len(l) == 0:\n",
    "                        break\n",
    "                    l = l.strip('\\n')\n",
    "                    items = [int(i) for i in l.split(' ')]\n",
    "                    uid, train_items = items[0], items[1:]\n",
    "\n",
    "                    for i in train_items:\n",
    "                        self.R[uid, i] = 1.\n",
    "                        # self.R[uid][i] = 1\n",
    "\n",
    "                    self.train_items[uid] = train_items\n",
    "\n",
    "                for l in f_test.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    uid, test_items = items[0], items[1:]\n",
    "                    self.test_set[uid] = test_items\n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def mean_adj_single(adj):\n",
    "            # D^-1 * A\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            # D^-1/2 * A * D^-1/2\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        # norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = mean_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # sample num pos items for u-th user\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # sample num neg items for u-th user\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_set.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_items[uid]\n",
    "            test_iids = self.test_set[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "        return split_uids, split_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e47d1-4f07-4e53-9964-61bfee427ef6",
   "metadata": {},
   "source": [
    "## 1.4. Metrics: \n",
    "\n",
    "precision_at_k, average_precision, mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "200e6638-ea12-488b-9e98-34f9bfda3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k, ground_truth, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "\n",
    "        Low but correct defination\n",
    "    \"\"\"\n",
    "    GT = set(ground_truth)\n",
    "    if len(GT) > k :\n",
    "        sent_list = [1.0] * k\n",
    "    else:\n",
    "        sent_list = [1.0]*len(GT) + [0.0]*(k-len(GT))\n",
    "    dcg_max = dcg_at_k(sent_list, k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    # if all_pos_num == 0:\n",
    "    #     return 0\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def AUC(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a57b3-c6c5-47bc-8f12-2aa5f9dee887",
   "metadata": {},
   "source": [
    "## 1.5. Change to NGCF folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "780aab82-59fd-453b-9db9-9505bde4e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/NGCF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getcwd(), 'NGCF'))  # Set the path to NGCF directory if needed.\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ea1d8-5aee-42b9-918a-88691559f459",
   "metadata": {},
   "source": [
    "## 1.6. Run the main.py code\n",
    "\n",
    "Open Visual Studio/Pycharm to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1848405-0d11-48f1-9425-434e3c21eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('python main.py --dataset gowalla --regs [1e-5] --embed_size 64 --layer_size [64,64,64] --lr 0.0001 --save_flag 1 --pretrain 0 --batch_size 1024 --epoch 400 --verbose 1 --node_dropout [0.1] --mess_dropout [0.1,0.1,0.1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e4c59",
   "metadata": {},
   "source": [
    "# 2. KGAT: Knowledge Graph Attention Network for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7a8c1",
   "metadata": {},
   "source": [
    "## 2.1. Aggregator and KGAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0616170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# L2 regularization loss, calculates the mean of squared values for a tensor\n",
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)\n",
    "\n",
    "# Aggregator class for message-passing in GNN\n",
    "class Aggregator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        # Initialize dimensions, dropout, and type of aggregator\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        # Dropout for message-passing and activation function\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Initialize linear transformations based on aggregator type\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # GCN type: single linear layer\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # GraphSAGE type: concatenates input and neighbor embeddings\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Bi-interaction type: uses two linear transformations\n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # Forward pass for aggregator\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ego_embeddings: user and entity embeddings\n",
    "        A_in: adjacency matrix as a sparse tensor\n",
    "        \"\"\"\n",
    "        # Neighbor aggregation\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            sum_embeddings = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embeddings = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings) \n",
    "        return embeddings\n",
    "\n",
    "# Knowledge Graph Attention Network (KGAT) model\n",
    "class KGAT(nn.Module):\n",
    "    \n",
    "    def __init__(self, args, n_users, n_entities, n_relations, A_in=None, user_pre_embed=None, item_pre_embed=None):\n",
    "        super(KGAT, self).__init__()\n",
    "        \n",
    "        # Initialize user and entity embeddings, relation embeddings\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.user_entity_embed = nn.Embedding(n_users + n_entities, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(n_relations, self.embed_dim)\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "\n",
    "        # Initialize hyperparameters and structure settings\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "        self.relation_dim = args.relation_dim\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(eval(args.conv_dim_list))\n",
    "        self.kg_l2loss_lambda = args.kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = args.cf_l2loss_lambda\n",
    "\n",
    "        # Initialize transformation matrices and embeddings\n",
    "        self.entity_user_embed = nn.Embedding(self.n_entities + self.n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        # Pretrain embedding weights if available\n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        # Aggregator layers for multi-layer GNN\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type))\n",
    "\n",
    "        # Initialize sparse adjacency matrix\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    # Calculate collaborative filtering embeddings\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        all_embed = torch.cat(all_embed, dim=1)\n",
    "        return all_embed\n",
    "\n",
    "    # Calculate collaborative filtering loss\n",
    "    def calc_cf_loss(self, user_ids, item_pos_ids, item_neg_ids):\n",
    "        all_embed = self.calc_cf_embeddings()\n",
    "        user_embed = all_embed[user_ids]\n",
    "        item_pos_embed = all_embed[item_pos_ids]\n",
    "        item_neg_embed = all_embed[item_neg_ids]\n",
    "\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)\n",
    "\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    # Calculate knowledge graph loss\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        r_embed = self.relation_embed(r)\n",
    "        W_r = self.trans_M[r]\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)\n",
    "\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    # Update attention scores for entities and relations\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "    # Calculate attention for the entire graph\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows, cols, values = [], [], []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "    # Calculate scores for user-item pairs\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        all_embed = self.calc_cf_embeddings()\n",
    "        user_embed = all_embed[user_ids]\n",
    "        item_embed = all_embed[item_ids]\n",
    "\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))\n",
    "        return cf_score\n",
    "\n",
    "    # Main forward function with different modes\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'train_cf':\n",
    "            return self.calc_cf_loss(*input)\n",
    "        if mode == 'train_kg':\n",
    "            return self.calc_kg_loss(*input)\n",
    "        if mode == 'update_att':\n",
    "            return self.update_attention(*input)\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e19cb6",
   "metadata": {},
   "source": [
    "## 2.2. Log_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82152e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Function to create a unique log ID by incrementing a counter\n",
    "def create_log_id(dir_path):\n",
    "    # Initialize the log count at 0\n",
    "    log_count = 0\n",
    "    # Create a file path with 'log0.log', 'log1.log', etc., until a unique file name is found\n",
    "    file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment log_count and generate a new file path\n",
    "        log_count += 1\n",
    "        file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    return log_count  # Return the unique log ID\n",
    "\n",
    "# Function to configure logging settings\n",
    "def logging_config(folder=None, name=None,\n",
    "                   level=logging.DEBUG,\n",
    "                   console_level=logging.DEBUG,\n",
    "                   no_console=True):\n",
    "\n",
    "    # Create the log folder if it does not exist\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # Clear any existing handlers from the root logger\n",
    "    for handler in logging.root.handlers:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.root.handlers = []  # Reset handlers to avoid duplicate logs\n",
    "\n",
    "    # Set the log file path using the provided folder and log name\n",
    "    logpath = os.path.join(folder, name + \".log\")\n",
    "    print(\"All logs will be saved to %s\" % logpath)\n",
    "\n",
    "    # Set the logging level for the root logger\n",
    "    logging.root.setLevel(level)\n",
    "\n",
    "    # Define the log format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a file handler for logging to a file\n",
    "    logfile = logging.FileHandler(logpath)\n",
    "    logfile.setLevel(level)           # Set the level for the file handler\n",
    "    logfile.setFormatter(formatter)   # Apply the format to the file handler\n",
    "    logging.root.addHandler(logfile)  # Add the file handler to the root logger\n",
    "\n",
    "    # Optionally add a console handler to also log to the console\n",
    "    if not no_console:\n",
    "        logconsole = logging.StreamHandler()  # Create a stream handler for console output\n",
    "        logconsole.setLevel(console_level)    # Set the level for console logging\n",
    "        logconsole.setFormatter(formatter)    # Apply the format to the console handler\n",
    "        logging.root.addHandler(logconsole)   # Add the console handler to the root logger\n",
    "\n",
    "    return folder  # Return the log folder path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8e804",
   "metadata": {},
   "source": [
    "## 2.3. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a65bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "\n",
    "# Function to calculate recall for a single example at a given cutoff k\n",
    "def calc_recall(rank, ground_truth, k):\n",
    "    \"\"\"\n",
    "    rank: list of predicted ranked items\n",
    "    ground_truth: list of true items\n",
    "    k: top-k items to consider\n",
    "    \"\"\"\n",
    "    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "# Function to calculate precision at k for a single user\n",
    "def precision_at_k(hit, k):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating whether top-k items were relevant (1) or not (0)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)[:k]  # Take the first k elements\n",
    "    return np.mean(hit)  # Compute mean precision\n",
    "\n",
    "# Function to calculate precision at k for a batch of users\n",
    "def precision_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array with rows representing users, columns binary (0 / 1) indicating relevance\n",
    "    \"\"\"\n",
    "    res = hits[:, :k].mean(axis=1)  # Calculate mean precision across users\n",
    "    return res\n",
    "\n",
    "# Function to calculate average precision\n",
    "def average_precision(hit, cut):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating relevant items\n",
    "    cut: maximum position to consider for precision\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)\n",
    "    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n",
    "\n",
    "# Function to calculate Discounted Cumulative Gain (DCG) at k\n",
    "def dcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    rel: list of relevance scores (binary or real) sorted by rank\n",
    "    \"\"\"\n",
    "    rel = np.asfarray(rel)[:k]\n",
    "    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))  # Compute DCG\n",
    "    return dcg\n",
    "\n",
    "# Function to calculate Normalized Discounted Cumulative Gain (NDCG) at k\n",
    "def ndcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    rel: list of relevance scores (binary or real)\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(rel, reverse=True), k)  # Ideal DCG for normalization\n",
    "    if not idcg:\n",
    "        return 0.0\n",
    "    return dcg_at_k(rel, k) / idcg  # NDCG is DCG divided by IDCG\n",
    "\n",
    "# Function to calculate NDCG at k for a batch of users\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array of binary values indicating relevance\n",
    "    \"\"\"\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)  # Compute DCG\n",
    "\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]  # Sort hits for ideal ranking\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    idcg[idcg == 0] = np.inf  # Handle cases where ideal DCG is zero\n",
    "    ndcg = (dcg / idcg)  # Compute NDCG\n",
    "    return ndcg\n",
    "\n",
    "# Function to calculate recall at k for a single user\n",
    "def recall_at_k(hit, k, all_pos_num):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating relevant items\n",
    "    all_pos_num: total number of relevant items\n",
    "    \"\"\"\n",
    "    hit = np.asfarray(hit)[:k]\n",
    "    return np.sum(hit) / all_pos_num  # Compute recall\n",
    "\n",
    "# Function to calculate recall at k for a batch of users\n",
    "def recall_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array of binary values indicating relevance\n",
    "    \"\"\"\n",
    "    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))  # Recall per user\n",
    "    return res\n",
    "\n",
    "# Function to calculate F1 score from precision and recall\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)  # F1 score formula\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate Area Under Curve (AUC) score\n",
    "def calc_auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)  # Compute AUC\n",
    "    except Exception:\n",
    "        res = 0.0  # Handle any exceptions\n",
    "    return res\n",
    "\n",
    "# Function to calculate log loss\n",
    "def logloss(ground_truth, prediction):\n",
    "    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))  # Compute log loss\n",
    "    return logloss\n",
    "\n",
    "# Function to calculate various metrics at different cutoff points (Ks) for collaborative filtering\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    \"\"\"\n",
    "    cf_scores: 2D array of scores for each user-item pair\n",
    "    train_user_dict: dictionary of training items for each user\n",
    "    test_user_dict: dictionary of test items for each user\n",
    "    user_ids: list of user IDs\n",
    "    item_ids: list of item IDs\n",
    "    Ks: list of top-K values for evaluation\n",
    "    \"\"\"\n",
    "    # Create a binary matrix indicating test items\n",
    "    test_pos_item_binary = np.zeros([len(user_ids), len(item_ids)], dtype=np.float32)\n",
    "    for idx, u in enumerate(user_ids):\n",
    "        train_pos_item_list = train_user_dict[u]\n",
    "        test_pos_item_list = test_user_dict[u]\n",
    "        cf_scores[idx][train_pos_item_list] = -np.inf  # Exclude training items from ranking\n",
    "        test_pos_item_binary[idx][test_pos_item_list] = 1  # Mark test items\n",
    "\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.cuda(), descending=True)  # Sort in descending order on GPU\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)  # Fallback to CPU sorting\n",
    "    rank_indices = rank_indices.cpu()  # Move result back to CPU\n",
    "\n",
    "    # Generate binary hit matrix based on sorted indices\n",
    "    binary_hit = []\n",
    "    for i in range(len(user_ids)):\n",
    "        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "    binary_hit = np.array(binary_hit, dtype=np.float32)\n",
    "\n",
    "    # Calculate precision, recall, and NDCG metrics for each k in Ks\n",
    "    metrics_dict = {}\n",
    "    for k in Ks:\n",
    "        metrics_dict[k] = {}\n",
    "        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['recall'] = recall_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['ndcg'] = ndcg_at_k_batch(binary_hit, k)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54465d",
   "metadata": {},
   "source": [
    "## 2.4. Model_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae0d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "# Function to determine early stopping based on recall scores\n",
    "def early_stopping(recall_list, stopping_steps):\n",
    "    # Find the maximum recall and its corresponding step\n",
    "    best_recall = max(recall_list)\n",
    "    best_step = recall_list.index(best_recall)\n",
    "    \n",
    "    # Check if the number of steps since the best step exceeds the stopping criterion\n",
    "    if len(recall_list) - best_step - 1 >= stopping_steps:\n",
    "        should_stop = True  # Set flag to stop training\n",
    "    else:\n",
    "        should_stop = False  # Continue training\n",
    "    return best_recall, should_stop  # Return best recall and stopping decision\n",
    "\n",
    "# Function to save model checkpoint\n",
    "def save_model(model, model_dir, current_epoch, last_best_epoch=None):\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    # Define the file path for saving the model at the current epoch\n",
    "    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n",
    "    # Save the model state dictionary and current epoch in a .pth file\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n",
    "\n",
    "    # Remove the previous best model file if applicable\n",
    "    if last_best_epoch is not None and current_epoch != last_best_epoch:\n",
    "        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n",
    "        # Delete the old model file if it exists\n",
    "        if os.path.exists(old_model_state_file):\n",
    "            os.system('rm {}'.format(old_model_state_file))  # Remove the file from the system\n",
    "\n",
    "# Function to load model checkpoint\n",
    "def load_model(model, model_path):\n",
    "    # Load the checkpoint from the specified path, mapping to CPU to ensure compatibility\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    # Load the saved model state dictionary into the model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Set the model to evaluation mode (for inference)\n",
    "    model.eval()\n",
    "    return model  # Return the loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c7b4",
   "metadata": {},
   "source": [
    "## 2.5. Loader_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Base class for loading and processing data for recommendation models\n",
    "class DataLoaderBase(object):\n",
    "\n",
    "    # Initialize the DataLoader with paths, data files, and configuration\n",
    "    def __init__(self, args, logging):\n",
    "        self.args = args\n",
    "        self.data_name = args.data_name\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "        self.pretrain_embedding_dir = args.pretrain_embedding_dir\n",
    "\n",
    "        # Set up data paths for training, testing, and knowledge graph files\n",
    "        self.data_dir = os.path.join(args.data_dir, args.data_name)\n",
    "        self.train_file = os.path.join(self.data_dir, 'train.txt')\n",
    "        self.test_file = os.path.join(self.data_dir, 'test.txt')\n",
    "        self.kg_file = os.path.join(self.data_dir, \"kg_final.txt\")\n",
    "\n",
    "        # Load collaborative filtering (CF) data and user-item interactions\n",
    "        self.cf_train_data, self.train_user_dict = self.load_cf(self.train_file)\n",
    "        self.cf_test_data, self.test_user_dict = self.load_cf(self.test_file)\n",
    "        self.statistic_cf()  # Calculate statistics on users and items\n",
    "\n",
    "        # Load pre-trained embeddings if required\n",
    "        if self.use_pretrain == 1:\n",
    "            self.load_pretrained_data()\n",
    "\n",
    "    # Load collaborative filtering data from a file\n",
    "    def load_cf(self, filename):\n",
    "        user = []\n",
    "        item = []\n",
    "        user_dict = dict()\n",
    "\n",
    "        # Read each line from the file representing user-item interactions\n",
    "        lines = open(filename, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmp = l.strip()\n",
    "            inter = [int(i) for i in tmp.split()]  # Convert to integers\n",
    "\n",
    "            if len(inter) > 1:\n",
    "                user_id, item_ids = inter[0], inter[1:]  # Extract user and item IDs\n",
    "                item_ids = list(set(item_ids))  # Remove duplicate items\n",
    "\n",
    "                # Append items and populate the user dictionary\n",
    "                for item_id in item_ids:\n",
    "                    user.append(user_id)\n",
    "                    item.append(item_id)\n",
    "                user_dict[user_id] = item_ids\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        user = np.array(user, dtype=np.int32)\n",
    "        item = np.array(item, dtype=np.int32)\n",
    "        return (user, item), user_dict\n",
    "\n",
    "    # Calculate statistics on the number of users, items, and interactions\n",
    "    def statistic_cf(self):\n",
    "        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n",
    "        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n",
    "        self.n_cf_train = len(self.cf_train_data[0])\n",
    "        self.n_cf_test = len(self.cf_test_data[0])\n",
    "\n",
    "    # Load knowledge graph (KG) data\n",
    "    def load_kg(self, filename):\n",
    "        kg_data = pd.read_csv(filename, sep=' ', names=['h', 'r', 't'], engine='python')  # Load head-relation-tail data\n",
    "        kg_data = kg_data.drop_duplicates()  # Remove duplicate triples\n",
    "        return kg_data\n",
    "\n",
    "    # Sample positive items for a user\n",
    "    def sample_pos_items_for_u(self, user_dict, user_id, n_sample_pos_items):\n",
    "        pos_items = user_dict[user_id]  # Get the list of positive items\n",
    "        n_pos_items = len(pos_items)\n",
    "\n",
    "        sample_pos_items = []\n",
    "        while True:\n",
    "            if len(sample_pos_items) == n_sample_pos_items:\n",
    "                break\n",
    "\n",
    "            pos_item_idx = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_item_id = pos_items[pos_item_idx]\n",
    "            if pos_item_id not in sample_pos_items:\n",
    "                sample_pos_items.append(pos_item_id)\n",
    "        return sample_pos_items\n",
    "\n",
    "    # Sample negative items for a user\n",
    "    def sample_neg_items_for_u(self, user_dict, user_id, n_sample_neg_items):\n",
    "        pos_items = user_dict[user_id]  # Get the list of positive items\n",
    "\n",
    "        sample_neg_items = []\n",
    "        while True:\n",
    "            if len(sample_neg_items) == n_sample_neg_items:\n",
    "                break\n",
    "\n",
    "            neg_item_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "            if neg_item_id not in pos_items and neg_item_id not in sample_neg_items:\n",
    "                sample_neg_items.append(neg_item_id)\n",
    "        return sample_neg_items\n",
    "\n",
    "    # Generate a batch of user-item interactions for collaborative filtering\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        exist_users = list(user_dict.keys())  # List of existing users\n",
    "        if batch_size <= len(exist_users):\n",
    "            batch_user = random.sample(exist_users, batch_size)  # Randomly sample users\n",
    "        else:\n",
    "            batch_user = [random.choice(exist_users) for _ in range(batch_size)]  # Sample with replacement if needed\n",
    "\n",
    "        batch_pos_item, batch_neg_item = [], []\n",
    "        for u in batch_user:\n",
    "            batch_pos_item += self.sample_pos_items_for_u(user_dict, u, 1)\n",
    "            batch_neg_item += self.sample_neg_items_for_u(user_dict, u, 1)\n",
    "\n",
    "        # Convert lists to torch tensors\n",
    "        batch_user = torch.LongTensor(batch_user)\n",
    "        batch_pos_item = torch.LongTensor(batch_pos_item)\n",
    "        batch_neg_item = torch.LongTensor(batch_neg_item)\n",
    "        return batch_user, batch_pos_item, batch_neg_item\n",
    "\n",
    "    # Sample positive triples for a given head entity in the knowledge graph\n",
    "    def sample_pos_triples_for_h(self, kg_dict, head, n_sample_pos_triples):\n",
    "        pos_triples = kg_dict[head]  # Get list of positive triples for head\n",
    "        n_pos_triples = len(pos_triples)\n",
    "\n",
    "        sample_relations, sample_pos_tails = [], []\n",
    "        while True:\n",
    "            if len(sample_relations) == n_sample_pos_triples:\n",
    "                break\n",
    "\n",
    "            pos_triple_idx = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "            tail = pos_triples[pos_triple_idx][0]\n",
    "            relation = pos_triples[pos_triple_idx][1]\n",
    "\n",
    "            if relation not in sample_relations and tail not in sample_pos_tails:\n",
    "                sample_relations.append(relation)\n",
    "                sample_pos_tails.append(tail)\n",
    "        return sample_relations, sample_pos_tails\n",
    "\n",
    "    # Sample negative triples for a given head entity and relation in the knowledge graph\n",
    "    def sample_neg_triples_for_h(self, kg_dict, head, relation, n_sample_neg_triples, highest_neg_idx):\n",
    "        pos_triples = kg_dict[head]  # Get positive triples for head\n",
    "\n",
    "        sample_neg_tails = []\n",
    "        while True:\n",
    "            if len(sample_neg_tails) == n_sample_neg_triples:\n",
    "                break\n",
    "\n",
    "            tail = np.random.randint(low=0, high=highest_neg_idx, size=1)[0]\n",
    "            if (tail, relation) not in pos_triples and tail not in sample_neg_tails:\n",
    "                sample_neg_tails.append(tail)\n",
    "        return sample_neg_tails\n",
    "\n",
    "    # Generate a batch of triples for knowledge graph training\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, highest_neg_idx):\n",
    "        exist_heads = list(kg_dict.keys())  # List of existing heads in KG\n",
    "        if batch_size <= len(exist_heads):\n",
    "            batch_head = random.sample(exist_heads, batch_size)  # Randomly sample heads\n",
    "        else:\n",
    "            batch_head = [random.choice(exist_heads) for _ in range(batch_size)]  # Sample with replacement if needed\n",
    "\n",
    "        batch_relation, batch_pos_tail, batch_neg_tail = [], [], []\n",
    "        for h in batch_head:\n",
    "            relation, pos_tail = self.sample_pos_triples_for_h(kg_dict, h, 1)\n",
    "            batch_relation += relation\n",
    "            batch_pos_tail += pos_tail\n",
    "\n",
    "            neg_tail = self.sample_neg_triples_for_h(kg_dict, h, relation[0], 1, highest_neg_idx)\n",
    "            batch_neg_tail += neg_tail\n",
    "\n",
    "        # Convert lists to torch tensors\n",
    "        batch_head = torch.LongTensor(batch_head)\n",
    "        batch_relation = torch.LongTensor(batch_relation)\n",
    "        batch_pos_tail = torch.LongTensor(batch_pos_tail)\n",
    "        batch_neg_tail = torch.LongTensor(batch_neg_tail)\n",
    "        return batch_head, batch_relation, batch_pos_tail, batch_neg_tail\n",
    "\n",
    "    # Load pre-trained embeddings for users and items\n",
    "    def load_pretrained_data(self):\n",
    "        pre_model = 'mf'  # Specify model type for pre-trained embeddings\n",
    "        pretrain_path = '%s/%s/%s.npz' % (self.pretrain_embedding_dir, self.data_name, pre_model)  # Define path\n",
    "        pretrain_data = np.load(pretrain_path)  # Load pre-trained embeddings\n",
    "        self.user_pre_embed = pretrain_data['user_embed']\n",
    "        self.item_pre_embed = pretrain_data['item_embed']\n",
    "\n",
    "        # Validate embedding dimensions match the specified parameters\n",
    "        assert self.user_pre_embed.shape[0] == self.n_users\n",
    "        assert self.item_pre_embed.shape[0] == self.n_items\n",
    "        assert self.user_pre_embed.shape[1] == self.args.embed_dim\n",
    "        assert self.item_pre_embed.shape[1] == self.args.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efbc0",
   "metadata": {},
   "source": [
    "## 2.6. Loader_KGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e65a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# DataLoaderKGAT class, extending from DataLoaderBase for the KGAT model\n",
    "class DataLoaderKGAT(DataLoaderBase):\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self, args, logging):\n",
    "        super().__init__(args, logging)  # Initialize DataLoaderBase class\n",
    "\n",
    "        # Define number of users and entities based on training and test data\n",
    "        self.n_users = max(self.cf_train_data[0].max(), self.cf_test_data[0].max()) + 1\n",
    "        self.n_entities = max(self.cf_train_data[1].max(), self.cf_test_data[1].max()) + 1\n",
    "        # Set batch sizes for collaborative filtering, KG, and testing\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        # Load KG data and construct the required structures\n",
    "        kg_data = self.load_kg(self.kg_file)\n",
    "        self.construct_data(kg_data)\n",
    "        self.print_info(logging)  # Log information about data\n",
    "\n",
    "        # Set the laplacian type and initialize adjacency and laplacian dictionaries\n",
    "        self.laplacian_type = args.laplacian_type\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "\n",
    "    # Method to construct data structures from KG data\n",
    "    def construct_data(self, kg_data):\n",
    "        # Duplicate KG data and reverse the head-tail for inverse relationships\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations  # Assign new relation IDs for inverse relations\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # Offset relation IDs for user-item interactions\n",
    "        kg_data['r'] += 2\n",
    "        self.n_relations = max(kg_data['r']) + 1  # Calculate total number of relations\n",
    "        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1  # Update entity count\n",
    "        self.n_users_entities = self.n_users + self.n_entities  # Total count for users and entities\n",
    "\n",
    "        # Remap user IDs to ensure distinct user and entity spaces\n",
    "        self.cf_train_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_train_data[0]))).astype(np.int32), \n",
    "                              self.cf_train_data[1].astype(np.int32))\n",
    "        self.cf_test_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_test_data[0]))).astype(np.int32), \n",
    "                             self.cf_test_data[1].astype(np.int32))\n",
    "\n",
    "        # Offset user IDs in training and test dictionaries\n",
    "        self.train_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.train_user_dict.items()}\n",
    "        self.test_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.test_user_dict.items()}\n",
    "\n",
    "        # Create interaction data in KG format for training\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[1]\n",
    "\n",
    "        # Inverse interactions for undirected connections\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[0]\n",
    "\n",
    "        # Concatenate all KG and CF data into a single KG training set\n",
    "        self.kg_train_data = pd.concat([kg_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)  # Total KG training samples\n",
    "\n",
    "        # Construct dictionaries and lists for head, tail, and relation mapping\n",
    "        h_list, t_list, r_list = [], [], []\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        # Populate head, relation, and tail lists and dictionaries\n",
    "        for row in self.kg_train_data.iterrows():\n",
    "            h, r, t = row[1]\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        self.h_list = torch.LongTensor(h_list)\n",
    "        self.t_list = torch.LongTensor(t_list)\n",
    "        self.r_list = torch.LongTensor(r_list)\n",
    "\n",
    "    # Convert a sparse matrix in COO format to a sparse PyTorch tensor\n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data  # Non-zero values of COO matrix\n",
    "        indices = np.vstack((coo.row, coo.col))  # Row and column indices\n",
    "\n",
    "        i = torch.LongTensor(indices)  # Convert indices to PyTorch tensor\n",
    "        v = torch.FloatTensor(values)  # Convert values to PyTorch tensor\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    # Create adjacency dictionary for each relation type\n",
    "    def create_adjacency_dict(self):\n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)  # Use 1 for all adjacency values\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj  # Store adjacency matrix for each relation\n",
    "\n",
    "    # Create Laplacian matrices based on chosen normalization (symmetric or random walk)\n",
    "    def create_laplacian_dict(self):\n",
    "        \n",
    "        # Symmetric normalization of Laplacian matrix\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0  # Handle inf values\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)  # Symmetric normalization\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        # Random walk normalization of Laplacian matrix\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0  # Handle inf values\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)  # Random walk normalization\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        # Choose normalization function based on laplacian type\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Create Laplacian matrix for each relation\n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        # Aggregate all Laplacians and convert to PyTorch tensor\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())\n",
    "\n",
    "    # Log information about the dataset\n",
    "    def print_info(self, logging):\n",
    "        logging.info('n_users:           %d' % self.n_users)\n",
    "        logging.info('n_items:           %d' % self.n_items)\n",
    "        logging.info('n_entities:        %d' % self.n_entities)\n",
    "        logging.info('n_users_entities:  %d' % self.n_users_entities)\n",
    "        logging.info('n_relations:       %d' % self.n_relations)\n",
    "\n",
    "        logging.info('n_h_list:          %d' % len(self.h_list))\n",
    "        logging.info('n_t_list:          %d' % len(self.t_list))\n",
    "        logging.info('n_r_list:          %d' % len(self.r_list))\n",
    "\n",
    "        logging.info('n_cf_train:        %d' % self.n_cf_train)\n",
    "        logging.info('n_cf_test:         %d' % self.n_cf_test)\n",
    "\n",
    "        logging.info('n_kg_train:        %d' % self.n_kg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79abcb6",
   "metadata": {},
   "source": [
    "## 2.7. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef68e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_kgat_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2019,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='amazon-book',\n",
    "                        help='Choose a dataset from {yelp2018, last-fm, amazon-book}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='./Data',\n",
    "                        help='Input data path.')\n",
    "\n",
    "    parser.add_argument('--use_pretrain', type=int, default=0,\n",
    "                        help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "    parser.add_argument('--pretrain_embedding_dir', nargs='?', default='./Data/pretrain',\n",
    "                        help='Path of learned embeddings.')\n",
    "    parser.add_argument('--pretrain_model_path', nargs='?', default='./trained_model/model.pth',\n",
    "                        help='Path of stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=1024,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=2048,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=100000,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--embed_dim', type=int, default=64,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=64,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--laplacian_type', type=str, default='random-walk',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {symmetric, random-walk}.')\n",
    "    parser.add_argument('--aggregation_type', type=str, default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=1,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=1,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=3,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=3,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=3,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[20, 40, 60, 80, 100]',\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    save_dir = 'trained_model/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        args.data_name, args.embed_dim, args.relation_dim, args.laplacian_type, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr, args.use_pretrain)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b36f9",
   "metadata": {},
   "source": [
    "## 2.8. Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b855f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 13:41:46,331 - root - INFO - Namespace(seed=2019, data_name='amazon-book', data_dir='./Data', use_pretrain=0, pretrain_embedding_dir='./Data/pretrain', pretrain_model_path='./trained_model/model.pth', cf_batch_size=1024, kg_batch_size=2048, test_batch_size=100000, embed_dim=64, relation_dim=64, laplacian_type='random-walk', aggregation_type='bi-interaction', conv_dim_list='[64, 32, 16]', mess_dropout='[0.1, 0.1, 0.1]', kg_l2loss_lambda=1e-05, cf_l2loss_lambda=1e-05, lr=0.0001, n_epoch=1, stopping_steps=1, cf_print_every=3, kg_print_every=3, evaluate_every=3, Ks='[20, 40, 60, 80, 100]', save_dir='trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All logs will be saved to trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/log3.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 13:43:46,256 - root - INFO - n_users:           52643\n",
      "2025-11-10 13:43:46,257 - root - INFO - n_items:           91599\n",
      "2025-11-10 13:43:46,257 - root - INFO - n_entities:        113487\n",
      "2025-11-10 13:43:46,257 - root - INFO - n_users_entities:  166130\n",
      "2025-11-10 13:43:46,258 - root - INFO - n_relations:       80\n",
      "2025-11-10 13:43:46,268 - root - INFO - n_h_list:          9876952\n",
      "2025-11-10 13:43:46,269 - root - INFO - n_t_list:          9876952\n",
      "2025-11-10 13:43:46,269 - root - INFO - n_r_list:          9876952\n",
      "2025-11-10 13:43:46,270 - root - INFO - n_cf_train:        2380730\n",
      "2025-11-10 13:43:46,270 - root - INFO - n_cf_test:         603378\n",
      "2025-11-10 13:43:46,270 - root - INFO - n_kg_train:        9876952\n",
      "/tmp/ipykernel_3822165/3746279008.py:132: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_3822165/3746279008.py:102: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
      "/tmp/ipykernel_3822165/2028869963.py:114: UserWarning: torch.sparse.SparseTensor(shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:680.)\n",
      "  self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
      "2025-11-10 13:43:51,310 - root - INFO - KGAT(\n",
      "  (user_entity_embed): Embedding(166130, 64)\n",
      "  (relation_embed): Embedding(80, 64)\n",
      "  (entity_user_embed): Embedding(166130, 64)\n",
      "  (aggregator_layers): ModuleList(\n",
      "    (0): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "    (2): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-11-10 13:43:53,978 - root - INFO - CF Training: Epoch 0001 Iter 0003 / 2325 | Time 0.1s | Iter Loss 0.7015 | Iter Mean Loss 0.6976\n",
      "2025-11-10 13:43:54,173 - root - INFO - CF Training: Epoch 0001 Iter 0006 / 2325 | Time 0.1s | Iter Loss 0.6975 | Iter Mean Loss 0.6976\n",
      "2025-11-10 13:43:54,378 - root - INFO - CF Training: Epoch 0001 Iter 0009 / 2325 | Time 0.1s | Iter Loss 0.6991 | Iter Mean Loss 0.6983\n",
      "2025-11-10 13:43:54,571 - root - INFO - CF Training: Epoch 0001 Iter 0012 / 2325 | Time 0.1s | Iter Loss 0.6996 | Iter Mean Loss 0.6980\n",
      "2025-11-10 13:43:54,765 - root - INFO - CF Training: Epoch 0001 Iter 0015 / 2325 | Time 0.1s | Iter Loss 0.6974 | Iter Mean Loss 0.6982\n",
      "2025-11-10 13:43:54,961 - root - INFO - CF Training: Epoch 0001 Iter 0018 / 2325 | Time 0.1s | Iter Loss 0.6958 | Iter Mean Loss 0.6981\n",
      "2025-11-10 13:43:55,157 - root - INFO - CF Training: Epoch 0001 Iter 0021 / 2325 | Time 0.1s | Iter Loss 0.6995 | Iter Mean Loss 0.6980\n",
      "2025-11-10 13:43:55,351 - root - INFO - CF Training: Epoch 0001 Iter 0024 / 2325 | Time 0.1s | Iter Loss 0.6957 | Iter Mean Loss 0.6977\n",
      "2025-11-10 13:43:55,545 - root - INFO - CF Training: Epoch 0001 Iter 0027 / 2325 | Time 0.1s | Iter Loss 0.6962 | Iter Mean Loss 0.6974\n",
      "2025-11-10 13:43:55,739 - root - INFO - CF Training: Epoch 0001 Iter 0030 / 2325 | Time 0.1s | Iter Loss 0.6987 | Iter Mean Loss 0.6973\n",
      "2025-11-10 13:43:55,932 - root - INFO - CF Training: Epoch 0001 Iter 0033 / 2325 | Time 0.1s | Iter Loss 0.6964 | Iter Mean Loss 0.6972\n",
      "2025-11-10 13:43:56,126 - root - INFO - CF Training: Epoch 0001 Iter 0036 / 2325 | Time 0.1s | Iter Loss 0.6951 | Iter Mean Loss 0.6970\n",
      "2025-11-10 13:43:56,320 - root - INFO - CF Training: Epoch 0001 Iter 0039 / 2325 | Time 0.1s | Iter Loss 0.6960 | Iter Mean Loss 0.6970\n",
      "2025-11-10 13:43:56,513 - root - INFO - CF Training: Epoch 0001 Iter 0042 / 2325 | Time 0.1s | Iter Loss 0.6975 | Iter Mean Loss 0.6969\n",
      "2025-11-10 13:43:56,707 - root - INFO - CF Training: Epoch 0001 Iter 0045 / 2325 | Time 0.1s | Iter Loss 0.6954 | Iter Mean Loss 0.6969\n",
      "2025-11-10 13:43:56,901 - root - INFO - CF Training: Epoch 0001 Iter 0048 / 2325 | Time 0.1s | Iter Loss 0.6970 | Iter Mean Loss 0.6968\n",
      "2025-11-10 13:43:57,095 - root - INFO - CF Training: Epoch 0001 Iter 0051 / 2325 | Time 0.1s | Iter Loss 0.6982 | Iter Mean Loss 0.6969\n",
      "2025-11-10 13:43:57,288 - root - INFO - CF Training: Epoch 0001 Iter 0054 / 2325 | Time 0.1s | Iter Loss 0.6940 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:57,482 - root - INFO - CF Training: Epoch 0001 Iter 0057 / 2325 | Time 0.1s | Iter Loss 0.6963 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:57,675 - root - INFO - CF Training: Epoch 0001 Iter 0060 / 2325 | Time 0.1s | Iter Loss 0.6967 | Iter Mean Loss 0.6966\n",
      "2025-11-10 13:43:57,870 - root - INFO - CF Training: Epoch 0001 Iter 0063 / 2325 | Time 0.1s | Iter Loss 0.6982 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:58,065 - root - INFO - CF Training: Epoch 0001 Iter 0066 / 2325 | Time 0.1s | Iter Loss 0.6972 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:58,259 - root - INFO - CF Training: Epoch 0001 Iter 0069 / 2325 | Time 0.1s | Iter Loss 0.6975 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:58,452 - root - INFO - CF Training: Epoch 0001 Iter 0072 / 2325 | Time 0.1s | Iter Loss 0.6956 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:58,647 - root - INFO - CF Training: Epoch 0001 Iter 0075 / 2325 | Time 0.1s | Iter Loss 0.6948 | Iter Mean Loss 0.6967\n",
      "2025-11-10 13:43:58,844 - root - INFO - CF Training: Epoch 0001 Iter 0078 / 2325 | Time 0.1s | Iter Loss 0.6990 | Iter Mean Loss 0.6966\n",
      "2025-11-10 13:43:59,038 - root - INFO - CF Training: Epoch 0001 Iter 0081 / 2325 | Time 0.1s | Iter Loss 0.6957 | Iter Mean Loss 0.6966\n",
      "2025-11-10 13:43:59,234 - root - INFO - CF Training: Epoch 0001 Iter 0084 / 2325 | Time 0.1s | Iter Loss 0.6965 | Iter Mean Loss 0.6966\n",
      "2025-11-10 13:43:59,431 - root - INFO - CF Training: Epoch 0001 Iter 0087 / 2325 | Time 0.1s | Iter Loss 0.6948 | Iter Mean Loss 0.6965\n",
      "2025-11-10 13:43:59,624 - root - INFO - CF Training: Epoch 0001 Iter 0090 / 2325 | Time 0.1s | Iter Loss 0.6944 | Iter Mean Loss 0.6965\n",
      "2025-11-10 13:43:59,818 - root - INFO - CF Training: Epoch 0001 Iter 0093 / 2325 | Time 0.1s | Iter Loss 0.6928 | Iter Mean Loss 0.6964\n",
      "2025-11-10 13:44:00,013 - root - INFO - CF Training: Epoch 0001 Iter 0096 / 2325 | Time 0.1s | Iter Loss 0.6951 | Iter Mean Loss 0.6963\n",
      "2025-11-10 13:44:00,207 - root - INFO - CF Training: Epoch 0001 Iter 0099 / 2325 | Time 0.1s | Iter Loss 0.6951 | Iter Mean Loss 0.6963\n",
      "2025-11-10 13:44:00,403 - root - INFO - CF Training: Epoch 0001 Iter 0102 / 2325 | Time 0.1s | Iter Loss 0.6942 | Iter Mean Loss 0.6962\n",
      "2025-11-10 13:44:00,597 - root - INFO - CF Training: Epoch 0001 Iter 0105 / 2325 | Time 0.1s | Iter Loss 0.6943 | Iter Mean Loss 0.6962\n",
      "2025-11-10 13:44:00,791 - root - INFO - CF Training: Epoch 0001 Iter 0108 / 2325 | Time 0.1s | Iter Loss 0.6888 | Iter Mean Loss 0.6961\n",
      "2025-11-10 13:44:00,985 - root - INFO - CF Training: Epoch 0001 Iter 0111 / 2325 | Time 0.1s | Iter Loss 0.6960 | Iter Mean Loss 0.6960\n",
      "2025-11-10 13:44:01,179 - root - INFO - CF Training: Epoch 0001 Iter 0114 / 2325 | Time 0.1s | Iter Loss 0.6888 | Iter Mean Loss 0.6959\n",
      "2025-11-10 13:44:01,372 - root - INFO - CF Training: Epoch 0001 Iter 0117 / 2325 | Time 0.1s | Iter Loss 0.6951 | Iter Mean Loss 0.6958\n",
      "2025-11-10 13:44:01,566 - root - INFO - CF Training: Epoch 0001 Iter 0120 / 2325 | Time 0.1s | Iter Loss 0.6905 | Iter Mean Loss 0.6957\n",
      "2025-11-10 13:44:01,759 - root - INFO - CF Training: Epoch 0001 Iter 0123 / 2325 | Time 0.1s | Iter Loss 0.6918 | Iter Mean Loss 0.6956\n",
      "2025-11-10 13:44:01,953 - root - INFO - CF Training: Epoch 0001 Iter 0126 / 2325 | Time 0.1s | Iter Loss 0.6933 | Iter Mean Loss 0.6955\n",
      "2025-11-10 13:44:02,147 - root - INFO - CF Training: Epoch 0001 Iter 0129 / 2325 | Time 0.1s | Iter Loss 0.6929 | Iter Mean Loss 0.6955\n",
      "2025-11-10 13:44:02,340 - root - INFO - CF Training: Epoch 0001 Iter 0132 / 2325 | Time 0.1s | Iter Loss 0.6921 | Iter Mean Loss 0.6954\n",
      "2025-11-10 13:44:02,534 - root - INFO - CF Training: Epoch 0001 Iter 0135 / 2325 | Time 0.1s | Iter Loss 0.6930 | Iter Mean Loss 0.6953\n",
      "2025-11-10 13:44:02,728 - root - INFO - CF Training: Epoch 0001 Iter 0138 / 2325 | Time 0.1s | Iter Loss 0.6910 | Iter Mean Loss 0.6952\n",
      "2025-11-10 13:44:02,923 - root - INFO - CF Training: Epoch 0001 Iter 0141 / 2325 | Time 0.1s | Iter Loss 0.6892 | Iter Mean Loss 0.6951\n",
      "2025-11-10 13:44:03,117 - root - INFO - CF Training: Epoch 0001 Iter 0144 / 2325 | Time 0.1s | Iter Loss 0.6910 | Iter Mean Loss 0.6950\n",
      "2025-11-10 13:44:03,310 - root - INFO - CF Training: Epoch 0001 Iter 0147 / 2325 | Time 0.1s | Iter Loss 0.6925 | Iter Mean Loss 0.6949\n",
      "2025-11-10 13:44:03,504 - root - INFO - CF Training: Epoch 0001 Iter 0150 / 2325 | Time 0.1s | Iter Loss 0.6892 | Iter Mean Loss 0.6948\n",
      "2025-11-10 13:44:03,697 - root - INFO - CF Training: Epoch 0001 Iter 0153 / 2325 | Time 0.1s | Iter Loss 0.6908 | Iter Mean Loss 0.6947\n",
      "2025-11-10 13:44:03,891 - root - INFO - CF Training: Epoch 0001 Iter 0156 / 2325 | Time 0.1s | Iter Loss 0.6895 | Iter Mean Loss 0.6946\n",
      "2025-11-10 13:44:04,087 - root - INFO - CF Training: Epoch 0001 Iter 0159 / 2325 | Time 0.1s | Iter Loss 0.6889 | Iter Mean Loss 0.6945\n",
      "2025-11-10 13:44:04,281 - root - INFO - CF Training: Epoch 0001 Iter 0162 / 2325 | Time 0.1s | Iter Loss 0.6876 | Iter Mean Loss 0.6943\n",
      "2025-11-10 13:44:04,475 - root - INFO - CF Training: Epoch 0001 Iter 0165 / 2325 | Time 0.1s | Iter Loss 0.6902 | Iter Mean Loss 0.6942\n",
      "2025-11-10 13:44:04,671 - root - INFO - CF Training: Epoch 0001 Iter 0168 / 2325 | Time 0.1s | Iter Loss 0.6841 | Iter Mean Loss 0.6940\n",
      "2025-11-10 13:44:04,864 - root - INFO - CF Training: Epoch 0001 Iter 0171 / 2325 | Time 0.1s | Iter Loss 0.6883 | Iter Mean Loss 0.6939\n",
      "2025-11-10 13:44:05,060 - root - INFO - CF Training: Epoch 0001 Iter 0174 / 2325 | Time 0.1s | Iter Loss 0.6873 | Iter Mean Loss 0.6938\n",
      "2025-11-10 13:44:05,253 - root - INFO - CF Training: Epoch 0001 Iter 0177 / 2325 | Time 0.1s | Iter Loss 0.6862 | Iter Mean Loss 0.6936\n",
      "2025-11-10 13:44:05,447 - root - INFO - CF Training: Epoch 0001 Iter 0180 / 2325 | Time 0.1s | Iter Loss 0.6864 | Iter Mean Loss 0.6935\n",
      "2025-11-10 13:44:05,642 - root - INFO - CF Training: Epoch 0001 Iter 0183 / 2325 | Time 0.1s | Iter Loss 0.6885 | Iter Mean Loss 0.6934\n",
      "2025-11-10 13:44:05,837 - root - INFO - CF Training: Epoch 0001 Iter 0186 / 2325 | Time 0.1s | Iter Loss 0.6856 | Iter Mean Loss 0.6933\n",
      "2025-11-10 13:44:06,031 - root - INFO - CF Training: Epoch 0001 Iter 0189 / 2325 | Time 0.1s | Iter Loss 0.6819 | Iter Mean Loss 0.6932\n",
      "2025-11-10 13:44:06,226 - root - INFO - CF Training: Epoch 0001 Iter 0192 / 2325 | Time 0.1s | Iter Loss 0.6818 | Iter Mean Loss 0.6931\n",
      "2025-11-10 13:44:06,420 - root - INFO - CF Training: Epoch 0001 Iter 0195 / 2325 | Time 0.1s | Iter Loss 0.6813 | Iter Mean Loss 0.6929\n",
      "2025-11-10 13:44:06,616 - root - INFO - CF Training: Epoch 0001 Iter 0198 / 2325 | Time 0.1s | Iter Loss 0.6798 | Iter Mean Loss 0.6928\n",
      "2025-11-10 13:44:06,811 - root - INFO - CF Training: Epoch 0001 Iter 0201 / 2325 | Time 0.1s | Iter Loss 0.6868 | Iter Mean Loss 0.6927\n",
      "2025-11-10 13:44:07,005 - root - INFO - CF Training: Epoch 0001 Iter 0204 / 2325 | Time 0.1s | Iter Loss 0.6812 | Iter Mean Loss 0.6925\n",
      "2025-11-10 13:44:07,199 - root - INFO - CF Training: Epoch 0001 Iter 0207 / 2325 | Time 0.1s | Iter Loss 0.6877 | Iter Mean Loss 0.6924\n",
      "2025-11-10 13:44:07,393 - root - INFO - CF Training: Epoch 0001 Iter 0210 / 2325 | Time 0.1s | Iter Loss 0.6809 | Iter Mean Loss 0.6921\n",
      "2025-11-10 13:44:07,588 - root - INFO - CF Training: Epoch 0001 Iter 0213 / 2325 | Time 0.1s | Iter Loss 0.6790 | Iter Mean Loss 0.6920\n",
      "2025-11-10 13:44:07,782 - root - INFO - CF Training: Epoch 0001 Iter 0216 / 2325 | Time 0.1s | Iter Loss 0.6786 | Iter Mean Loss 0.6918\n",
      "2025-11-10 13:44:07,979 - root - INFO - CF Training: Epoch 0001 Iter 0219 / 2325 | Time 0.1s | Iter Loss 0.6786 | Iter Mean Loss 0.6916\n",
      "2025-11-10 13:44:08,174 - root - INFO - CF Training: Epoch 0001 Iter 0222 / 2325 | Time 0.1s | Iter Loss 0.6765 | Iter Mean Loss 0.6914\n",
      "2025-11-10 13:44:08,367 - root - INFO - CF Training: Epoch 0001 Iter 0225 / 2325 | Time 0.1s | Iter Loss 0.6777 | Iter Mean Loss 0.6913\n",
      "2025-11-10 13:44:08,562 - root - INFO - CF Training: Epoch 0001 Iter 0228 / 2325 | Time 0.1s | Iter Loss 0.6695 | Iter Mean Loss 0.6911\n",
      "2025-11-10 13:44:08,755 - root - INFO - CF Training: Epoch 0001 Iter 0231 / 2325 | Time 0.1s | Iter Loss 0.6796 | Iter Mean Loss 0.6909\n",
      "2025-11-10 13:44:08,950 - root - INFO - CF Training: Epoch 0001 Iter 0234 / 2325 | Time 0.1s | Iter Loss 0.6757 | Iter Mean Loss 0.6907\n",
      "2025-11-10 13:44:09,144 - root - INFO - CF Training: Epoch 0001 Iter 0237 / 2325 | Time 0.1s | Iter Loss 0.6719 | Iter Mean Loss 0.6905\n",
      "2025-11-10 13:44:09,337 - root - INFO - CF Training: Epoch 0001 Iter 0240 / 2325 | Time 0.1s | Iter Loss 0.6735 | Iter Mean Loss 0.6902\n",
      "2025-11-10 13:44:09,531 - root - INFO - CF Training: Epoch 0001 Iter 0243 / 2325 | Time 0.1s | Iter Loss 0.6735 | Iter Mean Loss 0.6900\n",
      "2025-11-10 13:44:09,726 - root - INFO - CF Training: Epoch 0001 Iter 0246 / 2325 | Time 0.1s | Iter Loss 0.6716 | Iter Mean Loss 0.6898\n",
      "2025-11-10 13:44:09,919 - root - INFO - CF Training: Epoch 0001 Iter 0249 / 2325 | Time 0.1s | Iter Loss 0.6686 | Iter Mean Loss 0.6895\n",
      "2025-11-10 13:44:10,113 - root - INFO - CF Training: Epoch 0001 Iter 0252 / 2325 | Time 0.1s | Iter Loss 0.6669 | Iter Mean Loss 0.6893\n",
      "2025-11-10 13:44:10,308 - root - INFO - CF Training: Epoch 0001 Iter 0255 / 2325 | Time 0.1s | Iter Loss 0.6677 | Iter Mean Loss 0.6890\n",
      "2025-11-10 13:44:10,502 - root - INFO - CF Training: Epoch 0001 Iter 0258 / 2325 | Time 0.1s | Iter Loss 0.6675 | Iter Mean Loss 0.6887\n",
      "2025-11-10 13:44:10,696 - root - INFO - CF Training: Epoch 0001 Iter 0261 / 2325 | Time 0.1s | Iter Loss 0.6664 | Iter Mean Loss 0.6884\n",
      "2025-11-10 13:44:10,890 - root - INFO - CF Training: Epoch 0001 Iter 0264 / 2325 | Time 0.1s | Iter Loss 0.6628 | Iter Mean Loss 0.6881\n",
      "2025-11-10 13:44:11,083 - root - INFO - CF Training: Epoch 0001 Iter 0267 / 2325 | Time 0.1s | Iter Loss 0.6564 | Iter Mean Loss 0.6878\n",
      "2025-11-10 13:44:11,278 - root - INFO - CF Training: Epoch 0001 Iter 0270 / 2325 | Time 0.1s | Iter Loss 0.6578 | Iter Mean Loss 0.6875\n",
      "2025-11-10 13:44:11,472 - root - INFO - CF Training: Epoch 0001 Iter 0273 / 2325 | Time 0.1s | Iter Loss 0.6641 | Iter Mean Loss 0.6872\n",
      "2025-11-10 13:44:11,668 - root - INFO - CF Training: Epoch 0001 Iter 0276 / 2325 | Time 0.1s | Iter Loss 0.6639 | Iter Mean Loss 0.6870\n",
      "2025-11-10 13:44:11,863 - root - INFO - CF Training: Epoch 0001 Iter 0279 / 2325 | Time 0.1s | Iter Loss 0.6571 | Iter Mean Loss 0.6867\n",
      "2025-11-10 13:44:12,057 - root - INFO - CF Training: Epoch 0001 Iter 0282 / 2325 | Time 0.1s | Iter Loss 0.6474 | Iter Mean Loss 0.6863\n",
      "2025-11-10 13:44:12,251 - root - INFO - CF Training: Epoch 0001 Iter 0285 / 2325 | Time 0.1s | Iter Loss 0.6429 | Iter Mean Loss 0.6860\n",
      "2025-11-10 13:44:12,446 - root - INFO - CF Training: Epoch 0001 Iter 0288 / 2325 | Time 0.1s | Iter Loss 0.6543 | Iter Mean Loss 0.6857\n",
      "2025-11-10 13:44:12,641 - root - INFO - CF Training: Epoch 0001 Iter 0291 / 2325 | Time 0.1s | Iter Loss 0.6429 | Iter Mean Loss 0.6853\n",
      "2025-11-10 13:44:12,837 - root - INFO - CF Training: Epoch 0001 Iter 0294 / 2325 | Time 0.1s | Iter Loss 0.6540 | Iter Mean Loss 0.6850\n",
      "2025-11-10 13:44:13,030 - root - INFO - CF Training: Epoch 0001 Iter 0297 / 2325 | Time 0.1s | Iter Loss 0.6625 | Iter Mean Loss 0.6847\n",
      "2025-11-10 13:44:13,224 - root - INFO - CF Training: Epoch 0001 Iter 0300 / 2325 | Time 0.1s | Iter Loss 0.6455 | Iter Mean Loss 0.6843\n",
      "2025-11-10 13:44:13,418 - root - INFO - CF Training: Epoch 0001 Iter 0303 / 2325 | Time 0.1s | Iter Loss 0.6582 | Iter Mean Loss 0.6840\n",
      "2025-11-10 13:44:13,614 - root - INFO - CF Training: Epoch 0001 Iter 0306 / 2325 | Time 0.1s | Iter Loss 0.6449 | Iter Mean Loss 0.6837\n",
      "2025-11-10 13:44:13,810 - root - INFO - CF Training: Epoch 0001 Iter 0309 / 2325 | Time 0.1s | Iter Loss 0.6472 | Iter Mean Loss 0.6833\n",
      "2025-11-10 13:44:14,004 - root - INFO - CF Training: Epoch 0001 Iter 0312 / 2325 | Time 0.1s | Iter Loss 0.6472 | Iter Mean Loss 0.6829\n",
      "2025-11-10 13:44:14,200 - root - INFO - CF Training: Epoch 0001 Iter 0315 / 2325 | Time 0.1s | Iter Loss 0.6455 | Iter Mean Loss 0.6826\n",
      "2025-11-10 13:44:14,395 - root - INFO - CF Training: Epoch 0001 Iter 0318 / 2325 | Time 0.1s | Iter Loss 0.6506 | Iter Mean Loss 0.6823\n",
      "2025-11-10 13:44:14,591 - root - INFO - CF Training: Epoch 0001 Iter 0321 / 2325 | Time 0.1s | Iter Loss 0.6504 | Iter Mean Loss 0.6819\n",
      "2025-11-10 13:44:14,786 - root - INFO - CF Training: Epoch 0001 Iter 0324 / 2325 | Time 0.1s | Iter Loss 0.6548 | Iter Mean Loss 0.6817\n",
      "2025-11-10 13:44:14,980 - root - INFO - CF Training: Epoch 0001 Iter 0327 / 2325 | Time 0.1s | Iter Loss 0.6461 | Iter Mean Loss 0.6813\n",
      "2025-11-10 13:44:15,174 - root - INFO - CF Training: Epoch 0001 Iter 0330 / 2325 | Time 0.1s | Iter Loss 0.6366 | Iter Mean Loss 0.6809\n",
      "2025-11-10 13:44:15,372 - root - INFO - CF Training: Epoch 0001 Iter 0333 / 2325 | Time 0.1s | Iter Loss 0.6372 | Iter Mean Loss 0.6805\n",
      "2025-11-10 13:44:15,566 - root - INFO - CF Training: Epoch 0001 Iter 0336 / 2325 | Time 0.1s | Iter Loss 0.6351 | Iter Mean Loss 0.6801\n",
      "2025-11-10 13:44:15,760 - root - INFO - CF Training: Epoch 0001 Iter 0339 / 2325 | Time 0.1s | Iter Loss 0.6388 | Iter Mean Loss 0.6797\n",
      "2025-11-10 13:44:15,953 - root - INFO - CF Training: Epoch 0001 Iter 0342 / 2325 | Time 0.1s | Iter Loss 0.6275 | Iter Mean Loss 0.6793\n",
      "2025-11-10 13:44:16,147 - root - INFO - CF Training: Epoch 0001 Iter 0345 / 2325 | Time 0.1s | Iter Loss 0.6251 | Iter Mean Loss 0.6788\n",
      "2025-11-10 13:44:16,341 - root - INFO - CF Training: Epoch 0001 Iter 0348 / 2325 | Time 0.1s | Iter Loss 0.6222 | Iter Mean Loss 0.6783\n",
      "2025-11-10 13:44:16,535 - root - INFO - CF Training: Epoch 0001 Iter 0351 / 2325 | Time 0.1s | Iter Loss 0.6348 | Iter Mean Loss 0.6779\n",
      "2025-11-10 13:44:16,728 - root - INFO - CF Training: Epoch 0001 Iter 0354 / 2325 | Time 0.1s | Iter Loss 0.6257 | Iter Mean Loss 0.6774\n",
      "2025-11-10 13:44:16,924 - root - INFO - CF Training: Epoch 0001 Iter 0357 / 2325 | Time 0.1s | Iter Loss 0.6112 | Iter Mean Loss 0.6769\n",
      "2025-11-10 13:44:17,120 - root - INFO - CF Training: Epoch 0001 Iter 0360 / 2325 | Time 0.1s | Iter Loss 0.6201 | Iter Mean Loss 0.6764\n",
      "2025-11-10 13:44:17,315 - root - INFO - CF Training: Epoch 0001 Iter 0363 / 2325 | Time 0.1s | Iter Loss 0.6168 | Iter Mean Loss 0.6759\n",
      "2025-11-10 13:44:17,509 - root - INFO - CF Training: Epoch 0001 Iter 0366 / 2325 | Time 0.1s | Iter Loss 0.6172 | Iter Mean Loss 0.6754\n",
      "2025-11-10 13:44:17,704 - root - INFO - CF Training: Epoch 0001 Iter 0369 / 2325 | Time 0.1s | Iter Loss 0.6157 | Iter Mean Loss 0.6750\n",
      "2025-11-10 13:44:17,900 - root - INFO - CF Training: Epoch 0001 Iter 0372 / 2325 | Time 0.1s | Iter Loss 0.6078 | Iter Mean Loss 0.6745\n",
      "2025-11-10 13:44:18,095 - root - INFO - CF Training: Epoch 0001 Iter 0375 / 2325 | Time 0.1s | Iter Loss 0.6061 | Iter Mean Loss 0.6740\n",
      "2025-11-10 13:44:18,290 - root - INFO - CF Training: Epoch 0001 Iter 0378 / 2325 | Time 0.1s | Iter Loss 0.6117 | Iter Mean Loss 0.6735\n",
      "2025-11-10 13:44:18,485 - root - INFO - CF Training: Epoch 0001 Iter 0381 / 2325 | Time 0.1s | Iter Loss 0.6051 | Iter Mean Loss 0.6730\n",
      "2025-11-10 13:44:18,679 - root - INFO - CF Training: Epoch 0001 Iter 0384 / 2325 | Time 0.1s | Iter Loss 0.5992 | Iter Mean Loss 0.6725\n",
      "2025-11-10 13:44:18,874 - root - INFO - CF Training: Epoch 0001 Iter 0387 / 2325 | Time 0.1s | Iter Loss 0.6072 | Iter Mean Loss 0.6720\n",
      "2025-11-10 13:44:19,069 - root - INFO - CF Training: Epoch 0001 Iter 0390 / 2325 | Time 0.1s | Iter Loss 0.5956 | Iter Mean Loss 0.6714\n",
      "2025-11-10 13:44:19,264 - root - INFO - CF Training: Epoch 0001 Iter 0393 / 2325 | Time 0.1s | Iter Loss 0.5948 | Iter Mean Loss 0.6708\n",
      "2025-11-10 13:44:19,459 - root - INFO - CF Training: Epoch 0001 Iter 0396 / 2325 | Time 0.1s | Iter Loss 0.6068 | Iter Mean Loss 0.6703\n",
      "2025-11-10 13:44:19,652 - root - INFO - CF Training: Epoch 0001 Iter 0399 / 2325 | Time 0.1s | Iter Loss 0.5997 | Iter Mean Loss 0.6698\n",
      "2025-11-10 13:44:19,847 - root - INFO - CF Training: Epoch 0001 Iter 0402 / 2325 | Time 0.1s | Iter Loss 0.5879 | Iter Mean Loss 0.6692\n",
      "2025-11-10 13:44:20,044 - root - INFO - CF Training: Epoch 0001 Iter 0405 / 2325 | Time 0.1s | Iter Loss 0.5772 | Iter Mean Loss 0.6686\n",
      "2025-11-10 13:44:20,239 - root - INFO - CF Training: Epoch 0001 Iter 0408 / 2325 | Time 0.1s | Iter Loss 0.5888 | Iter Mean Loss 0.6680\n",
      "2025-11-10 13:44:20,434 - root - INFO - CF Training: Epoch 0001 Iter 0411 / 2325 | Time 0.1s | Iter Loss 0.5929 | Iter Mean Loss 0.6674\n",
      "2025-11-10 13:44:20,629 - root - INFO - CF Training: Epoch 0001 Iter 0414 / 2325 | Time 0.1s | Iter Loss 0.5766 | Iter Mean Loss 0.6667\n",
      "2025-11-10 13:44:20,823 - root - INFO - CF Training: Epoch 0001 Iter 0417 / 2325 | Time 0.1s | Iter Loss 0.5880 | Iter Mean Loss 0.6662\n",
      "2025-11-10 13:44:21,019 - root - INFO - CF Training: Epoch 0001 Iter 0420 / 2325 | Time 0.1s | Iter Loss 0.5680 | Iter Mean Loss 0.6656\n",
      "2025-11-10 13:44:21,213 - root - INFO - CF Training: Epoch 0001 Iter 0423 / 2325 | Time 0.1s | Iter Loss 0.5776 | Iter Mean Loss 0.6649\n",
      "2025-11-10 13:44:21,407 - root - INFO - CF Training: Epoch 0001 Iter 0426 / 2325 | Time 0.1s | Iter Loss 0.5712 | Iter Mean Loss 0.6642\n",
      "2025-11-10 13:44:21,602 - root - INFO - CF Training: Epoch 0001 Iter 0429 / 2325 | Time 0.1s | Iter Loss 0.5742 | Iter Mean Loss 0.6636\n",
      "2025-11-10 13:44:21,798 - root - INFO - CF Training: Epoch 0001 Iter 0432 / 2325 | Time 0.1s | Iter Loss 0.5663 | Iter Mean Loss 0.6630\n",
      "2025-11-10 13:44:21,992 - root - INFO - CF Training: Epoch 0001 Iter 0435 / 2325 | Time 0.1s | Iter Loss 0.5867 | Iter Mean Loss 0.6624\n",
      "2025-11-10 13:44:22,187 - root - INFO - CF Training: Epoch 0001 Iter 0438 / 2325 | Time 0.1s | Iter Loss 0.5686 | Iter Mean Loss 0.6619\n",
      "2025-11-10 13:44:22,381 - root - INFO - CF Training: Epoch 0001 Iter 0441 / 2325 | Time 0.1s | Iter Loss 0.5452 | Iter Mean Loss 0.6612\n",
      "2025-11-10 13:44:22,577 - root - INFO - CF Training: Epoch 0001 Iter 0444 / 2325 | Time 0.1s | Iter Loss 0.5684 | Iter Mean Loss 0.6606\n",
      "2025-11-10 13:44:22,772 - root - INFO - CF Training: Epoch 0001 Iter 0447 / 2325 | Time 0.1s | Iter Loss 0.5821 | Iter Mean Loss 0.6600\n",
      "2025-11-10 13:44:22,969 - root - INFO - CF Training: Epoch 0001 Iter 0450 / 2325 | Time 0.1s | Iter Loss 0.5627 | Iter Mean Loss 0.6593\n",
      "2025-11-10 13:44:23,164 - root - INFO - CF Training: Epoch 0001 Iter 0453 / 2325 | Time 0.1s | Iter Loss 0.5770 | Iter Mean Loss 0.6588\n",
      "2025-11-10 13:44:23,360 - root - INFO - CF Training: Epoch 0001 Iter 0456 / 2325 | Time 0.1s | Iter Loss 0.5664 | Iter Mean Loss 0.6582\n",
      "2025-11-10 13:44:23,554 - root - INFO - CF Training: Epoch 0001 Iter 0459 / 2325 | Time 0.1s | Iter Loss 0.5552 | Iter Mean Loss 0.6575\n",
      "2025-11-10 13:44:23,749 - root - INFO - CF Training: Epoch 0001 Iter 0462 / 2325 | Time 0.1s | Iter Loss 0.5600 | Iter Mean Loss 0.6569\n",
      "2025-11-10 13:44:23,946 - root - INFO - CF Training: Epoch 0001 Iter 0465 / 2325 | Time 0.1s | Iter Loss 0.5630 | Iter Mean Loss 0.6563\n",
      "2025-11-10 13:44:24,143 - root - INFO - CF Training: Epoch 0001 Iter 0468 / 2325 | Time 0.1s | Iter Loss 0.5818 | Iter Mean Loss 0.6557\n",
      "2025-11-10 13:44:24,338 - root - INFO - CF Training: Epoch 0001 Iter 0471 / 2325 | Time 0.1s | Iter Loss 0.5524 | Iter Mean Loss 0.6552\n",
      "2025-11-10 13:44:24,533 - root - INFO - CF Training: Epoch 0001 Iter 0474 / 2325 | Time 0.1s | Iter Loss 0.5540 | Iter Mean Loss 0.6545\n",
      "2025-11-10 13:44:24,727 - root - INFO - CF Training: Epoch 0001 Iter 0477 / 2325 | Time 0.1s | Iter Loss 0.5435 | Iter Mean Loss 0.6539\n",
      "2025-11-10 13:44:24,923 - root - INFO - CF Training: Epoch 0001 Iter 0480 / 2325 | Time 0.1s | Iter Loss 0.5511 | Iter Mean Loss 0.6533\n",
      "2025-11-10 13:44:25,118 - root - INFO - CF Training: Epoch 0001 Iter 0483 / 2325 | Time 0.1s | Iter Loss 0.5630 | Iter Mean Loss 0.6527\n",
      "2025-11-10 13:44:25,312 - root - INFO - CF Training: Epoch 0001 Iter 0486 / 2325 | Time 0.1s | Iter Loss 0.5577 | Iter Mean Loss 0.6522\n",
      "2025-11-10 13:44:25,507 - root - INFO - CF Training: Epoch 0001 Iter 0489 / 2325 | Time 0.1s | Iter Loss 0.5668 | Iter Mean Loss 0.6517\n",
      "2025-11-10 13:44:25,701 - root - INFO - CF Training: Epoch 0001 Iter 0492 / 2325 | Time 0.1s | Iter Loss 0.5511 | Iter Mean Loss 0.6510\n",
      "2025-11-10 13:44:25,897 - root - INFO - CF Training: Epoch 0001 Iter 0495 / 2325 | Time 0.1s | Iter Loss 0.5453 | Iter Mean Loss 0.6504\n",
      "2025-11-10 13:44:26,093 - root - INFO - CF Training: Epoch 0001 Iter 0498 / 2325 | Time 0.1s | Iter Loss 0.5546 | Iter Mean Loss 0.6498\n",
      "2025-11-10 13:44:26,288 - root - INFO - CF Training: Epoch 0001 Iter 0501 / 2325 | Time 0.1s | Iter Loss 0.5426 | Iter Mean Loss 0.6492\n",
      "2025-11-10 13:44:26,483 - root - INFO - CF Training: Epoch 0001 Iter 0504 / 2325 | Time 0.1s | Iter Loss 0.5279 | Iter Mean Loss 0.6486\n",
      "2025-11-10 13:44:26,678 - root - INFO - CF Training: Epoch 0001 Iter 0507 / 2325 | Time 0.1s | Iter Loss 0.5362 | Iter Mean Loss 0.6479\n",
      "2025-11-10 13:44:26,873 - root - INFO - CF Training: Epoch 0001 Iter 0510 / 2325 | Time 0.1s | Iter Loss 0.5647 | Iter Mean Loss 0.6474\n",
      "2025-11-10 13:44:27,070 - root - INFO - CF Training: Epoch 0001 Iter 0513 / 2325 | Time 0.1s | Iter Loss 0.5282 | Iter Mean Loss 0.6467\n",
      "2025-11-10 13:44:27,265 - root - INFO - CF Training: Epoch 0001 Iter 0516 / 2325 | Time 0.1s | Iter Loss 0.5377 | Iter Mean Loss 0.6461\n",
      "2025-11-10 13:44:27,460 - root - INFO - CF Training: Epoch 0001 Iter 0519 / 2325 | Time 0.1s | Iter Loss 0.5410 | Iter Mean Loss 0.6455\n",
      "2025-11-10 13:44:27,656 - root - INFO - CF Training: Epoch 0001 Iter 0522 / 2325 | Time 0.1s | Iter Loss 0.5384 | Iter Mean Loss 0.6449\n",
      "2025-11-10 13:44:27,851 - root - INFO - CF Training: Epoch 0001 Iter 0525 / 2325 | Time 0.1s | Iter Loss 0.5431 | Iter Mean Loss 0.6443\n",
      "2025-11-10 13:44:28,046 - root - INFO - CF Training: Epoch 0001 Iter 0528 / 2325 | Time 0.1s | Iter Loss 0.5131 | Iter Mean Loss 0.6436\n",
      "2025-11-10 13:44:28,240 - root - INFO - CF Training: Epoch 0001 Iter 0531 / 2325 | Time 0.1s | Iter Loss 0.5462 | Iter Mean Loss 0.6430\n",
      "2025-11-10 13:44:28,435 - root - INFO - CF Training: Epoch 0001 Iter 0534 / 2325 | Time 0.1s | Iter Loss 0.5262 | Iter Mean Loss 0.6425\n",
      "2025-11-10 13:44:28,631 - root - INFO - CF Training: Epoch 0001 Iter 0537 / 2325 | Time 0.1s | Iter Loss 0.5347 | Iter Mean Loss 0.6419\n",
      "2025-11-10 13:44:28,826 - root - INFO - CF Training: Epoch 0001 Iter 0540 / 2325 | Time 0.1s | Iter Loss 0.5347 | Iter Mean Loss 0.6413\n",
      "2025-11-10 13:44:29,021 - root - INFO - CF Training: Epoch 0001 Iter 0543 / 2325 | Time 0.1s | Iter Loss 0.5332 | Iter Mean Loss 0.6407\n",
      "2025-11-10 13:44:29,218 - root - INFO - CF Training: Epoch 0001 Iter 0546 / 2325 | Time 0.1s | Iter Loss 0.5480 | Iter Mean Loss 0.6402\n",
      "2025-11-10 13:44:29,412 - root - INFO - CF Training: Epoch 0001 Iter 0549 / 2325 | Time 0.1s | Iter Loss 0.5226 | Iter Mean Loss 0.6396\n",
      "2025-11-10 13:44:29,607 - root - INFO - CF Training: Epoch 0001 Iter 0552 / 2325 | Time 0.1s | Iter Loss 0.5409 | Iter Mean Loss 0.6390\n",
      "2025-11-10 13:44:29,802 - root - INFO - CF Training: Epoch 0001 Iter 0555 / 2325 | Time 0.1s | Iter Loss 0.5211 | Iter Mean Loss 0.6384\n",
      "2025-11-10 13:44:29,998 - root - INFO - CF Training: Epoch 0001 Iter 0558 / 2325 | Time 0.1s | Iter Loss 0.5138 | Iter Mean Loss 0.6378\n",
      "2025-11-10 13:44:30,194 - root - INFO - CF Training: Epoch 0001 Iter 0561 / 2325 | Time 0.1s | Iter Loss 0.5183 | Iter Mean Loss 0.6371\n",
      "2025-11-10 13:44:30,388 - root - INFO - CF Training: Epoch 0001 Iter 0564 / 2325 | Time 0.1s | Iter Loss 0.5299 | Iter Mean Loss 0.6365\n",
      "2025-11-10 13:44:30,585 - root - INFO - CF Training: Epoch 0001 Iter 0567 / 2325 | Time 0.1s | Iter Loss 0.5315 | Iter Mean Loss 0.6359\n",
      "2025-11-10 13:44:30,779 - root - INFO - CF Training: Epoch 0001 Iter 0570 / 2325 | Time 0.1s | Iter Loss 0.5198 | Iter Mean Loss 0.6353\n",
      "2025-11-10 13:44:30,975 - root - INFO - CF Training: Epoch 0001 Iter 0573 / 2325 | Time 0.1s | Iter Loss 0.5298 | Iter Mean Loss 0.6348\n",
      "2025-11-10 13:44:31,170 - root - INFO - CF Training: Epoch 0001 Iter 0576 / 2325 | Time 0.1s | Iter Loss 0.5203 | Iter Mean Loss 0.6341\n",
      "2025-11-10 13:44:31,367 - root - INFO - CF Training: Epoch 0001 Iter 0579 / 2325 | Time 0.1s | Iter Loss 0.5072 | Iter Mean Loss 0.6335\n",
      "2025-11-10 13:44:31,564 - root - INFO - CF Training: Epoch 0001 Iter 0582 / 2325 | Time 0.1s | Iter Loss 0.5127 | Iter Mean Loss 0.6329\n",
      "2025-11-10 13:44:31,758 - root - INFO - CF Training: Epoch 0001 Iter 0585 / 2325 | Time 0.1s | Iter Loss 0.5071 | Iter Mean Loss 0.6323\n",
      "2025-11-10 13:44:31,954 - root - INFO - CF Training: Epoch 0001 Iter 0588 / 2325 | Time 0.1s | Iter Loss 0.5091 | Iter Mean Loss 0.6317\n",
      "2025-11-10 13:44:32,149 - root - INFO - CF Training: Epoch 0001 Iter 0591 / 2325 | Time 0.1s | Iter Loss 0.5043 | Iter Mean Loss 0.6311\n",
      "2025-11-10 13:44:32,344 - root - INFO - CF Training: Epoch 0001 Iter 0594 / 2325 | Time 0.1s | Iter Loss 0.5080 | Iter Mean Loss 0.6306\n",
      "2025-11-10 13:44:32,540 - root - INFO - CF Training: Epoch 0001 Iter 0597 / 2325 | Time 0.1s | Iter Loss 0.5232 | Iter Mean Loss 0.6300\n",
      "2025-11-10 13:44:32,735 - root - INFO - CF Training: Epoch 0001 Iter 0600 / 2325 | Time 0.1s | Iter Loss 0.5069 | Iter Mean Loss 0.6294\n",
      "2025-11-10 13:44:32,930 - root - INFO - CF Training: Epoch 0001 Iter 0603 / 2325 | Time 0.1s | Iter Loss 0.5093 | Iter Mean Loss 0.6288\n",
      "2025-11-10 13:44:33,125 - root - INFO - CF Training: Epoch 0001 Iter 0606 / 2325 | Time 0.1s | Iter Loss 0.5163 | Iter Mean Loss 0.6282\n",
      "2025-11-10 13:44:33,319 - root - INFO - CF Training: Epoch 0001 Iter 0609 / 2325 | Time 0.1s | Iter Loss 0.5088 | Iter Mean Loss 0.6277\n",
      "2025-11-10 13:44:33,514 - root - INFO - CF Training: Epoch 0001 Iter 0612 / 2325 | Time 0.1s | Iter Loss 0.5088 | Iter Mean Loss 0.6271\n",
      "2025-11-10 13:44:33,709 - root - INFO - CF Training: Epoch 0001 Iter 0615 / 2325 | Time 0.1s | Iter Loss 0.4965 | Iter Mean Loss 0.6265\n",
      "2025-11-10 13:44:33,904 - root - INFO - CF Training: Epoch 0001 Iter 0618 / 2325 | Time 0.1s | Iter Loss 0.5118 | Iter Mean Loss 0.6260\n",
      "2025-11-10 13:44:34,099 - root - INFO - CF Training: Epoch 0001 Iter 0621 / 2325 | Time 0.1s | Iter Loss 0.4959 | Iter Mean Loss 0.6254\n",
      "2025-11-10 13:44:34,295 - root - INFO - CF Training: Epoch 0001 Iter 0624 / 2325 | Time 0.1s | Iter Loss 0.4965 | Iter Mean Loss 0.6248\n",
      "2025-11-10 13:44:34,489 - root - INFO - CF Training: Epoch 0001 Iter 0627 / 2325 | Time 0.1s | Iter Loss 0.4986 | Iter Mean Loss 0.6243\n",
      "2025-11-10 13:44:34,685 - root - INFO - CF Training: Epoch 0001 Iter 0630 / 2325 | Time 0.1s | Iter Loss 0.4945 | Iter Mean Loss 0.6237\n",
      "2025-11-10 13:44:34,879 - root - INFO - CF Training: Epoch 0001 Iter 0633 / 2325 | Time 0.1s | Iter Loss 0.5077 | Iter Mean Loss 0.6231\n",
      "2025-11-10 13:44:35,077 - root - INFO - CF Training: Epoch 0001 Iter 0636 / 2325 | Time 0.1s | Iter Loss 0.4975 | Iter Mean Loss 0.6225\n",
      "2025-11-10 13:44:35,272 - root - INFO - CF Training: Epoch 0001 Iter 0639 / 2325 | Time 0.1s | Iter Loss 0.4972 | Iter Mean Loss 0.6220\n",
      "2025-11-10 13:44:35,467 - root - INFO - CF Training: Epoch 0001 Iter 0642 / 2325 | Time 0.1s | Iter Loss 0.4677 | Iter Mean Loss 0.6213\n",
      "2025-11-10 13:44:35,661 - root - INFO - CF Training: Epoch 0001 Iter 0645 / 2325 | Time 0.1s | Iter Loss 0.4978 | Iter Mean Loss 0.6207\n",
      "2025-11-10 13:44:35,856 - root - INFO - CF Training: Epoch 0001 Iter 0648 / 2325 | Time 0.1s | Iter Loss 0.4959 | Iter Mean Loss 0.6202\n",
      "2025-11-10 13:44:36,050 - root - INFO - CF Training: Epoch 0001 Iter 0651 / 2325 | Time 0.1s | Iter Loss 0.4993 | Iter Mean Loss 0.6196\n",
      "2025-11-10 13:44:36,244 - root - INFO - CF Training: Epoch 0001 Iter 0654 / 2325 | Time 0.1s | Iter Loss 0.5046 | Iter Mean Loss 0.6191\n",
      "2025-11-10 13:44:36,439 - root - INFO - CF Training: Epoch 0001 Iter 0657 / 2325 | Time 0.1s | Iter Loss 0.5077 | Iter Mean Loss 0.6185\n",
      "2025-11-10 13:44:36,634 - root - INFO - CF Training: Epoch 0001 Iter 0660 / 2325 | Time 0.1s | Iter Loss 0.5037 | Iter Mean Loss 0.6180\n",
      "2025-11-10 13:44:36,830 - root - INFO - CF Training: Epoch 0001 Iter 0663 / 2325 | Time 0.1s | Iter Loss 0.4958 | Iter Mean Loss 0.6175\n",
      "2025-11-10 13:44:37,025 - root - INFO - CF Training: Epoch 0001 Iter 0666 / 2325 | Time 0.1s | Iter Loss 0.4927 | Iter Mean Loss 0.6169\n",
      "2025-11-10 13:44:37,222 - root - INFO - CF Training: Epoch 0001 Iter 0669 / 2325 | Time 0.1s | Iter Loss 0.5146 | Iter Mean Loss 0.6164\n",
      "2025-11-10 13:44:37,416 - root - INFO - CF Training: Epoch 0001 Iter 0672 / 2325 | Time 0.1s | Iter Loss 0.4892 | Iter Mean Loss 0.6158\n",
      "2025-11-10 13:44:37,611 - root - INFO - CF Training: Epoch 0001 Iter 0675 / 2325 | Time 0.1s | Iter Loss 0.4865 | Iter Mean Loss 0.6152\n",
      "2025-11-10 13:44:37,806 - root - INFO - CF Training: Epoch 0001 Iter 0678 / 2325 | Time 0.1s | Iter Loss 0.4732 | Iter Mean Loss 0.6146\n",
      "2025-11-10 13:44:38,000 - root - INFO - CF Training: Epoch 0001 Iter 0681 / 2325 | Time 0.1s | Iter Loss 0.4895 | Iter Mean Loss 0.6141\n",
      "2025-11-10 13:44:38,196 - root - INFO - CF Training: Epoch 0001 Iter 0684 / 2325 | Time 0.1s | Iter Loss 0.4753 | Iter Mean Loss 0.6135\n",
      "2025-11-10 13:44:38,393 - root - INFO - CF Training: Epoch 0001 Iter 0687 / 2325 | Time 0.1s | Iter Loss 0.4830 | Iter Mean Loss 0.6130\n",
      "2025-11-10 13:44:38,589 - root - INFO - CF Training: Epoch 0001 Iter 0690 / 2325 | Time 0.1s | Iter Loss 0.4909 | Iter Mean Loss 0.6124\n",
      "2025-11-10 13:44:38,786 - root - INFO - CF Training: Epoch 0001 Iter 0693 / 2325 | Time 0.1s | Iter Loss 0.5014 | Iter Mean Loss 0.6118\n",
      "2025-11-10 13:44:38,983 - root - INFO - CF Training: Epoch 0001 Iter 0696 / 2325 | Time 0.1s | Iter Loss 0.4778 | Iter Mean Loss 0.6113\n",
      "2025-11-10 13:44:39,181 - root - INFO - CF Training: Epoch 0001 Iter 0699 / 2325 | Time 0.1s | Iter Loss 0.4966 | Iter Mean Loss 0.6108\n",
      "2025-11-10 13:44:39,379 - root - INFO - CF Training: Epoch 0001 Iter 0702 / 2325 | Time 0.1s | Iter Loss 0.4855 | Iter Mean Loss 0.6102\n",
      "2025-11-10 13:44:39,574 - root - INFO - CF Training: Epoch 0001 Iter 0705 / 2325 | Time 0.1s | Iter Loss 0.4881 | Iter Mean Loss 0.6097\n",
      "2025-11-10 13:44:39,768 - root - INFO - CF Training: Epoch 0001 Iter 0708 / 2325 | Time 0.1s | Iter Loss 0.4858 | Iter Mean Loss 0.6092\n",
      "2025-11-10 13:44:39,964 - root - INFO - CF Training: Epoch 0001 Iter 0711 / 2325 | Time 0.1s | Iter Loss 0.4818 | Iter Mean Loss 0.6086\n",
      "2025-11-10 13:44:40,159 - root - INFO - CF Training: Epoch 0001 Iter 0714 / 2325 | Time 0.1s | Iter Loss 0.4570 | Iter Mean Loss 0.6081\n",
      "2025-11-10 13:44:40,355 - root - INFO - CF Training: Epoch 0001 Iter 0717 / 2325 | Time 0.1s | Iter Loss 0.4703 | Iter Mean Loss 0.6075\n",
      "2025-11-10 13:44:40,551 - root - INFO - CF Training: Epoch 0001 Iter 0720 / 2325 | Time 0.1s | Iter Loss 0.4698 | Iter Mean Loss 0.6070\n",
      "2025-11-10 13:44:40,748 - root - INFO - CF Training: Epoch 0001 Iter 0723 / 2325 | Time 0.1s | Iter Loss 0.4876 | Iter Mean Loss 0.6065\n",
      "2025-11-10 13:44:40,942 - root - INFO - CF Training: Epoch 0001 Iter 0726 / 2325 | Time 0.1s | Iter Loss 0.4833 | Iter Mean Loss 0.6059\n",
      "2025-11-10 13:44:41,137 - root - INFO - CF Training: Epoch 0001 Iter 0729 / 2325 | Time 0.1s | Iter Loss 0.4650 | Iter Mean Loss 0.6054\n",
      "2025-11-10 13:44:41,332 - root - INFO - CF Training: Epoch 0001 Iter 0732 / 2325 | Time 0.1s | Iter Loss 0.4640 | Iter Mean Loss 0.6048\n",
      "2025-11-10 13:44:41,527 - root - INFO - CF Training: Epoch 0001 Iter 0735 / 2325 | Time 0.1s | Iter Loss 0.4867 | Iter Mean Loss 0.6043\n",
      "2025-11-10 13:44:41,722 - root - INFO - CF Training: Epoch 0001 Iter 0738 / 2325 | Time 0.1s | Iter Loss 0.4652 | Iter Mean Loss 0.6038\n",
      "2025-11-10 13:44:41,916 - root - INFO - CF Training: Epoch 0001 Iter 0741 / 2325 | Time 0.1s | Iter Loss 0.4896 | Iter Mean Loss 0.6033\n",
      "2025-11-10 13:44:42,112 - root - INFO - CF Training: Epoch 0001 Iter 0744 / 2325 | Time 0.1s | Iter Loss 0.4874 | Iter Mean Loss 0.6028\n",
      "2025-11-10 13:44:42,306 - root - INFO - CF Training: Epoch 0001 Iter 0747 / 2325 | Time 0.1s | Iter Loss 0.4912 | Iter Mean Loss 0.6023\n",
      "2025-11-10 13:44:42,501 - root - INFO - CF Training: Epoch 0001 Iter 0750 / 2325 | Time 0.1s | Iter Loss 0.4578 | Iter Mean Loss 0.6018\n",
      "2025-11-10 13:44:42,696 - root - INFO - CF Training: Epoch 0001 Iter 0753 / 2325 | Time 0.1s | Iter Loss 0.4809 | Iter Mean Loss 0.6012\n",
      "2025-11-10 13:44:42,890 - root - INFO - CF Training: Epoch 0001 Iter 0756 / 2325 | Time 0.1s | Iter Loss 0.4855 | Iter Mean Loss 0.6008\n",
      "2025-11-10 13:44:43,085 - root - INFO - CF Training: Epoch 0001 Iter 0759 / 2325 | Time 0.1s | Iter Loss 0.4493 | Iter Mean Loss 0.6002\n",
      "2025-11-10 13:44:43,278 - root - INFO - CF Training: Epoch 0001 Iter 0762 / 2325 | Time 0.1s | Iter Loss 0.4700 | Iter Mean Loss 0.5997\n",
      "2025-11-10 13:44:43,474 - root - INFO - CF Training: Epoch 0001 Iter 0765 / 2325 | Time 0.1s | Iter Loss 0.4775 | Iter Mean Loss 0.5992\n",
      "2025-11-10 13:44:43,668 - root - INFO - CF Training: Epoch 0001 Iter 0768 / 2325 | Time 0.1s | Iter Loss 0.4774 | Iter Mean Loss 0.5987\n",
      "2025-11-10 13:44:43,864 - root - INFO - CF Training: Epoch 0001 Iter 0771 / 2325 | Time 0.1s | Iter Loss 0.4621 | Iter Mean Loss 0.5982\n",
      "2025-11-10 13:44:44,059 - root - INFO - CF Training: Epoch 0001 Iter 0774 / 2325 | Time 0.1s | Iter Loss 0.4533 | Iter Mean Loss 0.5977\n",
      "2025-11-10 13:44:44,254 - root - INFO - CF Training: Epoch 0001 Iter 0777 / 2325 | Time 0.1s | Iter Loss 0.4714 | Iter Mean Loss 0.5972\n",
      "2025-11-10 13:44:44,448 - root - INFO - CF Training: Epoch 0001 Iter 0780 / 2325 | Time 0.1s | Iter Loss 0.4639 | Iter Mean Loss 0.5967\n",
      "2025-11-10 13:44:44,645 - root - INFO - CF Training: Epoch 0001 Iter 0783 / 2325 | Time 0.1s | Iter Loss 0.4690 | Iter Mean Loss 0.5962\n",
      "2025-11-10 13:44:44,840 - root - INFO - CF Training: Epoch 0001 Iter 0786 / 2325 | Time 0.1s | Iter Loss 0.4653 | Iter Mean Loss 0.5957\n",
      "2025-11-10 13:44:45,035 - root - INFO - CF Training: Epoch 0001 Iter 0789 / 2325 | Time 0.1s | Iter Loss 0.4516 | Iter Mean Loss 0.5951\n",
      "2025-11-10 13:44:45,231 - root - INFO - CF Training: Epoch 0001 Iter 0792 / 2325 | Time 0.1s | Iter Loss 0.4445 | Iter Mean Loss 0.5946\n",
      "2025-11-10 13:44:45,426 - root - INFO - CF Training: Epoch 0001 Iter 0795 / 2325 | Time 0.1s | Iter Loss 0.4757 | Iter Mean Loss 0.5941\n",
      "2025-11-10 13:44:45,622 - root - INFO - CF Training: Epoch 0001 Iter 0798 / 2325 | Time 0.1s | Iter Loss 0.4724 | Iter Mean Loss 0.5937\n",
      "2025-11-10 13:44:45,817 - root - INFO - CF Training: Epoch 0001 Iter 0801 / 2325 | Time 0.1s | Iter Loss 0.4626 | Iter Mean Loss 0.5932\n",
      "2025-11-10 13:44:46,013 - root - INFO - CF Training: Epoch 0001 Iter 0804 / 2325 | Time 0.1s | Iter Loss 0.4389 | Iter Mean Loss 0.5926\n",
      "2025-11-10 13:44:46,208 - root - INFO - CF Training: Epoch 0001 Iter 0807 / 2325 | Time 0.1s | Iter Loss 0.4601 | Iter Mean Loss 0.5922\n",
      "2025-11-10 13:44:46,403 - root - INFO - CF Training: Epoch 0001 Iter 0810 / 2325 | Time 0.1s | Iter Loss 0.4791 | Iter Mean Loss 0.5917\n",
      "2025-11-10 13:44:46,599 - root - INFO - CF Training: Epoch 0001 Iter 0813 / 2325 | Time 0.1s | Iter Loss 0.4704 | Iter Mean Loss 0.5912\n",
      "2025-11-10 13:44:46,794 - root - INFO - CF Training: Epoch 0001 Iter 0816 / 2325 | Time 0.1s | Iter Loss 0.4452 | Iter Mean Loss 0.5907\n",
      "2025-11-10 13:44:46,988 - root - INFO - CF Training: Epoch 0001 Iter 0819 / 2325 | Time 0.1s | Iter Loss 0.4582 | Iter Mean Loss 0.5902\n",
      "2025-11-10 13:44:47,184 - root - INFO - CF Training: Epoch 0001 Iter 0822 / 2325 | Time 0.1s | Iter Loss 0.4580 | Iter Mean Loss 0.5897\n",
      "2025-11-10 13:44:47,379 - root - INFO - CF Training: Epoch 0001 Iter 0825 / 2325 | Time 0.1s | Iter Loss 0.4519 | Iter Mean Loss 0.5892\n",
      "2025-11-10 13:44:47,575 - root - INFO - CF Training: Epoch 0001 Iter 0828 / 2325 | Time 0.1s | Iter Loss 0.4691 | Iter Mean Loss 0.5888\n",
      "2025-11-10 13:44:47,770 - root - INFO - CF Training: Epoch 0001 Iter 0831 / 2325 | Time 0.1s | Iter Loss 0.4430 | Iter Mean Loss 0.5883\n",
      "2025-11-10 13:44:47,965 - root - INFO - CF Training: Epoch 0001 Iter 0834 / 2325 | Time 0.1s | Iter Loss 0.4355 | Iter Mean Loss 0.5878\n",
      "2025-11-10 13:44:48,160 - root - INFO - CF Training: Epoch 0001 Iter 0837 / 2325 | Time 0.1s | Iter Loss 0.4643 | Iter Mean Loss 0.5873\n",
      "2025-11-10 13:44:48,355 - root - INFO - CF Training: Epoch 0001 Iter 0840 / 2325 | Time 0.1s | Iter Loss 0.4692 | Iter Mean Loss 0.5869\n",
      "2025-11-10 13:44:48,550 - root - INFO - CF Training: Epoch 0001 Iter 0843 / 2325 | Time 0.1s | Iter Loss 0.4535 | Iter Mean Loss 0.5864\n",
      "2025-11-10 13:44:48,744 - root - INFO - CF Training: Epoch 0001 Iter 0846 / 2325 | Time 0.1s | Iter Loss 0.4478 | Iter Mean Loss 0.5859\n",
      "2025-11-10 13:44:48,940 - root - INFO - CF Training: Epoch 0001 Iter 0849 / 2325 | Time 0.1s | Iter Loss 0.4456 | Iter Mean Loss 0.5854\n",
      "2025-11-10 13:44:49,136 - root - INFO - CF Training: Epoch 0001 Iter 0852 / 2325 | Time 0.1s | Iter Loss 0.4480 | Iter Mean Loss 0.5850\n",
      "2025-11-10 13:44:49,334 - root - INFO - CF Training: Epoch 0001 Iter 0855 / 2325 | Time 0.1s | Iter Loss 0.4724 | Iter Mean Loss 0.5845\n",
      "2025-11-10 13:44:49,531 - root - INFO - CF Training: Epoch 0001 Iter 0858 / 2325 | Time 0.1s | Iter Loss 0.4654 | Iter Mean Loss 0.5841\n",
      "2025-11-10 13:44:49,726 - root - INFO - CF Training: Epoch 0001 Iter 0861 / 2325 | Time 0.1s | Iter Loss 0.4561 | Iter Mean Loss 0.5836\n",
      "2025-11-10 13:44:49,922 - root - INFO - CF Training: Epoch 0001 Iter 0864 / 2325 | Time 0.1s | Iter Loss 0.4536 | Iter Mean Loss 0.5832\n",
      "2025-11-10 13:44:50,117 - root - INFO - CF Training: Epoch 0001 Iter 0867 / 2325 | Time 0.1s | Iter Loss 0.4464 | Iter Mean Loss 0.5827\n",
      "2025-11-10 13:44:50,313 - root - INFO - CF Training: Epoch 0001 Iter 0870 / 2325 | Time 0.1s | Iter Loss 0.4508 | Iter Mean Loss 0.5822\n",
      "2025-11-10 13:44:50,508 - root - INFO - CF Training: Epoch 0001 Iter 0873 / 2325 | Time 0.1s | Iter Loss 0.4339 | Iter Mean Loss 0.5817\n",
      "2025-11-10 13:44:50,703 - root - INFO - CF Training: Epoch 0001 Iter 0876 / 2325 | Time 0.1s | Iter Loss 0.4477 | Iter Mean Loss 0.5813\n",
      "2025-11-10 13:44:50,901 - root - INFO - CF Training: Epoch 0001 Iter 0879 / 2325 | Time 0.1s | Iter Loss 0.4618 | Iter Mean Loss 0.5808\n",
      "2025-11-10 13:44:51,096 - root - INFO - CF Training: Epoch 0001 Iter 0882 / 2325 | Time 0.1s | Iter Loss 0.4340 | Iter Mean Loss 0.5803\n",
      "2025-11-10 13:44:51,291 - root - INFO - CF Training: Epoch 0001 Iter 0885 / 2325 | Time 0.1s | Iter Loss 0.4373 | Iter Mean Loss 0.5799\n",
      "2025-11-10 13:44:51,485 - root - INFO - CF Training: Epoch 0001 Iter 0888 / 2325 | Time 0.1s | Iter Loss 0.4290 | Iter Mean Loss 0.5794\n",
      "2025-11-10 13:44:51,683 - root - INFO - CF Training: Epoch 0001 Iter 0891 / 2325 | Time 0.1s | Iter Loss 0.4649 | Iter Mean Loss 0.5790\n",
      "2025-11-10 13:44:51,877 - root - INFO - CF Training: Epoch 0001 Iter 0894 / 2325 | Time 0.1s | Iter Loss 0.4557 | Iter Mean Loss 0.5786\n",
      "2025-11-10 13:44:52,072 - root - INFO - CF Training: Epoch 0001 Iter 0897 / 2325 | Time 0.1s | Iter Loss 0.4517 | Iter Mean Loss 0.5781\n",
      "2025-11-10 13:44:52,267 - root - INFO - CF Training: Epoch 0001 Iter 0900 / 2325 | Time 0.1s | Iter Loss 0.4419 | Iter Mean Loss 0.5776\n",
      "2025-11-10 13:44:52,462 - root - INFO - CF Training: Epoch 0001 Iter 0903 / 2325 | Time 0.1s | Iter Loss 0.4500 | Iter Mean Loss 0.5772\n",
      "2025-11-10 13:44:52,659 - root - INFO - CF Training: Epoch 0001 Iter 0906 / 2325 | Time 0.1s | Iter Loss 0.4400 | Iter Mean Loss 0.5768\n",
      "2025-11-10 13:44:52,856 - root - INFO - CF Training: Epoch 0001 Iter 0909 / 2325 | Time 0.1s | Iter Loss 0.4478 | Iter Mean Loss 0.5763\n",
      "2025-11-10 13:44:53,054 - root - INFO - CF Training: Epoch 0001 Iter 0912 / 2325 | Time 0.1s | Iter Loss 0.4443 | Iter Mean Loss 0.5759\n",
      "2025-11-10 13:44:53,251 - root - INFO - CF Training: Epoch 0001 Iter 0915 / 2325 | Time 0.1s | Iter Loss 0.4629 | Iter Mean Loss 0.5754\n",
      "2025-11-10 13:44:53,448 - root - INFO - CF Training: Epoch 0001 Iter 0918 / 2325 | Time 0.1s | Iter Loss 0.4511 | Iter Mean Loss 0.5750\n",
      "2025-11-10 13:44:53,644 - root - INFO - CF Training: Epoch 0001 Iter 0921 / 2325 | Time 0.1s | Iter Loss 0.4486 | Iter Mean Loss 0.5746\n",
      "2025-11-10 13:44:53,841 - root - INFO - CF Training: Epoch 0001 Iter 0924 / 2325 | Time 0.1s | Iter Loss 0.4352 | Iter Mean Loss 0.5741\n",
      "2025-11-10 13:44:54,039 - root - INFO - CF Training: Epoch 0001 Iter 0927 / 2325 | Time 0.1s | Iter Loss 0.4356 | Iter Mean Loss 0.5737\n",
      "2025-11-10 13:44:54,235 - root - INFO - CF Training: Epoch 0001 Iter 0930 / 2325 | Time 0.1s | Iter Loss 0.4374 | Iter Mean Loss 0.5733\n",
      "2025-11-10 13:44:54,430 - root - INFO - CF Training: Epoch 0001 Iter 0933 / 2325 | Time 0.1s | Iter Loss 0.4471 | Iter Mean Loss 0.5729\n",
      "2025-11-10 13:44:54,625 - root - INFO - CF Training: Epoch 0001 Iter 0936 / 2325 | Time 0.1s | Iter Loss 0.4522 | Iter Mean Loss 0.5725\n",
      "2025-11-10 13:44:54,820 - root - INFO - CF Training: Epoch 0001 Iter 0939 / 2325 | Time 0.1s | Iter Loss 0.4327 | Iter Mean Loss 0.5720\n",
      "2025-11-10 13:44:55,016 - root - INFO - CF Training: Epoch 0001 Iter 0942 / 2325 | Time 0.1s | Iter Loss 0.4362 | Iter Mean Loss 0.5716\n",
      "2025-11-10 13:44:55,210 - root - INFO - CF Training: Epoch 0001 Iter 0945 / 2325 | Time 0.1s | Iter Loss 0.4353 | Iter Mean Loss 0.5712\n",
      "2025-11-10 13:44:55,405 - root - INFO - CF Training: Epoch 0001 Iter 0948 / 2325 | Time 0.1s | Iter Loss 0.4462 | Iter Mean Loss 0.5707\n",
      "2025-11-10 13:44:55,601 - root - INFO - CF Training: Epoch 0001 Iter 0951 / 2325 | Time 0.1s | Iter Loss 0.4279 | Iter Mean Loss 0.5703\n",
      "2025-11-10 13:44:55,795 - root - INFO - CF Training: Epoch 0001 Iter 0954 / 2325 | Time 0.1s | Iter Loss 0.4284 | Iter Mean Loss 0.5699\n",
      "2025-11-10 13:44:55,990 - root - INFO - CF Training: Epoch 0001 Iter 0957 / 2325 | Time 0.1s | Iter Loss 0.4255 | Iter Mean Loss 0.5695\n",
      "2025-11-10 13:44:56,184 - root - INFO - CF Training: Epoch 0001 Iter 0960 / 2325 | Time 0.1s | Iter Loss 0.4373 | Iter Mean Loss 0.5690\n",
      "2025-11-10 13:44:56,381 - root - INFO - CF Training: Epoch 0001 Iter 0963 / 2325 | Time 0.1s | Iter Loss 0.4456 | Iter Mean Loss 0.5686\n",
      "2025-11-10 13:44:56,577 - root - INFO - CF Training: Epoch 0001 Iter 0966 / 2325 | Time 0.1s | Iter Loss 0.4232 | Iter Mean Loss 0.5682\n",
      "2025-11-10 13:44:56,771 - root - INFO - CF Training: Epoch 0001 Iter 0969 / 2325 | Time 0.1s | Iter Loss 0.4377 | Iter Mean Loss 0.5678\n",
      "2025-11-10 13:44:56,967 - root - INFO - CF Training: Epoch 0001 Iter 0972 / 2325 | Time 0.1s | Iter Loss 0.4456 | Iter Mean Loss 0.5674\n",
      "2025-11-10 13:44:57,163 - root - INFO - CF Training: Epoch 0001 Iter 0975 / 2325 | Time 0.1s | Iter Loss 0.4439 | Iter Mean Loss 0.5670\n",
      "2025-11-10 13:44:57,358 - root - INFO - CF Training: Epoch 0001 Iter 0978 / 2325 | Time 0.1s | Iter Loss 0.4250 | Iter Mean Loss 0.5666\n",
      "2025-11-10 13:44:57,552 - root - INFO - CF Training: Epoch 0001 Iter 0981 / 2325 | Time 0.1s | Iter Loss 0.4371 | Iter Mean Loss 0.5662\n",
      "2025-11-10 13:44:57,749 - root - INFO - CF Training: Epoch 0001 Iter 0984 / 2325 | Time 0.1s | Iter Loss 0.4429 | Iter Mean Loss 0.5658\n",
      "2025-11-10 13:44:57,943 - root - INFO - CF Training: Epoch 0001 Iter 0987 / 2325 | Time 0.1s | Iter Loss 0.4227 | Iter Mean Loss 0.5654\n",
      "2025-11-10 13:44:58,138 - root - INFO - CF Training: Epoch 0001 Iter 0990 / 2325 | Time 0.1s | Iter Loss 0.4434 | Iter Mean Loss 0.5650\n",
      "2025-11-10 13:44:58,332 - root - INFO - CF Training: Epoch 0001 Iter 0993 / 2325 | Time 0.1s | Iter Loss 0.4413 | Iter Mean Loss 0.5647\n",
      "2025-11-10 13:44:58,526 - root - INFO - CF Training: Epoch 0001 Iter 0996 / 2325 | Time 0.1s | Iter Loss 0.4435 | Iter Mean Loss 0.5643\n",
      "2025-11-10 13:44:58,721 - root - INFO - CF Training: Epoch 0001 Iter 0999 / 2325 | Time 0.1s | Iter Loss 0.4385 | Iter Mean Loss 0.5639\n",
      "2025-11-10 13:44:58,915 - root - INFO - CF Training: Epoch 0001 Iter 1002 / 2325 | Time 0.1s | Iter Loss 0.4344 | Iter Mean Loss 0.5635\n",
      "2025-11-10 13:44:59,115 - root - INFO - CF Training: Epoch 0001 Iter 1005 / 2325 | Time 0.1s | Iter Loss 0.4294 | Iter Mean Loss 0.5631\n",
      "2025-11-10 13:44:59,310 - root - INFO - CF Training: Epoch 0001 Iter 1008 / 2325 | Time 0.1s | Iter Loss 0.4506 | Iter Mean Loss 0.5627\n",
      "2025-11-10 13:44:59,505 - root - INFO - CF Training: Epoch 0001 Iter 1011 / 2325 | Time 0.1s | Iter Loss 0.4383 | Iter Mean Loss 0.5624\n",
      "2025-11-10 13:44:59,700 - root - INFO - CF Training: Epoch 0001 Iter 1014 / 2325 | Time 0.1s | Iter Loss 0.4241 | Iter Mean Loss 0.5620\n",
      "2025-11-10 13:44:59,896 - root - INFO - CF Training: Epoch 0001 Iter 1017 / 2325 | Time 0.1s | Iter Loss 0.4395 | Iter Mean Loss 0.5616\n",
      "2025-11-10 13:45:00,092 - root - INFO - CF Training: Epoch 0001 Iter 1020 / 2325 | Time 0.1s | Iter Loss 0.4411 | Iter Mean Loss 0.5612\n",
      "2025-11-10 13:45:00,287 - root - INFO - CF Training: Epoch 0001 Iter 1023 / 2325 | Time 0.1s | Iter Loss 0.4264 | Iter Mean Loss 0.5609\n",
      "2025-11-10 13:45:00,482 - root - INFO - CF Training: Epoch 0001 Iter 1026 / 2325 | Time 0.1s | Iter Loss 0.4343 | Iter Mean Loss 0.5605\n",
      "2025-11-10 13:45:00,677 - root - INFO - CF Training: Epoch 0001 Iter 1029 / 2325 | Time 0.1s | Iter Loss 0.4397 | Iter Mean Loss 0.5601\n",
      "2025-11-10 13:45:00,871 - root - INFO - CF Training: Epoch 0001 Iter 1032 / 2325 | Time 0.1s | Iter Loss 0.4406 | Iter Mean Loss 0.5597\n",
      "2025-11-10 13:45:01,067 - root - INFO - CF Training: Epoch 0001 Iter 1035 / 2325 | Time 0.1s | Iter Loss 0.4283 | Iter Mean Loss 0.5594\n",
      "2025-11-10 13:45:01,262 - root - INFO - CF Training: Epoch 0001 Iter 1038 / 2325 | Time 0.1s | Iter Loss 0.4214 | Iter Mean Loss 0.5590\n",
      "2025-11-10 13:45:01,456 - root - INFO - CF Training: Epoch 0001 Iter 1041 / 2325 | Time 0.1s | Iter Loss 0.4127 | Iter Mean Loss 0.5586\n",
      "2025-11-10 13:45:01,652 - root - INFO - CF Training: Epoch 0001 Iter 1044 / 2325 | Time 0.1s | Iter Loss 0.4285 | Iter Mean Loss 0.5582\n",
      "2025-11-10 13:45:01,847 - root - INFO - CF Training: Epoch 0001 Iter 1047 / 2325 | Time 0.1s | Iter Loss 0.4169 | Iter Mean Loss 0.5578\n",
      "2025-11-10 13:45:02,042 - root - INFO - CF Training: Epoch 0001 Iter 1050 / 2325 | Time 0.1s | Iter Loss 0.4199 | Iter Mean Loss 0.5575\n",
      "2025-11-10 13:45:02,240 - root - INFO - CF Training: Epoch 0001 Iter 1053 / 2325 | Time 0.1s | Iter Loss 0.4315 | Iter Mean Loss 0.5571\n",
      "2025-11-10 13:45:02,435 - root - INFO - CF Training: Epoch 0001 Iter 1056 / 2325 | Time 0.1s | Iter Loss 0.4365 | Iter Mean Loss 0.5567\n",
      "2025-11-10 13:45:02,631 - root - INFO - CF Training: Epoch 0001 Iter 1059 / 2325 | Time 0.1s | Iter Loss 0.4346 | Iter Mean Loss 0.5564\n",
      "2025-11-10 13:45:02,826 - root - INFO - CF Training: Epoch 0001 Iter 1062 / 2325 | Time 0.1s | Iter Loss 0.4382 | Iter Mean Loss 0.5560\n",
      "2025-11-10 13:45:03,021 - root - INFO - CF Training: Epoch 0001 Iter 1065 / 2325 | Time 0.1s | Iter Loss 0.4273 | Iter Mean Loss 0.5557\n",
      "2025-11-10 13:45:03,217 - root - INFO - CF Training: Epoch 0001 Iter 1068 / 2325 | Time 0.1s | Iter Loss 0.4100 | Iter Mean Loss 0.5553\n",
      "2025-11-10 13:45:03,412 - root - INFO - CF Training: Epoch 0001 Iter 1071 / 2325 | Time 0.1s | Iter Loss 0.4339 | Iter Mean Loss 0.5549\n",
      "2025-11-10 13:45:03,608 - root - INFO - CF Training: Epoch 0001 Iter 1074 / 2325 | Time 0.1s | Iter Loss 0.4144 | Iter Mean Loss 0.5546\n",
      "2025-11-10 13:45:03,802 - root - INFO - CF Training: Epoch 0001 Iter 1077 / 2325 | Time 0.1s | Iter Loss 0.4170 | Iter Mean Loss 0.5542\n",
      "2025-11-10 13:45:03,997 - root - INFO - CF Training: Epoch 0001 Iter 1080 / 2325 | Time 0.1s | Iter Loss 0.4268 | Iter Mean Loss 0.5538\n",
      "2025-11-10 13:45:04,191 - root - INFO - CF Training: Epoch 0001 Iter 1083 / 2325 | Time 0.1s | Iter Loss 0.4143 | Iter Mean Loss 0.5535\n",
      "2025-11-10 13:45:04,386 - root - INFO - CF Training: Epoch 0001 Iter 1086 / 2325 | Time 0.1s | Iter Loss 0.4176 | Iter Mean Loss 0.5531\n",
      "2025-11-10 13:45:04,582 - root - INFO - CF Training: Epoch 0001 Iter 1089 / 2325 | Time 0.1s | Iter Loss 0.4429 | Iter Mean Loss 0.5528\n",
      "2025-11-10 13:45:04,777 - root - INFO - CF Training: Epoch 0001 Iter 1092 / 2325 | Time 0.1s | Iter Loss 0.4350 | Iter Mean Loss 0.5525\n",
      "2025-11-10 13:45:04,972 - root - INFO - CF Training: Epoch 0001 Iter 1095 / 2325 | Time 0.1s | Iter Loss 0.4171 | Iter Mean Loss 0.5521\n",
      "2025-11-10 13:45:05,169 - root - INFO - CF Training: Epoch 0001 Iter 1098 / 2325 | Time 0.1s | Iter Loss 0.4527 | Iter Mean Loss 0.5518\n",
      "2025-11-10 13:45:05,364 - root - INFO - CF Training: Epoch 0001 Iter 1101 / 2325 | Time 0.1s | Iter Loss 0.4308 | Iter Mean Loss 0.5514\n",
      "2025-11-10 13:45:05,561 - root - INFO - CF Training: Epoch 0001 Iter 1104 / 2325 | Time 0.1s | Iter Loss 0.4522 | Iter Mean Loss 0.5511\n",
      "2025-11-10 13:45:05,757 - root - INFO - CF Training: Epoch 0001 Iter 1107 / 2325 | Time 0.1s | Iter Loss 0.4205 | Iter Mean Loss 0.5508\n",
      "2025-11-10 13:45:05,952 - root - INFO - CF Training: Epoch 0001 Iter 1110 / 2325 | Time 0.1s | Iter Loss 0.4287 | Iter Mean Loss 0.5504\n",
      "2025-11-10 13:45:06,148 - root - INFO - CF Training: Epoch 0001 Iter 1113 / 2325 | Time 0.1s | Iter Loss 0.4224 | Iter Mean Loss 0.5501\n",
      "2025-11-10 13:45:06,344 - root - INFO - CF Training: Epoch 0001 Iter 1116 / 2325 | Time 0.1s | Iter Loss 0.4139 | Iter Mean Loss 0.5497\n",
      "2025-11-10 13:45:06,539 - root - INFO - CF Training: Epoch 0001 Iter 1119 / 2325 | Time 0.1s | Iter Loss 0.4073 | Iter Mean Loss 0.5494\n",
      "2025-11-10 13:45:06,735 - root - INFO - CF Training: Epoch 0001 Iter 1122 / 2325 | Time 0.1s | Iter Loss 0.4175 | Iter Mean Loss 0.5490\n",
      "2025-11-10 13:45:06,930 - root - INFO - CF Training: Epoch 0001 Iter 1125 / 2325 | Time 0.1s | Iter Loss 0.4194 | Iter Mean Loss 0.5487\n",
      "2025-11-10 13:45:07,126 - root - INFO - CF Training: Epoch 0001 Iter 1128 / 2325 | Time 0.1s | Iter Loss 0.4098 | Iter Mean Loss 0.5483\n",
      "2025-11-10 13:45:07,322 - root - INFO - CF Training: Epoch 0001 Iter 1131 / 2325 | Time 0.1s | Iter Loss 0.4378 | Iter Mean Loss 0.5479\n",
      "2025-11-10 13:45:07,518 - root - INFO - CF Training: Epoch 0001 Iter 1134 / 2325 | Time 0.1s | Iter Loss 0.4190 | Iter Mean Loss 0.5476\n",
      "2025-11-10 13:45:07,713 - root - INFO - CF Training: Epoch 0001 Iter 1137 / 2325 | Time 0.1s | Iter Loss 0.4141 | Iter Mean Loss 0.5473\n",
      "2025-11-10 13:45:07,908 - root - INFO - CF Training: Epoch 0001 Iter 1140 / 2325 | Time 0.1s | Iter Loss 0.4107 | Iter Mean Loss 0.5469\n",
      "2025-11-10 13:45:08,103 - root - INFO - CF Training: Epoch 0001 Iter 1143 / 2325 | Time 0.1s | Iter Loss 0.4397 | Iter Mean Loss 0.5466\n",
      "2025-11-10 13:45:08,299 - root - INFO - CF Training: Epoch 0001 Iter 1146 / 2325 | Time 0.1s | Iter Loss 0.4180 | Iter Mean Loss 0.5463\n",
      "2025-11-10 13:45:08,495 - root - INFO - CF Training: Epoch 0001 Iter 1149 / 2325 | Time 0.1s | Iter Loss 0.4218 | Iter Mean Loss 0.5459\n",
      "2025-11-10 13:45:08,690 - root - INFO - CF Training: Epoch 0001 Iter 1152 / 2325 | Time 0.1s | Iter Loss 0.4211 | Iter Mean Loss 0.5456\n",
      "2025-11-10 13:45:08,885 - root - INFO - CF Training: Epoch 0001 Iter 1155 / 2325 | Time 0.1s | Iter Loss 0.4141 | Iter Mean Loss 0.5453\n",
      "2025-11-10 13:45:09,080 - root - INFO - CF Training: Epoch 0001 Iter 1158 / 2325 | Time 0.1s | Iter Loss 0.4230 | Iter Mean Loss 0.5450\n",
      "2025-11-10 13:45:09,274 - root - INFO - CF Training: Epoch 0001 Iter 1161 / 2325 | Time 0.1s | Iter Loss 0.4166 | Iter Mean Loss 0.5446\n",
      "2025-11-10 13:45:09,469 - root - INFO - CF Training: Epoch 0001 Iter 1164 / 2325 | Time 0.1s | Iter Loss 0.4229 | Iter Mean Loss 0.5443\n",
      "2025-11-10 13:45:09,665 - root - INFO - CF Training: Epoch 0001 Iter 1167 / 2325 | Time 0.1s | Iter Loss 0.4049 | Iter Mean Loss 0.5440\n",
      "2025-11-10 13:45:09,860 - root - INFO - CF Training: Epoch 0001 Iter 1170 / 2325 | Time 0.1s | Iter Loss 0.4122 | Iter Mean Loss 0.5436\n",
      "2025-11-10 13:45:10,055 - root - INFO - CF Training: Epoch 0001 Iter 1173 / 2325 | Time 0.1s | Iter Loss 0.4258 | Iter Mean Loss 0.5433\n",
      "2025-11-10 13:45:10,250 - root - INFO - CF Training: Epoch 0001 Iter 1176 / 2325 | Time 0.1s | Iter Loss 0.4103 | Iter Mean Loss 0.5429\n",
      "2025-11-10 13:45:10,444 - root - INFO - CF Training: Epoch 0001 Iter 1179 / 2325 | Time 0.1s | Iter Loss 0.4197 | Iter Mean Loss 0.5426\n",
      "2025-11-10 13:45:10,639 - root - INFO - CF Training: Epoch 0001 Iter 1182 / 2325 | Time 0.1s | Iter Loss 0.4114 | Iter Mean Loss 0.5423\n",
      "2025-11-10 13:45:10,834 - root - INFO - CF Training: Epoch 0001 Iter 1185 / 2325 | Time 0.1s | Iter Loss 0.4069 | Iter Mean Loss 0.5420\n",
      "2025-11-10 13:45:11,029 - root - INFO - CF Training: Epoch 0001 Iter 1188 / 2325 | Time 0.1s | Iter Loss 0.4142 | Iter Mean Loss 0.5416\n",
      "2025-11-10 13:45:11,226 - root - INFO - CF Training: Epoch 0001 Iter 1191 / 2325 | Time 0.1s | Iter Loss 0.4116 | Iter Mean Loss 0.5413\n",
      "2025-11-10 13:45:11,423 - root - INFO - CF Training: Epoch 0001 Iter 1194 / 2325 | Time 0.1s | Iter Loss 0.4031 | Iter Mean Loss 0.5410\n",
      "2025-11-10 13:45:11,618 - root - INFO - CF Training: Epoch 0001 Iter 1197 / 2325 | Time 0.1s | Iter Loss 0.4219 | Iter Mean Loss 0.5407\n",
      "2025-11-10 13:45:11,812 - root - INFO - CF Training: Epoch 0001 Iter 1200 / 2325 | Time 0.1s | Iter Loss 0.4195 | Iter Mean Loss 0.5404\n",
      "2025-11-10 13:45:12,005 - root - INFO - CF Training: Epoch 0001 Iter 1203 / 2325 | Time 0.1s | Iter Loss 0.4106 | Iter Mean Loss 0.5401\n",
      "2025-11-10 13:45:12,199 - root - INFO - CF Training: Epoch 0001 Iter 1206 / 2325 | Time 0.1s | Iter Loss 0.4275 | Iter Mean Loss 0.5398\n",
      "2025-11-10 13:45:12,393 - root - INFO - CF Training: Epoch 0001 Iter 1209 / 2325 | Time 0.1s | Iter Loss 0.4133 | Iter Mean Loss 0.5395\n",
      "2025-11-10 13:45:12,587 - root - INFO - CF Training: Epoch 0001 Iter 1212 / 2325 | Time 0.1s | Iter Loss 0.4041 | Iter Mean Loss 0.5392\n",
      "2025-11-10 13:45:12,780 - root - INFO - CF Training: Epoch 0001 Iter 1215 / 2325 | Time 0.1s | Iter Loss 0.4259 | Iter Mean Loss 0.5389\n",
      "2025-11-10 13:45:12,974 - root - INFO - CF Training: Epoch 0001 Iter 1218 / 2325 | Time 0.1s | Iter Loss 0.4159 | Iter Mean Loss 0.5385\n",
      "2025-11-10 13:45:13,168 - root - INFO - CF Training: Epoch 0001 Iter 1221 / 2325 | Time 0.1s | Iter Loss 0.4118 | Iter Mean Loss 0.5382\n",
      "2025-11-10 13:45:13,362 - root - INFO - CF Training: Epoch 0001 Iter 1224 / 2325 | Time 0.1s | Iter Loss 0.4090 | Iter Mean Loss 0.5379\n",
      "2025-11-10 13:45:13,556 - root - INFO - CF Training: Epoch 0001 Iter 1227 / 2325 | Time 0.1s | Iter Loss 0.4063 | Iter Mean Loss 0.5376\n",
      "2025-11-10 13:45:13,753 - root - INFO - CF Training: Epoch 0001 Iter 1230 / 2325 | Time 0.1s | Iter Loss 0.4154 | Iter Mean Loss 0.5373\n",
      "2025-11-10 13:45:13,947 - root - INFO - CF Training: Epoch 0001 Iter 1233 / 2325 | Time 0.1s | Iter Loss 0.4082 | Iter Mean Loss 0.5370\n",
      "2025-11-10 13:45:14,141 - root - INFO - CF Training: Epoch 0001 Iter 1236 / 2325 | Time 0.1s | Iter Loss 0.4053 | Iter Mean Loss 0.5367\n",
      "2025-11-10 13:45:14,335 - root - INFO - CF Training: Epoch 0001 Iter 1239 / 2325 | Time 0.1s | Iter Loss 0.4135 | Iter Mean Loss 0.5364\n",
      "2025-11-10 13:45:14,529 - root - INFO - CF Training: Epoch 0001 Iter 1242 / 2325 | Time 0.1s | Iter Loss 0.4118 | Iter Mean Loss 0.5361\n",
      "2025-11-10 13:45:14,723 - root - INFO - CF Training: Epoch 0001 Iter 1245 / 2325 | Time 0.1s | Iter Loss 0.4279 | Iter Mean Loss 0.5358\n",
      "2025-11-10 13:45:14,916 - root - INFO - CF Training: Epoch 0001 Iter 1248 / 2325 | Time 0.1s | Iter Loss 0.4274 | Iter Mean Loss 0.5355\n",
      "2025-11-10 13:45:15,110 - root - INFO - CF Training: Epoch 0001 Iter 1251 / 2325 | Time 0.1s | Iter Loss 0.4172 | Iter Mean Loss 0.5352\n",
      "2025-11-10 13:45:15,304 - root - INFO - CF Training: Epoch 0001 Iter 1254 / 2325 | Time 0.1s | Iter Loss 0.4145 | Iter Mean Loss 0.5349\n",
      "2025-11-10 13:45:15,498 - root - INFO - CF Training: Epoch 0001 Iter 1257 / 2325 | Time 0.1s | Iter Loss 0.3991 | Iter Mean Loss 0.5346\n",
      "2025-11-10 13:45:15,692 - root - INFO - CF Training: Epoch 0001 Iter 1260 / 2325 | Time 0.1s | Iter Loss 0.4235 | Iter Mean Loss 0.5343\n",
      "2025-11-10 13:45:15,885 - root - INFO - CF Training: Epoch 0001 Iter 1263 / 2325 | Time 0.1s | Iter Loss 0.4151 | Iter Mean Loss 0.5341\n",
      "2025-11-10 13:45:16,079 - root - INFO - CF Training: Epoch 0001 Iter 1266 / 2325 | Time 0.1s | Iter Loss 0.4021 | Iter Mean Loss 0.5338\n",
      "2025-11-10 13:45:16,273 - root - INFO - CF Training: Epoch 0001 Iter 1269 / 2325 | Time 0.1s | Iter Loss 0.4240 | Iter Mean Loss 0.5335\n",
      "2025-11-10 13:45:16,467 - root - INFO - CF Training: Epoch 0001 Iter 1272 / 2325 | Time 0.1s | Iter Loss 0.3996 | Iter Mean Loss 0.5332\n",
      "2025-11-10 13:45:16,662 - root - INFO - CF Training: Epoch 0001 Iter 1275 / 2325 | Time 0.1s | Iter Loss 0.4043 | Iter Mean Loss 0.5329\n",
      "2025-11-10 13:45:16,856 - root - INFO - CF Training: Epoch 0001 Iter 1278 / 2325 | Time 0.1s | Iter Loss 0.4205 | Iter Mean Loss 0.5326\n",
      "2025-11-10 13:45:17,052 - root - INFO - CF Training: Epoch 0001 Iter 1281 / 2325 | Time 0.1s | Iter Loss 0.3998 | Iter Mean Loss 0.5323\n",
      "2025-11-10 13:45:17,245 - root - INFO - CF Training: Epoch 0001 Iter 1284 / 2325 | Time 0.1s | Iter Loss 0.4031 | Iter Mean Loss 0.5320\n",
      "2025-11-10 13:45:17,439 - root - INFO - CF Training: Epoch 0001 Iter 1287 / 2325 | Time 0.1s | Iter Loss 0.4123 | Iter Mean Loss 0.5317\n",
      "2025-11-10 13:45:17,633 - root - INFO - CF Training: Epoch 0001 Iter 1290 / 2325 | Time 0.1s | Iter Loss 0.4060 | Iter Mean Loss 0.5314\n",
      "2025-11-10 13:45:17,827 - root - INFO - CF Training: Epoch 0001 Iter 1293 / 2325 | Time 0.1s | Iter Loss 0.3914 | Iter Mean Loss 0.5311\n",
      "2025-11-10 13:45:18,022 - root - INFO - CF Training: Epoch 0001 Iter 1296 / 2325 | Time 0.1s | Iter Loss 0.4119 | Iter Mean Loss 0.5309\n",
      "2025-11-10 13:45:18,216 - root - INFO - CF Training: Epoch 0001 Iter 1299 / 2325 | Time 0.1s | Iter Loss 0.4141 | Iter Mean Loss 0.5306\n",
      "2025-11-10 13:45:18,410 - root - INFO - CF Training: Epoch 0001 Iter 1302 / 2325 | Time 0.1s | Iter Loss 0.4005 | Iter Mean Loss 0.5303\n",
      "2025-11-10 13:45:18,603 - root - INFO - CF Training: Epoch 0001 Iter 1305 / 2325 | Time 0.1s | Iter Loss 0.3959 | Iter Mean Loss 0.5300\n",
      "2025-11-10 13:45:18,797 - root - INFO - CF Training: Epoch 0001 Iter 1308 / 2325 | Time 0.1s | Iter Loss 0.4096 | Iter Mean Loss 0.5297\n",
      "2025-11-10 13:45:18,993 - root - INFO - CF Training: Epoch 0001 Iter 1311 / 2325 | Time 0.1s | Iter Loss 0.4156 | Iter Mean Loss 0.5295\n",
      "2025-11-10 13:45:19,189 - root - INFO - CF Training: Epoch 0001 Iter 1314 / 2325 | Time 0.1s | Iter Loss 0.3958 | Iter Mean Loss 0.5292\n",
      "2025-11-10 13:45:19,383 - root - INFO - CF Training: Epoch 0001 Iter 1317 / 2325 | Time 0.1s | Iter Loss 0.4063 | Iter Mean Loss 0.5289\n",
      "2025-11-10 13:45:19,576 - root - INFO - CF Training: Epoch 0001 Iter 1320 / 2325 | Time 0.1s | Iter Loss 0.4102 | Iter Mean Loss 0.5286\n",
      "2025-11-10 13:45:19,770 - root - INFO - CF Training: Epoch 0001 Iter 1323 / 2325 | Time 0.1s | Iter Loss 0.4090 | Iter Mean Loss 0.5283\n",
      "2025-11-10 13:45:19,964 - root - INFO - CF Training: Epoch 0001 Iter 1326 / 2325 | Time 0.1s | Iter Loss 0.4093 | Iter Mean Loss 0.5280\n",
      "2025-11-10 13:45:20,159 - root - INFO - CF Training: Epoch 0001 Iter 1329 / 2325 | Time 0.1s | Iter Loss 0.4143 | Iter Mean Loss 0.5277\n",
      "2025-11-10 13:45:20,352 - root - INFO - CF Training: Epoch 0001 Iter 1332 / 2325 | Time 0.1s | Iter Loss 0.3894 | Iter Mean Loss 0.5275\n",
      "2025-11-10 13:45:20,546 - root - INFO - CF Training: Epoch 0001 Iter 1335 / 2325 | Time 0.1s | Iter Loss 0.3974 | Iter Mean Loss 0.5272\n",
      "2025-11-10 13:45:20,743 - root - INFO - CF Training: Epoch 0001 Iter 1338 / 2325 | Time 0.1s | Iter Loss 0.3996 | Iter Mean Loss 0.5269\n",
      "2025-11-10 13:45:20,936 - root - INFO - CF Training: Epoch 0001 Iter 1341 / 2325 | Time 0.1s | Iter Loss 0.3991 | Iter Mean Loss 0.5266\n",
      "2025-11-10 13:45:21,132 - root - INFO - CF Training: Epoch 0001 Iter 1344 / 2325 | Time 0.1s | Iter Loss 0.3990 | Iter Mean Loss 0.5263\n",
      "2025-11-10 13:45:21,327 - root - INFO - CF Training: Epoch 0001 Iter 1347 / 2325 | Time 0.1s | Iter Loss 0.4049 | Iter Mean Loss 0.5260\n",
      "2025-11-10 13:45:21,523 - root - INFO - CF Training: Epoch 0001 Iter 1350 / 2325 | Time 0.1s | Iter Loss 0.4121 | Iter Mean Loss 0.5258\n",
      "2025-11-10 13:45:21,719 - root - INFO - CF Training: Epoch 0001 Iter 1353 / 2325 | Time 0.1s | Iter Loss 0.4091 | Iter Mean Loss 0.5255\n",
      "2025-11-10 13:45:21,914 - root - INFO - CF Training: Epoch 0001 Iter 1356 / 2325 | Time 0.1s | Iter Loss 0.4105 | Iter Mean Loss 0.5252\n",
      "2025-11-10 13:45:22,110 - root - INFO - CF Training: Epoch 0001 Iter 1359 / 2325 | Time 0.1s | Iter Loss 0.3789 | Iter Mean Loss 0.5250\n",
      "2025-11-10 13:45:22,304 - root - INFO - CF Training: Epoch 0001 Iter 1362 / 2325 | Time 0.1s | Iter Loss 0.4076 | Iter Mean Loss 0.5247\n",
      "2025-11-10 13:45:22,498 - root - INFO - CF Training: Epoch 0001 Iter 1365 / 2325 | Time 0.1s | Iter Loss 0.4019 | Iter Mean Loss 0.5244\n",
      "2025-11-10 13:45:22,693 - root - INFO - CF Training: Epoch 0001 Iter 1368 / 2325 | Time 0.1s | Iter Loss 0.3919 | Iter Mean Loss 0.5242\n",
      "2025-11-10 13:45:22,888 - root - INFO - CF Training: Epoch 0001 Iter 1371 / 2325 | Time 0.1s | Iter Loss 0.3891 | Iter Mean Loss 0.5239\n",
      "2025-11-10 13:45:23,083 - root - INFO - CF Training: Epoch 0001 Iter 1374 / 2325 | Time 0.1s | Iter Loss 0.3998 | Iter Mean Loss 0.5236\n",
      "2025-11-10 13:45:23,278 - root - INFO - CF Training: Epoch 0001 Iter 1377 / 2325 | Time 0.1s | Iter Loss 0.4194 | Iter Mean Loss 0.5234\n",
      "2025-11-10 13:45:23,472 - root - INFO - CF Training: Epoch 0001 Iter 1380 / 2325 | Time 0.1s | Iter Loss 0.3891 | Iter Mean Loss 0.5231\n",
      "2025-11-10 13:45:23,666 - root - INFO - CF Training: Epoch 0001 Iter 1383 / 2325 | Time 0.1s | Iter Loss 0.4161 | Iter Mean Loss 0.5229\n",
      "2025-11-10 13:45:23,861 - root - INFO - CF Training: Epoch 0001 Iter 1386 / 2325 | Time 0.1s | Iter Loss 0.4215 | Iter Mean Loss 0.5226\n",
      "2025-11-10 13:45:24,054 - root - INFO - CF Training: Epoch 0001 Iter 1389 / 2325 | Time 0.1s | Iter Loss 0.4169 | Iter Mean Loss 0.5224\n",
      "2025-11-10 13:45:24,248 - root - INFO - CF Training: Epoch 0001 Iter 1392 / 2325 | Time 0.1s | Iter Loss 0.3859 | Iter Mean Loss 0.5221\n",
      "2025-11-10 13:45:24,442 - root - INFO - CF Training: Epoch 0001 Iter 1395 / 2325 | Time 0.1s | Iter Loss 0.4060 | Iter Mean Loss 0.5218\n",
      "2025-11-10 13:45:24,635 - root - INFO - CF Training: Epoch 0001 Iter 1398 / 2325 | Time 0.1s | Iter Loss 0.3966 | Iter Mean Loss 0.5216\n",
      "2025-11-10 13:45:24,829 - root - INFO - CF Training: Epoch 0001 Iter 1401 / 2325 | Time 0.1s | Iter Loss 0.4037 | Iter Mean Loss 0.5214\n",
      "2025-11-10 13:45:25,023 - root - INFO - CF Training: Epoch 0001 Iter 1404 / 2325 | Time 0.1s | Iter Loss 0.4026 | Iter Mean Loss 0.5211\n",
      "2025-11-10 13:45:25,217 - root - INFO - CF Training: Epoch 0001 Iter 1407 / 2325 | Time 0.1s | Iter Loss 0.3896 | Iter Mean Loss 0.5209\n",
      "2025-11-10 13:45:25,411 - root - INFO - CF Training: Epoch 0001 Iter 1410 / 2325 | Time 0.1s | Iter Loss 0.3883 | Iter Mean Loss 0.5206\n",
      "2025-11-10 13:45:25,605 - root - INFO - CF Training: Epoch 0001 Iter 1413 / 2325 | Time 0.1s | Iter Loss 0.4059 | Iter Mean Loss 0.5203\n",
      "2025-11-10 13:45:25,798 - root - INFO - CF Training: Epoch 0001 Iter 1416 / 2325 | Time 0.1s | Iter Loss 0.3850 | Iter Mean Loss 0.5201\n",
      "2025-11-10 13:45:25,992 - root - INFO - CF Training: Epoch 0001 Iter 1419 / 2325 | Time 0.1s | Iter Loss 0.4068 | Iter Mean Loss 0.5198\n",
      "2025-11-10 13:45:26,186 - root - INFO - CF Training: Epoch 0001 Iter 1422 / 2325 | Time 0.1s | Iter Loss 0.3985 | Iter Mean Loss 0.5196\n",
      "2025-11-10 13:45:26,381 - root - INFO - CF Training: Epoch 0001 Iter 1425 / 2325 | Time 0.1s | Iter Loss 0.4115 | Iter Mean Loss 0.5193\n",
      "2025-11-10 13:45:26,575 - root - INFO - CF Training: Epoch 0001 Iter 1428 / 2325 | Time 0.1s | Iter Loss 0.4123 | Iter Mean Loss 0.5191\n",
      "2025-11-10 13:45:26,769 - root - INFO - CF Training: Epoch 0001 Iter 1431 / 2325 | Time 0.1s | Iter Loss 0.3942 | Iter Mean Loss 0.5189\n",
      "2025-11-10 13:45:26,965 - root - INFO - CF Training: Epoch 0001 Iter 1434 / 2325 | Time 0.1s | Iter Loss 0.3975 | Iter Mean Loss 0.5186\n",
      "2025-11-10 13:45:27,160 - root - INFO - CF Training: Epoch 0001 Iter 1437 / 2325 | Time 0.1s | Iter Loss 0.4060 | Iter Mean Loss 0.5183\n",
      "2025-11-10 13:45:27,355 - root - INFO - CF Training: Epoch 0001 Iter 1440 / 2325 | Time 0.1s | Iter Loss 0.3914 | Iter Mean Loss 0.5181\n",
      "2025-11-10 13:45:27,549 - root - INFO - CF Training: Epoch 0001 Iter 1443 / 2325 | Time 0.1s | Iter Loss 0.3937 | Iter Mean Loss 0.5178\n",
      "2025-11-10 13:45:27,745 - root - INFO - CF Training: Epoch 0001 Iter 1446 / 2325 | Time 0.1s | Iter Loss 0.3862 | Iter Mean Loss 0.5176\n",
      "2025-11-10 13:45:27,939 - root - INFO - CF Training: Epoch 0001 Iter 1449 / 2325 | Time 0.1s | Iter Loss 0.4099 | Iter Mean Loss 0.5173\n",
      "2025-11-10 13:45:28,133 - root - INFO - CF Training: Epoch 0001 Iter 1452 / 2325 | Time 0.1s | Iter Loss 0.3896 | Iter Mean Loss 0.5171\n",
      "2025-11-10 13:45:28,329 - root - INFO - CF Training: Epoch 0001 Iter 1455 / 2325 | Time 0.1s | Iter Loss 0.4203 | Iter Mean Loss 0.5169\n",
      "2025-11-10 13:45:28,523 - root - INFO - CF Training: Epoch 0001 Iter 1458 / 2325 | Time 0.1s | Iter Loss 0.3743 | Iter Mean Loss 0.5166\n",
      "2025-11-10 13:45:28,716 - root - INFO - CF Training: Epoch 0001 Iter 1461 / 2325 | Time 0.1s | Iter Loss 0.3807 | Iter Mean Loss 0.5163\n",
      "2025-11-10 13:45:28,911 - root - INFO - CF Training: Epoch 0001 Iter 1464 / 2325 | Time 0.1s | Iter Loss 0.3862 | Iter Mean Loss 0.5161\n",
      "2025-11-10 13:45:29,106 - root - INFO - CF Training: Epoch 0001 Iter 1467 / 2325 | Time 0.1s | Iter Loss 0.4009 | Iter Mean Loss 0.5158\n",
      "2025-11-10 13:45:29,300 - root - INFO - CF Training: Epoch 0001 Iter 1470 / 2325 | Time 0.1s | Iter Loss 0.3774 | Iter Mean Loss 0.5156\n",
      "2025-11-10 13:45:29,494 - root - INFO - CF Training: Epoch 0001 Iter 1473 / 2325 | Time 0.1s | Iter Loss 0.4063 | Iter Mean Loss 0.5154\n",
      "2025-11-10 13:45:29,688 - root - INFO - CF Training: Epoch 0001 Iter 1476 / 2325 | Time 0.1s | Iter Loss 0.3908 | Iter Mean Loss 0.5151\n",
      "2025-11-10 13:45:29,884 - root - INFO - CF Training: Epoch 0001 Iter 1479 / 2325 | Time 0.1s | Iter Loss 0.3857 | Iter Mean Loss 0.5148\n",
      "2025-11-10 13:45:30,078 - root - INFO - CF Training: Epoch 0001 Iter 1482 / 2325 | Time 0.1s | Iter Loss 0.3965 | Iter Mean Loss 0.5146\n",
      "2025-11-10 13:45:30,272 - root - INFO - CF Training: Epoch 0001 Iter 1485 / 2325 | Time 0.1s | Iter Loss 0.3967 | Iter Mean Loss 0.5144\n",
      "2025-11-10 13:45:30,467 - root - INFO - CF Training: Epoch 0001 Iter 1488 / 2325 | Time 0.1s | Iter Loss 0.3915 | Iter Mean Loss 0.5141\n",
      "2025-11-10 13:45:30,661 - root - INFO - CF Training: Epoch 0001 Iter 1491 / 2325 | Time 0.1s | Iter Loss 0.3770 | Iter Mean Loss 0.5139\n",
      "2025-11-10 13:45:30,855 - root - INFO - CF Training: Epoch 0001 Iter 1494 / 2325 | Time 0.1s | Iter Loss 0.4036 | Iter Mean Loss 0.5136\n",
      "2025-11-10 13:45:31,049 - root - INFO - CF Training: Epoch 0001 Iter 1497 / 2325 | Time 0.1s | Iter Loss 0.3802 | Iter Mean Loss 0.5134\n",
      "2025-11-10 13:45:31,243 - root - INFO - CF Training: Epoch 0001 Iter 1500 / 2325 | Time 0.1s | Iter Loss 0.3976 | Iter Mean Loss 0.5132\n",
      "2025-11-10 13:45:31,438 - root - INFO - CF Training: Epoch 0001 Iter 1503 / 2325 | Time 0.1s | Iter Loss 0.3964 | Iter Mean Loss 0.5129\n",
      "2025-11-10 13:45:31,631 - root - INFO - CF Training: Epoch 0001 Iter 1506 / 2325 | Time 0.1s | Iter Loss 0.4123 | Iter Mean Loss 0.5127\n",
      "2025-11-10 13:45:31,826 - root - INFO - CF Training: Epoch 0001 Iter 1509 / 2325 | Time 0.1s | Iter Loss 0.3896 | Iter Mean Loss 0.5125\n",
      "2025-11-10 13:45:32,021 - root - INFO - CF Training: Epoch 0001 Iter 1512 / 2325 | Time 0.1s | Iter Loss 0.3918 | Iter Mean Loss 0.5122\n",
      "2025-11-10 13:45:32,218 - root - INFO - CF Training: Epoch 0001 Iter 1515 / 2325 | Time 0.1s | Iter Loss 0.3957 | Iter Mean Loss 0.5120\n",
      "2025-11-10 13:45:32,411 - root - INFO - CF Training: Epoch 0001 Iter 1518 / 2325 | Time 0.1s | Iter Loss 0.3843 | Iter Mean Loss 0.5118\n",
      "2025-11-10 13:45:32,605 - root - INFO - CF Training: Epoch 0001 Iter 1521 / 2325 | Time 0.1s | Iter Loss 0.3982 | Iter Mean Loss 0.5115\n",
      "2025-11-10 13:45:32,799 - root - INFO - CF Training: Epoch 0001 Iter 1524 / 2325 | Time 0.1s | Iter Loss 0.3802 | Iter Mean Loss 0.5113\n",
      "2025-11-10 13:45:32,993 - root - INFO - CF Training: Epoch 0001 Iter 1527 / 2325 | Time 0.1s | Iter Loss 0.3959 | Iter Mean Loss 0.5110\n",
      "2025-11-10 13:45:33,188 - root - INFO - CF Training: Epoch 0001 Iter 1530 / 2325 | Time 0.1s | Iter Loss 0.3972 | Iter Mean Loss 0.5108\n",
      "2025-11-10 13:45:33,381 - root - INFO - CF Training: Epoch 0001 Iter 1533 / 2325 | Time 0.1s | Iter Loss 0.3911 | Iter Mean Loss 0.5106\n",
      "2025-11-10 13:45:33,575 - root - INFO - CF Training: Epoch 0001 Iter 1536 / 2325 | Time 0.1s | Iter Loss 0.3808 | Iter Mean Loss 0.5103\n",
      "2025-11-10 13:45:33,769 - root - INFO - CF Training: Epoch 0001 Iter 1539 / 2325 | Time 0.1s | Iter Loss 0.3655 | Iter Mean Loss 0.5101\n",
      "2025-11-10 13:45:33,962 - root - INFO - CF Training: Epoch 0001 Iter 1542 / 2325 | Time 0.1s | Iter Loss 0.3962 | Iter Mean Loss 0.5099\n",
      "2025-11-10 13:45:34,157 - root - INFO - CF Training: Epoch 0001 Iter 1545 / 2325 | Time 0.1s | Iter Loss 0.3860 | Iter Mean Loss 0.5096\n",
      "2025-11-10 13:45:34,350 - root - INFO - CF Training: Epoch 0001 Iter 1548 / 2325 | Time 0.1s | Iter Loss 0.3815 | Iter Mean Loss 0.5094\n",
      "2025-11-10 13:45:34,545 - root - INFO - CF Training: Epoch 0001 Iter 1551 / 2325 | Time 0.1s | Iter Loss 0.3943 | Iter Mean Loss 0.5092\n",
      "2025-11-10 13:45:34,738 - root - INFO - CF Training: Epoch 0001 Iter 1554 / 2325 | Time 0.1s | Iter Loss 0.4033 | Iter Mean Loss 0.5090\n",
      "2025-11-10 13:45:34,933 - root - INFO - CF Training: Epoch 0001 Iter 1557 / 2325 | Time 0.1s | Iter Loss 0.3954 | Iter Mean Loss 0.5087\n",
      "2025-11-10 13:45:35,126 - root - INFO - CF Training: Epoch 0001 Iter 1560 / 2325 | Time 0.1s | Iter Loss 0.3959 | Iter Mean Loss 0.5085\n",
      "2025-11-10 13:45:35,320 - root - INFO - CF Training: Epoch 0001 Iter 1563 / 2325 | Time 0.1s | Iter Loss 0.3954 | Iter Mean Loss 0.5083\n",
      "2025-11-10 13:45:35,514 - root - INFO - CF Training: Epoch 0001 Iter 1566 / 2325 | Time 0.1s | Iter Loss 0.3771 | Iter Mean Loss 0.5081\n",
      "2025-11-10 13:45:35,708 - root - INFO - CF Training: Epoch 0001 Iter 1569 / 2325 | Time 0.1s | Iter Loss 0.3858 | Iter Mean Loss 0.5078\n",
      "2025-11-10 13:45:35,903 - root - INFO - CF Training: Epoch 0001 Iter 1572 / 2325 | Time 0.1s | Iter Loss 0.3813 | Iter Mean Loss 0.5076\n",
      "2025-11-10 13:45:36,098 - root - INFO - CF Training: Epoch 0001 Iter 1575 / 2325 | Time 0.1s | Iter Loss 0.3931 | Iter Mean Loss 0.5074\n",
      "2025-11-10 13:45:36,292 - root - INFO - CF Training: Epoch 0001 Iter 1578 / 2325 | Time 0.1s | Iter Loss 0.3852 | Iter Mean Loss 0.5072\n",
      "2025-11-10 13:45:36,488 - root - INFO - CF Training: Epoch 0001 Iter 1581 / 2325 | Time 0.1s | Iter Loss 0.3925 | Iter Mean Loss 0.5069\n",
      "2025-11-10 13:45:36,681 - root - INFO - CF Training: Epoch 0001 Iter 1584 / 2325 | Time 0.1s | Iter Loss 0.3970 | Iter Mean Loss 0.5067\n",
      "2025-11-10 13:45:36,875 - root - INFO - CF Training: Epoch 0001 Iter 1587 / 2325 | Time 0.1s | Iter Loss 0.3895 | Iter Mean Loss 0.5065\n",
      "2025-11-10 13:45:37,069 - root - INFO - CF Training: Epoch 0001 Iter 1590 / 2325 | Time 0.1s | Iter Loss 0.3885 | Iter Mean Loss 0.5063\n",
      "2025-11-10 13:45:37,262 - root - INFO - CF Training: Epoch 0001 Iter 1593 / 2325 | Time 0.1s | Iter Loss 0.4116 | Iter Mean Loss 0.5061\n",
      "2025-11-10 13:45:37,456 - root - INFO - CF Training: Epoch 0001 Iter 1596 / 2325 | Time 0.1s | Iter Loss 0.3909 | Iter Mean Loss 0.5059\n",
      "2025-11-10 13:45:37,651 - root - INFO - CF Training: Epoch 0001 Iter 1599 / 2325 | Time 0.1s | Iter Loss 0.4150 | Iter Mean Loss 0.5057\n",
      "2025-11-10 13:45:37,847 - root - INFO - CF Training: Epoch 0001 Iter 1602 / 2325 | Time 0.1s | Iter Loss 0.3979 | Iter Mean Loss 0.5055\n",
      "2025-11-10 13:45:38,040 - root - INFO - CF Training: Epoch 0001 Iter 1605 / 2325 | Time 0.1s | Iter Loss 0.3756 | Iter Mean Loss 0.5052\n",
      "2025-11-10 13:45:38,234 - root - INFO - CF Training: Epoch 0001 Iter 1608 / 2325 | Time 0.1s | Iter Loss 0.3994 | Iter Mean Loss 0.5050\n",
      "2025-11-10 13:45:38,429 - root - INFO - CF Training: Epoch 0001 Iter 1611 / 2325 | Time 0.1s | Iter Loss 0.3944 | Iter Mean Loss 0.5048\n",
      "2025-11-10 13:45:38,622 - root - INFO - CF Training: Epoch 0001 Iter 1614 / 2325 | Time 0.1s | Iter Loss 0.3855 | Iter Mean Loss 0.5046\n",
      "2025-11-10 13:45:38,817 - root - INFO - CF Training: Epoch 0001 Iter 1617 / 2325 | Time 0.1s | Iter Loss 0.3971 | Iter Mean Loss 0.5044\n",
      "2025-11-10 13:45:39,011 - root - INFO - CF Training: Epoch 0001 Iter 1620 / 2325 | Time 0.1s | Iter Loss 0.3831 | Iter Mean Loss 0.5042\n",
      "2025-11-10 13:45:39,205 - root - INFO - CF Training: Epoch 0001 Iter 1623 / 2325 | Time 0.1s | Iter Loss 0.3972 | Iter Mean Loss 0.5040\n",
      "2025-11-10 13:45:39,399 - root - INFO - CF Training: Epoch 0001 Iter 1626 / 2325 | Time 0.1s | Iter Loss 0.4069 | Iter Mean Loss 0.5037\n",
      "2025-11-10 13:45:39,592 - root - INFO - CF Training: Epoch 0001 Iter 1629 / 2325 | Time 0.1s | Iter Loss 0.3889 | Iter Mean Loss 0.5035\n",
      "2025-11-10 13:45:39,786 - root - INFO - CF Training: Epoch 0001 Iter 1632 / 2325 | Time 0.1s | Iter Loss 0.3598 | Iter Mean Loss 0.5033\n",
      "2025-11-10 13:45:39,980 - root - INFO - CF Training: Epoch 0001 Iter 1635 / 2325 | Time 0.1s | Iter Loss 0.4004 | Iter Mean Loss 0.5031\n",
      "2025-11-10 13:45:40,173 - root - INFO - CF Training: Epoch 0001 Iter 1638 / 2325 | Time 0.1s | Iter Loss 0.3728 | Iter Mean Loss 0.5029\n",
      "2025-11-10 13:45:40,370 - root - INFO - CF Training: Epoch 0001 Iter 1641 / 2325 | Time 0.1s | Iter Loss 0.3751 | Iter Mean Loss 0.5027\n",
      "2025-11-10 13:45:40,564 - root - INFO - CF Training: Epoch 0001 Iter 1644 / 2325 | Time 0.1s | Iter Loss 0.4174 | Iter Mean Loss 0.5025\n",
      "2025-11-10 13:45:40,758 - root - INFO - CF Training: Epoch 0001 Iter 1647 / 2325 | Time 0.1s | Iter Loss 0.3777 | Iter Mean Loss 0.5022\n",
      "2025-11-10 13:45:40,952 - root - INFO - CF Training: Epoch 0001 Iter 1650 / 2325 | Time 0.1s | Iter Loss 0.3965 | Iter Mean Loss 0.5020\n",
      "2025-11-10 13:45:41,146 - root - INFO - CF Training: Epoch 0001 Iter 1653 / 2325 | Time 0.1s | Iter Loss 0.4027 | Iter Mean Loss 0.5018\n",
      "2025-11-10 13:45:41,340 - root - INFO - CF Training: Epoch 0001 Iter 1656 / 2325 | Time 0.1s | Iter Loss 0.3770 | Iter Mean Loss 0.5016\n",
      "2025-11-10 13:45:41,535 - root - INFO - CF Training: Epoch 0001 Iter 1659 / 2325 | Time 0.1s | Iter Loss 0.3880 | Iter Mean Loss 0.5014\n",
      "2025-11-10 13:45:41,729 - root - INFO - CF Training: Epoch 0001 Iter 1662 / 2325 | Time 0.1s | Iter Loss 0.3644 | Iter Mean Loss 0.5012\n",
      "2025-11-10 13:45:41,926 - root - INFO - CF Training: Epoch 0001 Iter 1665 / 2325 | Time 0.1s | Iter Loss 0.3846 | Iter Mean Loss 0.5010\n",
      "2025-11-10 13:45:42,121 - root - INFO - CF Training: Epoch 0001 Iter 1668 / 2325 | Time 0.1s | Iter Loss 0.3656 | Iter Mean Loss 0.5008\n",
      "2025-11-10 13:45:42,314 - root - INFO - CF Training: Epoch 0001 Iter 1671 / 2325 | Time 0.1s | Iter Loss 0.3735 | Iter Mean Loss 0.5006\n",
      "2025-11-10 13:45:42,510 - root - INFO - CF Training: Epoch 0001 Iter 1674 / 2325 | Time 0.1s | Iter Loss 0.3821 | Iter Mean Loss 0.5004\n",
      "2025-11-10 13:45:42,707 - root - INFO - CF Training: Epoch 0001 Iter 1677 / 2325 | Time 0.1s | Iter Loss 0.3823 | Iter Mean Loss 0.5002\n",
      "2025-11-10 13:45:42,901 - root - INFO - CF Training: Epoch 0001 Iter 1680 / 2325 | Time 0.1s | Iter Loss 0.3809 | Iter Mean Loss 0.4999\n",
      "2025-11-10 13:45:43,095 - root - INFO - CF Training: Epoch 0001 Iter 1683 / 2325 | Time 0.1s | Iter Loss 0.3883 | Iter Mean Loss 0.4997\n",
      "2025-11-10 13:45:43,289 - root - INFO - CF Training: Epoch 0001 Iter 1686 / 2325 | Time 0.1s | Iter Loss 0.3835 | Iter Mean Loss 0.4995\n",
      "2025-11-10 13:45:43,483 - root - INFO - CF Training: Epoch 0001 Iter 1689 / 2325 | Time 0.1s | Iter Loss 0.3857 | Iter Mean Loss 0.4993\n",
      "2025-11-10 13:45:43,676 - root - INFO - CF Training: Epoch 0001 Iter 1692 / 2325 | Time 0.1s | Iter Loss 0.3920 | Iter Mean Loss 0.4992\n",
      "2025-11-10 13:45:43,870 - root - INFO - CF Training: Epoch 0001 Iter 1695 / 2325 | Time 0.1s | Iter Loss 0.3739 | Iter Mean Loss 0.4990\n",
      "2025-11-10 13:45:44,064 - root - INFO - CF Training: Epoch 0001 Iter 1698 / 2325 | Time 0.1s | Iter Loss 0.3894 | Iter Mean Loss 0.4987\n",
      "2025-11-10 13:45:44,258 - root - INFO - CF Training: Epoch 0001 Iter 1701 / 2325 | Time 0.1s | Iter Loss 0.3890 | Iter Mean Loss 0.4985\n",
      "2025-11-10 13:45:44,451 - root - INFO - CF Training: Epoch 0001 Iter 1704 / 2325 | Time 0.1s | Iter Loss 0.3923 | Iter Mean Loss 0.4983\n",
      "2025-11-10 13:45:44,647 - root - INFO - CF Training: Epoch 0001 Iter 1707 / 2325 | Time 0.1s | Iter Loss 0.3990 | Iter Mean Loss 0.4982\n",
      "2025-11-10 13:45:44,841 - root - INFO - CF Training: Epoch 0001 Iter 1710 / 2325 | Time 0.1s | Iter Loss 0.3763 | Iter Mean Loss 0.4980\n",
      "2025-11-10 13:45:45,034 - root - INFO - CF Training: Epoch 0001 Iter 1713 / 2325 | Time 0.1s | Iter Loss 0.3982 | Iter Mean Loss 0.4978\n",
      "2025-11-10 13:45:45,228 - root - INFO - CF Training: Epoch 0001 Iter 1716 / 2325 | Time 0.1s | Iter Loss 0.3827 | Iter Mean Loss 0.4975\n",
      "2025-11-10 13:45:45,425 - root - INFO - CF Training: Epoch 0001 Iter 1719 / 2325 | Time 0.1s | Iter Loss 0.3912 | Iter Mean Loss 0.4974\n",
      "2025-11-10 13:45:45,622 - root - INFO - CF Training: Epoch 0001 Iter 1722 / 2325 | Time 0.1s | Iter Loss 0.3872 | Iter Mean Loss 0.4972\n",
      "2025-11-10 13:45:45,817 - root - INFO - CF Training: Epoch 0001 Iter 1725 / 2325 | Time 0.1s | Iter Loss 0.3710 | Iter Mean Loss 0.4970\n",
      "2025-11-10 13:45:46,010 - root - INFO - CF Training: Epoch 0001 Iter 1728 / 2325 | Time 0.1s | Iter Loss 0.3829 | Iter Mean Loss 0.4968\n",
      "2025-11-10 13:45:46,205 - root - INFO - CF Training: Epoch 0001 Iter 1731 / 2325 | Time 0.1s | Iter Loss 0.3761 | Iter Mean Loss 0.4966\n",
      "2025-11-10 13:45:46,399 - root - INFO - CF Training: Epoch 0001 Iter 1734 / 2325 | Time 0.1s | Iter Loss 0.3602 | Iter Mean Loss 0.4963\n",
      "2025-11-10 13:45:46,593 - root - INFO - CF Training: Epoch 0001 Iter 1737 / 2325 | Time 0.1s | Iter Loss 0.3805 | Iter Mean Loss 0.4961\n",
      "2025-11-10 13:45:46,786 - root - INFO - CF Training: Epoch 0001 Iter 1740 / 2325 | Time 0.1s | Iter Loss 0.3698 | Iter Mean Loss 0.4960\n",
      "2025-11-10 13:45:46,980 - root - INFO - CF Training: Epoch 0001 Iter 1743 / 2325 | Time 0.1s | Iter Loss 0.3654 | Iter Mean Loss 0.4958\n",
      "2025-11-10 13:45:47,174 - root - INFO - CF Training: Epoch 0001 Iter 1746 / 2325 | Time 0.1s | Iter Loss 0.3897 | Iter Mean Loss 0.4956\n",
      "2025-11-10 13:45:47,368 - root - INFO - CF Training: Epoch 0001 Iter 1749 / 2325 | Time 0.1s | Iter Loss 0.3828 | Iter Mean Loss 0.4954\n",
      "2025-11-10 13:45:47,562 - root - INFO - CF Training: Epoch 0001 Iter 1752 / 2325 | Time 0.1s | Iter Loss 0.3681 | Iter Mean Loss 0.4952\n",
      "2025-11-10 13:45:47,756 - root - INFO - CF Training: Epoch 0001 Iter 1755 / 2325 | Time 0.1s | Iter Loss 0.3708 | Iter Mean Loss 0.4950\n",
      "2025-11-10 13:45:47,950 - root - INFO - CF Training: Epoch 0001 Iter 1758 / 2325 | Time 0.1s | Iter Loss 0.3830 | Iter Mean Loss 0.4948\n",
      "2025-11-10 13:45:48,145 - root - INFO - CF Training: Epoch 0001 Iter 1761 / 2325 | Time 0.1s | Iter Loss 0.3867 | Iter Mean Loss 0.4946\n",
      "2025-11-10 13:45:48,339 - root - INFO - CF Training: Epoch 0001 Iter 1764 / 2325 | Time 0.1s | Iter Loss 0.3790 | Iter Mean Loss 0.4944\n",
      "2025-11-10 13:45:48,532 - root - INFO - CF Training: Epoch 0001 Iter 1767 / 2325 | Time 0.1s | Iter Loss 0.3792 | Iter Mean Loss 0.4942\n",
      "2025-11-10 13:45:48,726 - root - INFO - CF Training: Epoch 0001 Iter 1770 / 2325 | Time 0.1s | Iter Loss 0.3877 | Iter Mean Loss 0.4940\n",
      "2025-11-10 13:45:48,920 - root - INFO - CF Training: Epoch 0001 Iter 1773 / 2325 | Time 0.1s | Iter Loss 0.3685 | Iter Mean Loss 0.4938\n",
      "2025-11-10 13:45:49,114 - root - INFO - CF Training: Epoch 0001 Iter 1776 / 2325 | Time 0.1s | Iter Loss 0.3817 | Iter Mean Loss 0.4936\n",
      "2025-11-10 13:45:49,309 - root - INFO - CF Training: Epoch 0001 Iter 1779 / 2325 | Time 0.1s | Iter Loss 0.3757 | Iter Mean Loss 0.4934\n",
      "2025-11-10 13:45:49,503 - root - INFO - CF Training: Epoch 0001 Iter 1782 / 2325 | Time 0.1s | Iter Loss 0.3882 | Iter Mean Loss 0.4932\n",
      "2025-11-10 13:45:49,698 - root - INFO - CF Training: Epoch 0001 Iter 1785 / 2325 | Time 0.1s | Iter Loss 0.3876 | Iter Mean Loss 0.4930\n",
      "2025-11-10 13:45:49,892 - root - INFO - CF Training: Epoch 0001 Iter 1788 / 2325 | Time 0.1s | Iter Loss 0.3902 | Iter Mean Loss 0.4929\n",
      "2025-11-10 13:45:50,085 - root - INFO - CF Training: Epoch 0001 Iter 1791 / 2325 | Time 0.1s | Iter Loss 0.3755 | Iter Mean Loss 0.4927\n",
      "2025-11-10 13:45:50,279 - root - INFO - CF Training: Epoch 0001 Iter 1794 / 2325 | Time 0.1s | Iter Loss 0.3813 | Iter Mean Loss 0.4925\n",
      "2025-11-10 13:45:50,473 - root - INFO - CF Training: Epoch 0001 Iter 1797 / 2325 | Time 0.1s | Iter Loss 0.3644 | Iter Mean Loss 0.4923\n",
      "2025-11-10 13:45:50,668 - root - INFO - CF Training: Epoch 0001 Iter 1800 / 2325 | Time 0.1s | Iter Loss 0.3887 | Iter Mean Loss 0.4921\n",
      "2025-11-10 13:45:50,861 - root - INFO - CF Training: Epoch 0001 Iter 1803 / 2325 | Time 0.1s | Iter Loss 0.3748 | Iter Mean Loss 0.4919\n",
      "2025-11-10 13:45:51,057 - root - INFO - CF Training: Epoch 0001 Iter 1806 / 2325 | Time 0.1s | Iter Loss 0.3952 | Iter Mean Loss 0.4917\n",
      "2025-11-10 13:45:51,253 - root - INFO - CF Training: Epoch 0001 Iter 1809 / 2325 | Time 0.1s | Iter Loss 0.3693 | Iter Mean Loss 0.4915\n",
      "2025-11-10 13:45:51,446 - root - INFO - CF Training: Epoch 0001 Iter 1812 / 2325 | Time 0.1s | Iter Loss 0.3961 | Iter Mean Loss 0.4913\n",
      "2025-11-10 13:45:51,642 - root - INFO - CF Training: Epoch 0001 Iter 1815 / 2325 | Time 0.1s | Iter Loss 0.3792 | Iter Mean Loss 0.4911\n",
      "2025-11-10 13:45:51,836 - root - INFO - CF Training: Epoch 0001 Iter 1818 / 2325 | Time 0.1s | Iter Loss 0.3771 | Iter Mean Loss 0.4910\n",
      "2025-11-10 13:45:52,029 - root - INFO - CF Training: Epoch 0001 Iter 1821 / 2325 | Time 0.1s | Iter Loss 0.3801 | Iter Mean Loss 0.4908\n",
      "2025-11-10 13:45:52,224 - root - INFO - CF Training: Epoch 0001 Iter 1824 / 2325 | Time 0.1s | Iter Loss 0.3867 | Iter Mean Loss 0.4906\n",
      "2025-11-10 13:45:52,418 - root - INFO - CF Training: Epoch 0001 Iter 1827 / 2325 | Time 0.1s | Iter Loss 0.3819 | Iter Mean Loss 0.4904\n",
      "2025-11-10 13:45:52,612 - root - INFO - CF Training: Epoch 0001 Iter 1830 / 2325 | Time 0.1s | Iter Loss 0.3879 | Iter Mean Loss 0.4902\n",
      "2025-11-10 13:45:52,806 - root - INFO - CF Training: Epoch 0001 Iter 1833 / 2325 | Time 0.1s | Iter Loss 0.3519 | Iter Mean Loss 0.4900\n",
      "2025-11-10 13:45:52,999 - root - INFO - CF Training: Epoch 0001 Iter 1836 / 2325 | Time 0.1s | Iter Loss 0.3847 | Iter Mean Loss 0.4899\n",
      "2025-11-10 13:45:53,193 - root - INFO - CF Training: Epoch 0001 Iter 1839 / 2325 | Time 0.1s | Iter Loss 0.3753 | Iter Mean Loss 0.4897\n",
      "2025-11-10 13:45:53,387 - root - INFO - CF Training: Epoch 0001 Iter 1842 / 2325 | Time 0.1s | Iter Loss 0.3700 | Iter Mean Loss 0.4895\n",
      "2025-11-10 13:45:53,581 - root - INFO - CF Training: Epoch 0001 Iter 1845 / 2325 | Time 0.1s | Iter Loss 0.3815 | Iter Mean Loss 0.4893\n",
      "2025-11-10 13:45:53,775 - root - INFO - CF Training: Epoch 0001 Iter 1848 / 2325 | Time 0.1s | Iter Loss 0.3631 | Iter Mean Loss 0.4891\n",
      "2025-11-10 13:45:53,969 - root - INFO - CF Training: Epoch 0001 Iter 1851 / 2325 | Time 0.1s | Iter Loss 0.3745 | Iter Mean Loss 0.4890\n",
      "2025-11-10 13:45:54,164 - root - INFO - CF Training: Epoch 0001 Iter 1854 / 2325 | Time 0.1s | Iter Loss 0.3905 | Iter Mean Loss 0.4888\n",
      "2025-11-10 13:45:54,358 - root - INFO - CF Training: Epoch 0001 Iter 1857 / 2325 | Time 0.1s | Iter Loss 0.3625 | Iter Mean Loss 0.4886\n",
      "2025-11-10 13:45:54,552 - root - INFO - CF Training: Epoch 0001 Iter 1860 / 2325 | Time 0.1s | Iter Loss 0.3562 | Iter Mean Loss 0.4884\n",
      "2025-11-10 13:45:54,747 - root - INFO - CF Training: Epoch 0001 Iter 1863 / 2325 | Time 0.1s | Iter Loss 0.3754 | Iter Mean Loss 0.4882\n",
      "2025-11-10 13:45:54,941 - root - INFO - CF Training: Epoch 0001 Iter 1866 / 2325 | Time 0.1s | Iter Loss 0.3762 | Iter Mean Loss 0.4880\n",
      "2025-11-10 13:45:55,138 - root - INFO - CF Training: Epoch 0001 Iter 1869 / 2325 | Time 0.1s | Iter Loss 0.3624 | Iter Mean Loss 0.4878\n",
      "2025-11-10 13:45:55,333 - root - INFO - CF Training: Epoch 0001 Iter 1872 / 2325 | Time 0.1s | Iter Loss 0.4074 | Iter Mean Loss 0.4877\n",
      "2025-11-10 13:45:55,527 - root - INFO - CF Training: Epoch 0001 Iter 1875 / 2325 | Time 0.1s | Iter Loss 0.3846 | Iter Mean Loss 0.4875\n",
      "2025-11-10 13:45:55,721 - root - INFO - CF Training: Epoch 0001 Iter 1878 / 2325 | Time 0.1s | Iter Loss 0.3725 | Iter Mean Loss 0.4873\n",
      "2025-11-10 13:45:55,914 - root - INFO - CF Training: Epoch 0001 Iter 1881 / 2325 | Time 0.1s | Iter Loss 0.3674 | Iter Mean Loss 0.4871\n",
      "2025-11-10 13:45:56,108 - root - INFO - CF Training: Epoch 0001 Iter 1884 / 2325 | Time 0.1s | Iter Loss 0.3587 | Iter Mean Loss 0.4869\n",
      "2025-11-10 13:45:56,303 - root - INFO - CF Training: Epoch 0001 Iter 1887 / 2325 | Time 0.1s | Iter Loss 0.3642 | Iter Mean Loss 0.4867\n",
      "2025-11-10 13:45:56,497 - root - INFO - CF Training: Epoch 0001 Iter 1890 / 2325 | Time 0.1s | Iter Loss 0.3760 | Iter Mean Loss 0.4866\n",
      "2025-11-10 13:45:56,691 - root - INFO - CF Training: Epoch 0001 Iter 1893 / 2325 | Time 0.1s | Iter Loss 0.3489 | Iter Mean Loss 0.4864\n",
      "2025-11-10 13:45:56,885 - root - INFO - CF Training: Epoch 0001 Iter 1896 / 2325 | Time 0.1s | Iter Loss 0.3783 | Iter Mean Loss 0.4862\n",
      "2025-11-10 13:45:57,080 - root - INFO - CF Training: Epoch 0001 Iter 1899 / 2325 | Time 0.1s | Iter Loss 0.3924 | Iter Mean Loss 0.4860\n",
      "2025-11-10 13:45:57,274 - root - INFO - CF Training: Epoch 0001 Iter 1902 / 2325 | Time 0.1s | Iter Loss 0.3801 | Iter Mean Loss 0.4859\n",
      "2025-11-10 13:45:57,468 - root - INFO - CF Training: Epoch 0001 Iter 1905 / 2325 | Time 0.1s | Iter Loss 0.3845 | Iter Mean Loss 0.4857\n",
      "2025-11-10 13:45:57,662 - root - INFO - CF Training: Epoch 0001 Iter 1908 / 2325 | Time 0.1s | Iter Loss 0.3650 | Iter Mean Loss 0.4855\n",
      "2025-11-10 13:45:57,856 - root - INFO - CF Training: Epoch 0001 Iter 1911 / 2325 | Time 0.1s | Iter Loss 0.3657 | Iter Mean Loss 0.4853\n",
      "2025-11-10 13:45:58,051 - root - INFO - CF Training: Epoch 0001 Iter 1914 / 2325 | Time 0.1s | Iter Loss 0.3480 | Iter Mean Loss 0.4851\n",
      "2025-11-10 13:45:58,244 - root - INFO - CF Training: Epoch 0001 Iter 1917 / 2325 | Time 0.1s | Iter Loss 0.3714 | Iter Mean Loss 0.4849\n",
      "2025-11-10 13:45:58,440 - root - INFO - CF Training: Epoch 0001 Iter 1920 / 2325 | Time 0.1s | Iter Loss 0.3725 | Iter Mean Loss 0.4848\n",
      "2025-11-10 13:45:58,634 - root - INFO - CF Training: Epoch 0001 Iter 1923 / 2325 | Time 0.1s | Iter Loss 0.3736 | Iter Mean Loss 0.4846\n",
      "2025-11-10 13:45:58,828 - root - INFO - CF Training: Epoch 0001 Iter 1926 / 2325 | Time 0.1s | Iter Loss 0.3476 | Iter Mean Loss 0.4844\n",
      "2025-11-10 13:45:59,022 - root - INFO - CF Training: Epoch 0001 Iter 1929 / 2325 | Time 0.1s | Iter Loss 0.3609 | Iter Mean Loss 0.4842\n",
      "2025-11-10 13:45:59,217 - root - INFO - CF Training: Epoch 0001 Iter 1932 / 2325 | Time 0.1s | Iter Loss 0.3551 | Iter Mean Loss 0.4840\n",
      "2025-11-10 13:45:59,411 - root - INFO - CF Training: Epoch 0001 Iter 1935 / 2325 | Time 0.1s | Iter Loss 0.3621 | Iter Mean Loss 0.4839\n",
      "2025-11-10 13:45:59,604 - root - INFO - CF Training: Epoch 0001 Iter 1938 / 2325 | Time 0.1s | Iter Loss 0.3570 | Iter Mean Loss 0.4837\n",
      "2025-11-10 13:45:59,799 - root - INFO - CF Training: Epoch 0001 Iter 1941 / 2325 | Time 0.1s | Iter Loss 0.3691 | Iter Mean Loss 0.4835\n",
      "2025-11-10 13:45:59,993 - root - INFO - CF Training: Epoch 0001 Iter 1944 / 2325 | Time 0.1s | Iter Loss 0.3677 | Iter Mean Loss 0.4834\n",
      "2025-11-10 13:46:00,186 - root - INFO - CF Training: Epoch 0001 Iter 1947 / 2325 | Time 0.1s | Iter Loss 0.3631 | Iter Mean Loss 0.4832\n",
      "2025-11-10 13:46:00,380 - root - INFO - CF Training: Epoch 0001 Iter 1950 / 2325 | Time 0.1s | Iter Loss 0.3617 | Iter Mean Loss 0.4830\n",
      "2025-11-10 13:46:00,575 - root - INFO - CF Training: Epoch 0001 Iter 1953 / 2325 | Time 0.1s | Iter Loss 0.3739 | Iter Mean Loss 0.4828\n",
      "2025-11-10 13:46:00,768 - root - INFO - CF Training: Epoch 0001 Iter 1956 / 2325 | Time 0.1s | Iter Loss 0.3682 | Iter Mean Loss 0.4827\n",
      "2025-11-10 13:46:00,963 - root - INFO - CF Training: Epoch 0001 Iter 1959 / 2325 | Time 0.1s | Iter Loss 0.3668 | Iter Mean Loss 0.4825\n",
      "2025-11-10 13:46:01,157 - root - INFO - CF Training: Epoch 0001 Iter 1962 / 2325 | Time 0.1s | Iter Loss 0.3547 | Iter Mean Loss 0.4823\n",
      "2025-11-10 13:46:01,351 - root - INFO - CF Training: Epoch 0001 Iter 1965 / 2325 | Time 0.1s | Iter Loss 0.3643 | Iter Mean Loss 0.4821\n",
      "2025-11-10 13:46:01,545 - root - INFO - CF Training: Epoch 0001 Iter 1968 / 2325 | Time 0.1s | Iter Loss 0.3540 | Iter Mean Loss 0.4819\n",
      "2025-11-10 13:46:01,739 - root - INFO - CF Training: Epoch 0001 Iter 1971 / 2325 | Time 0.1s | Iter Loss 0.3714 | Iter Mean Loss 0.4818\n",
      "2025-11-10 13:46:01,933 - root - INFO - CF Training: Epoch 0001 Iter 1974 / 2325 | Time 0.1s | Iter Loss 0.3701 | Iter Mean Loss 0.4816\n",
      "2025-11-10 13:46:02,127 - root - INFO - CF Training: Epoch 0001 Iter 1977 / 2325 | Time 0.1s | Iter Loss 0.3603 | Iter Mean Loss 0.4814\n",
      "2025-11-10 13:46:02,320 - root - INFO - CF Training: Epoch 0001 Iter 1980 / 2325 | Time 0.1s | Iter Loss 0.3622 | Iter Mean Loss 0.4812\n",
      "2025-11-10 13:46:02,514 - root - INFO - CF Training: Epoch 0001 Iter 1983 / 2325 | Time 0.1s | Iter Loss 0.3695 | Iter Mean Loss 0.4811\n",
      "2025-11-10 13:46:02,708 - root - INFO - CF Training: Epoch 0001 Iter 1986 / 2325 | Time 0.1s | Iter Loss 0.3785 | Iter Mean Loss 0.4809\n",
      "2025-11-10 13:46:02,902 - root - INFO - CF Training: Epoch 0001 Iter 1989 / 2325 | Time 0.1s | Iter Loss 0.3702 | Iter Mean Loss 0.4807\n",
      "2025-11-10 13:46:03,099 - root - INFO - CF Training: Epoch 0001 Iter 1992 / 2325 | Time 0.1s | Iter Loss 0.3717 | Iter Mean Loss 0.4805\n",
      "2025-11-10 13:46:03,293 - root - INFO - CF Training: Epoch 0001 Iter 1995 / 2325 | Time 0.1s | Iter Loss 0.3803 | Iter Mean Loss 0.4804\n",
      "2025-11-10 13:46:03,490 - root - INFO - CF Training: Epoch 0001 Iter 1998 / 2325 | Time 0.1s | Iter Loss 0.3699 | Iter Mean Loss 0.4802\n",
      "2025-11-10 13:46:03,684 - root - INFO - CF Training: Epoch 0001 Iter 2001 / 2325 | Time 0.1s | Iter Loss 0.3769 | Iter Mean Loss 0.4801\n",
      "2025-11-10 13:46:03,879 - root - INFO - CF Training: Epoch 0001 Iter 2004 / 2325 | Time 0.1s | Iter Loss 0.3632 | Iter Mean Loss 0.4799\n",
      "2025-11-10 13:46:04,075 - root - INFO - CF Training: Epoch 0001 Iter 2007 / 2325 | Time 0.1s | Iter Loss 0.3570 | Iter Mean Loss 0.4797\n",
      "2025-11-10 13:46:04,270 - root - INFO - CF Training: Epoch 0001 Iter 2010 / 2325 | Time 0.1s | Iter Loss 0.3633 | Iter Mean Loss 0.4795\n",
      "2025-11-10 13:46:04,463 - root - INFO - CF Training: Epoch 0001 Iter 2013 / 2325 | Time 0.1s | Iter Loss 0.3749 | Iter Mean Loss 0.4794\n",
      "2025-11-10 13:46:04,659 - root - INFO - CF Training: Epoch 0001 Iter 2016 / 2325 | Time 0.1s | Iter Loss 0.3763 | Iter Mean Loss 0.4792\n",
      "2025-11-10 13:46:04,852 - root - INFO - CF Training: Epoch 0001 Iter 2019 / 2325 | Time 0.1s | Iter Loss 0.3722 | Iter Mean Loss 0.4791\n",
      "2025-11-10 13:46:05,048 - root - INFO - CF Training: Epoch 0001 Iter 2022 / 2325 | Time 0.1s | Iter Loss 0.3772 | Iter Mean Loss 0.4789\n",
      "2025-11-10 13:46:05,243 - root - INFO - CF Training: Epoch 0001 Iter 2025 / 2325 | Time 0.1s | Iter Loss 0.3637 | Iter Mean Loss 0.4787\n",
      "2025-11-10 13:46:05,437 - root - INFO - CF Training: Epoch 0001 Iter 2028 / 2325 | Time 0.1s | Iter Loss 0.3600 | Iter Mean Loss 0.4786\n",
      "2025-11-10 13:46:05,630 - root - INFO - CF Training: Epoch 0001 Iter 2031 / 2325 | Time 0.1s | Iter Loss 0.3609 | Iter Mean Loss 0.4784\n",
      "2025-11-10 13:46:05,824 - root - INFO - CF Training: Epoch 0001 Iter 2034 / 2325 | Time 0.1s | Iter Loss 0.3515 | Iter Mean Loss 0.4782\n",
      "2025-11-10 13:46:06,018 - root - INFO - CF Training: Epoch 0001 Iter 2037 / 2325 | Time 0.1s | Iter Loss 0.3624 | Iter Mean Loss 0.4780\n",
      "2025-11-10 13:46:06,212 - root - INFO - CF Training: Epoch 0001 Iter 2040 / 2325 | Time 0.1s | Iter Loss 0.3682 | Iter Mean Loss 0.4779\n",
      "2025-11-10 13:46:06,407 - root - INFO - CF Training: Epoch 0001 Iter 2043 / 2325 | Time 0.1s | Iter Loss 0.3494 | Iter Mean Loss 0.4777\n",
      "2025-11-10 13:46:06,601 - root - INFO - CF Training: Epoch 0001 Iter 2046 / 2325 | Time 0.1s | Iter Loss 0.3649 | Iter Mean Loss 0.4775\n",
      "2025-11-10 13:46:06,795 - root - INFO - CF Training: Epoch 0001 Iter 2049 / 2325 | Time 0.1s | Iter Loss 0.3720 | Iter Mean Loss 0.4774\n",
      "2025-11-10 13:46:06,990 - root - INFO - CF Training: Epoch 0001 Iter 2052 / 2325 | Time 0.1s | Iter Loss 0.3503 | Iter Mean Loss 0.4772\n",
      "2025-11-10 13:46:07,184 - root - INFO - CF Training: Epoch 0001 Iter 2055 / 2325 | Time 0.1s | Iter Loss 0.3767 | Iter Mean Loss 0.4771\n",
      "2025-11-10 13:46:07,378 - root - INFO - CF Training: Epoch 0001 Iter 2058 / 2325 | Time 0.1s | Iter Loss 0.3808 | Iter Mean Loss 0.4769\n",
      "2025-11-10 13:46:07,574 - root - INFO - CF Training: Epoch 0001 Iter 2061 / 2325 | Time 0.1s | Iter Loss 0.3586 | Iter Mean Loss 0.4767\n",
      "2025-11-10 13:46:07,768 - root - INFO - CF Training: Epoch 0001 Iter 2064 / 2325 | Time 0.1s | Iter Loss 0.3640 | Iter Mean Loss 0.4766\n",
      "2025-11-10 13:46:07,962 - root - INFO - CF Training: Epoch 0001 Iter 2067 / 2325 | Time 0.1s | Iter Loss 0.3787 | Iter Mean Loss 0.4764\n",
      "2025-11-10 13:46:08,155 - root - INFO - CF Training: Epoch 0001 Iter 2070 / 2325 | Time 0.1s | Iter Loss 0.3894 | Iter Mean Loss 0.4763\n",
      "2025-11-10 13:46:08,350 - root - INFO - CF Training: Epoch 0001 Iter 2073 / 2325 | Time 0.1s | Iter Loss 0.3674 | Iter Mean Loss 0.4761\n",
      "2025-11-10 13:46:08,545 - root - INFO - CF Training: Epoch 0001 Iter 2076 / 2325 | Time 0.1s | Iter Loss 0.3566 | Iter Mean Loss 0.4759\n",
      "2025-11-10 13:46:08,741 - root - INFO - CF Training: Epoch 0001 Iter 2079 / 2325 | Time 0.1s | Iter Loss 0.3710 | Iter Mean Loss 0.4758\n",
      "2025-11-10 13:46:08,937 - root - INFO - CF Training: Epoch 0001 Iter 2082 / 2325 | Time 0.1s | Iter Loss 0.3476 | Iter Mean Loss 0.4756\n",
      "2025-11-10 13:46:09,131 - root - INFO - CF Training: Epoch 0001 Iter 2085 / 2325 | Time 0.1s | Iter Loss 0.3644 | Iter Mean Loss 0.4755\n",
      "2025-11-10 13:46:09,325 - root - INFO - CF Training: Epoch 0001 Iter 2088 / 2325 | Time 0.1s | Iter Loss 0.3637 | Iter Mean Loss 0.4753\n",
      "2025-11-10 13:46:09,518 - root - INFO - CF Training: Epoch 0001 Iter 2091 / 2325 | Time 0.1s | Iter Loss 0.3514 | Iter Mean Loss 0.4751\n",
      "2025-11-10 13:46:09,712 - root - INFO - CF Training: Epoch 0001 Iter 2094 / 2325 | Time 0.1s | Iter Loss 0.3527 | Iter Mean Loss 0.4750\n",
      "2025-11-10 13:46:09,906 - root - INFO - CF Training: Epoch 0001 Iter 2097 / 2325 | Time 0.1s | Iter Loss 0.3623 | Iter Mean Loss 0.4748\n",
      "2025-11-10 13:46:10,100 - root - INFO - CF Training: Epoch 0001 Iter 2100 / 2325 | Time 0.1s | Iter Loss 0.3687 | Iter Mean Loss 0.4746\n",
      "2025-11-10 13:46:10,294 - root - INFO - CF Training: Epoch 0001 Iter 2103 / 2325 | Time 0.1s | Iter Loss 0.3650 | Iter Mean Loss 0.4745\n",
      "2025-11-10 13:46:10,488 - root - INFO - CF Training: Epoch 0001 Iter 2106 / 2325 | Time 0.1s | Iter Loss 0.3703 | Iter Mean Loss 0.4743\n",
      "2025-11-10 13:46:10,684 - root - INFO - CF Training: Epoch 0001 Iter 2109 / 2325 | Time 0.1s | Iter Loss 0.3935 | Iter Mean Loss 0.4742\n",
      "2025-11-10 13:46:10,878 - root - INFO - CF Training: Epoch 0001 Iter 2112 / 2325 | Time 0.1s | Iter Loss 0.3625 | Iter Mean Loss 0.4740\n",
      "2025-11-10 13:46:11,074 - root - INFO - CF Training: Epoch 0001 Iter 2115 / 2325 | Time 0.1s | Iter Loss 0.3564 | Iter Mean Loss 0.4739\n",
      "2025-11-10 13:46:11,268 - root - INFO - CF Training: Epoch 0001 Iter 2118 / 2325 | Time 0.1s | Iter Loss 0.3661 | Iter Mean Loss 0.4737\n",
      "2025-11-10 13:46:11,463 - root - INFO - CF Training: Epoch 0001 Iter 2121 / 2325 | Time 0.1s | Iter Loss 0.3690 | Iter Mean Loss 0.4736\n",
      "2025-11-10 13:46:11,657 - root - INFO - CF Training: Epoch 0001 Iter 2124 / 2325 | Time 0.1s | Iter Loss 0.3796 | Iter Mean Loss 0.4734\n",
      "2025-11-10 13:46:11,852 - root - INFO - CF Training: Epoch 0001 Iter 2127 / 2325 | Time 0.1s | Iter Loss 0.3509 | Iter Mean Loss 0.4732\n",
      "2025-11-10 13:46:12,046 - root - INFO - CF Training: Epoch 0001 Iter 2130 / 2325 | Time 0.1s | Iter Loss 0.3624 | Iter Mean Loss 0.4731\n",
      "2025-11-10 13:46:12,241 - root - INFO - CF Training: Epoch 0001 Iter 2133 / 2325 | Time 0.1s | Iter Loss 0.3654 | Iter Mean Loss 0.4729\n",
      "2025-11-10 13:46:12,436 - root - INFO - CF Training: Epoch 0001 Iter 2136 / 2325 | Time 0.1s | Iter Loss 0.3518 | Iter Mean Loss 0.4727\n",
      "2025-11-10 13:46:12,631 - root - INFO - CF Training: Epoch 0001 Iter 2139 / 2325 | Time 0.1s | Iter Loss 0.3488 | Iter Mean Loss 0.4726\n",
      "2025-11-10 13:46:12,826 - root - INFO - CF Training: Epoch 0001 Iter 2142 / 2325 | Time 0.1s | Iter Loss 0.3550 | Iter Mean Loss 0.4724\n",
      "2025-11-10 13:46:13,023 - root - INFO - CF Training: Epoch 0001 Iter 2145 / 2325 | Time 0.1s | Iter Loss 0.3703 | Iter Mean Loss 0.4723\n",
      "2025-11-10 13:46:13,217 - root - INFO - CF Training: Epoch 0001 Iter 2148 / 2325 | Time 0.1s | Iter Loss 0.3646 | Iter Mean Loss 0.4721\n",
      "2025-11-10 13:46:13,414 - root - INFO - CF Training: Epoch 0001 Iter 2151 / 2325 | Time 0.1s | Iter Loss 0.3683 | Iter Mean Loss 0.4720\n",
      "2025-11-10 13:46:13,609 - root - INFO - CF Training: Epoch 0001 Iter 2154 / 2325 | Time 0.1s | Iter Loss 0.3676 | Iter Mean Loss 0.4718\n",
      "2025-11-10 13:46:13,804 - root - INFO - CF Training: Epoch 0001 Iter 2157 / 2325 | Time 0.1s | Iter Loss 0.3512 | Iter Mean Loss 0.4716\n",
      "2025-11-10 13:46:14,000 - root - INFO - CF Training: Epoch 0001 Iter 2160 / 2325 | Time 0.1s | Iter Loss 0.3646 | Iter Mean Loss 0.4715\n",
      "2025-11-10 13:46:14,194 - root - INFO - CF Training: Epoch 0001 Iter 2163 / 2325 | Time 0.1s | Iter Loss 0.3719 | Iter Mean Loss 0.4714\n",
      "2025-11-10 13:46:14,388 - root - INFO - CF Training: Epoch 0001 Iter 2166 / 2325 | Time 0.1s | Iter Loss 0.3579 | Iter Mean Loss 0.4712\n",
      "2025-11-10 13:46:14,583 - root - INFO - CF Training: Epoch 0001 Iter 2169 / 2325 | Time 0.1s | Iter Loss 0.3608 | Iter Mean Loss 0.4710\n",
      "2025-11-10 13:46:14,777 - root - INFO - CF Training: Epoch 0001 Iter 2172 / 2325 | Time 0.1s | Iter Loss 0.3638 | Iter Mean Loss 0.4709\n",
      "2025-11-10 13:46:14,973 - root - INFO - CF Training: Epoch 0001 Iter 2175 / 2325 | Time 0.1s | Iter Loss 0.3701 | Iter Mean Loss 0.4708\n",
      "2025-11-10 13:46:15,167 - root - INFO - CF Training: Epoch 0001 Iter 2178 / 2325 | Time 0.1s | Iter Loss 0.3669 | Iter Mean Loss 0.4706\n",
      "2025-11-10 13:46:15,363 - root - INFO - CF Training: Epoch 0001 Iter 2181 / 2325 | Time 0.1s | Iter Loss 0.3785 | Iter Mean Loss 0.4705\n",
      "2025-11-10 13:46:15,558 - root - INFO - CF Training: Epoch 0001 Iter 2184 / 2325 | Time 0.1s | Iter Loss 0.3583 | Iter Mean Loss 0.4703\n",
      "2025-11-10 13:46:15,753 - root - INFO - CF Training: Epoch 0001 Iter 2187 / 2325 | Time 0.1s | Iter Loss 0.3552 | Iter Mean Loss 0.4702\n",
      "2025-11-10 13:46:15,947 - root - INFO - CF Training: Epoch 0001 Iter 2190 / 2325 | Time 0.1s | Iter Loss 0.3619 | Iter Mean Loss 0.4700\n",
      "2025-11-10 13:46:16,143 - root - INFO - CF Training: Epoch 0001 Iter 2193 / 2325 | Time 0.1s | Iter Loss 0.3506 | Iter Mean Loss 0.4699\n",
      "2025-11-10 13:46:16,337 - root - INFO - CF Training: Epoch 0001 Iter 2196 / 2325 | Time 0.1s | Iter Loss 0.3569 | Iter Mean Loss 0.4697\n",
      "2025-11-10 13:46:16,531 - root - INFO - CF Training: Epoch 0001 Iter 2199 / 2325 | Time 0.1s | Iter Loss 0.3503 | Iter Mean Loss 0.4696\n",
      "2025-11-10 13:46:16,725 - root - INFO - CF Training: Epoch 0001 Iter 2202 / 2325 | Time 0.1s | Iter Loss 0.3619 | Iter Mean Loss 0.4694\n",
      "2025-11-10 13:46:16,919 - root - INFO - CF Training: Epoch 0001 Iter 2205 / 2325 | Time 0.1s | Iter Loss 0.3746 | Iter Mean Loss 0.4693\n",
      "2025-11-10 13:46:17,113 - root - INFO - CF Training: Epoch 0001 Iter 2208 / 2325 | Time 0.1s | Iter Loss 0.3514 | Iter Mean Loss 0.4691\n",
      "2025-11-10 13:46:17,308 - root - INFO - CF Training: Epoch 0001 Iter 2211 / 2325 | Time 0.1s | Iter Loss 0.3604 | Iter Mean Loss 0.4690\n",
      "2025-11-10 13:46:17,504 - root - INFO - CF Training: Epoch 0001 Iter 2214 / 2325 | Time 0.1s | Iter Loss 0.3622 | Iter Mean Loss 0.4688\n",
      "2025-11-10 13:46:17,698 - root - INFO - CF Training: Epoch 0001 Iter 2217 / 2325 | Time 0.1s | Iter Loss 0.3475 | Iter Mean Loss 0.4687\n",
      "2025-11-10 13:46:17,892 - root - INFO - CF Training: Epoch 0001 Iter 2220 / 2325 | Time 0.1s | Iter Loss 0.3507 | Iter Mean Loss 0.4685\n",
      "2025-11-10 13:46:18,085 - root - INFO - CF Training: Epoch 0001 Iter 2223 / 2325 | Time 0.1s | Iter Loss 0.3698 | Iter Mean Loss 0.4684\n",
      "2025-11-10 13:46:18,280 - root - INFO - CF Training: Epoch 0001 Iter 2226 / 2325 | Time 0.1s | Iter Loss 0.3549 | Iter Mean Loss 0.4682\n",
      "2025-11-10 13:46:18,475 - root - INFO - CF Training: Epoch 0001 Iter 2229 / 2325 | Time 0.1s | Iter Loss 0.3807 | Iter Mean Loss 0.4681\n",
      "2025-11-10 13:46:18,669 - root - INFO - CF Training: Epoch 0001 Iter 2232 / 2325 | Time 0.1s | Iter Loss 0.3581 | Iter Mean Loss 0.4679\n",
      "2025-11-10 13:46:18,863 - root - INFO - CF Training: Epoch 0001 Iter 2235 / 2325 | Time 0.1s | Iter Loss 0.3558 | Iter Mean Loss 0.4678\n",
      "2025-11-10 13:46:19,057 - root - INFO - CF Training: Epoch 0001 Iter 2238 / 2325 | Time 0.1s | Iter Loss 0.3364 | Iter Mean Loss 0.4676\n",
      "2025-11-10 13:46:19,251 - root - INFO - CF Training: Epoch 0001 Iter 2241 / 2325 | Time 0.1s | Iter Loss 0.3740 | Iter Mean Loss 0.4675\n",
      "2025-11-10 13:46:19,446 - root - INFO - CF Training: Epoch 0001 Iter 2244 / 2325 | Time 0.1s | Iter Loss 0.3571 | Iter Mean Loss 0.4673\n",
      "2025-11-10 13:46:19,639 - root - INFO - CF Training: Epoch 0001 Iter 2247 / 2325 | Time 0.1s | Iter Loss 0.3602 | Iter Mean Loss 0.4672\n",
      "2025-11-10 13:46:19,833 - root - INFO - CF Training: Epoch 0001 Iter 2250 / 2325 | Time 0.1s | Iter Loss 0.3534 | Iter Mean Loss 0.4670\n",
      "2025-11-10 13:46:20,027 - root - INFO - CF Training: Epoch 0001 Iter 2253 / 2325 | Time 0.1s | Iter Loss 0.3585 | Iter Mean Loss 0.4669\n",
      "2025-11-10 13:46:20,222 - root - INFO - CF Training: Epoch 0001 Iter 2256 / 2325 | Time 0.1s | Iter Loss 0.3557 | Iter Mean Loss 0.4667\n",
      "2025-11-10 13:46:20,415 - root - INFO - CF Training: Epoch 0001 Iter 2259 / 2325 | Time 0.1s | Iter Loss 0.3614 | Iter Mean Loss 0.4666\n",
      "2025-11-10 13:46:20,609 - root - INFO - CF Training: Epoch 0001 Iter 2262 / 2325 | Time 0.1s | Iter Loss 0.3543 | Iter Mean Loss 0.4665\n",
      "2025-11-10 13:46:20,804 - root - INFO - CF Training: Epoch 0001 Iter 2265 / 2325 | Time 0.1s | Iter Loss 0.3610 | Iter Mean Loss 0.4663\n",
      "2025-11-10 13:46:20,997 - root - INFO - CF Training: Epoch 0001 Iter 2268 / 2325 | Time 0.1s | Iter Loss 0.3665 | Iter Mean Loss 0.4662\n",
      "2025-11-10 13:46:21,191 - root - INFO - CF Training: Epoch 0001 Iter 2271 / 2325 | Time 0.1s | Iter Loss 0.3468 | Iter Mean Loss 0.4660\n",
      "2025-11-10 13:46:21,386 - root - INFO - CF Training: Epoch 0001 Iter 2274 / 2325 | Time 0.1s | Iter Loss 0.3468 | Iter Mean Loss 0.4659\n",
      "2025-11-10 13:46:21,581 - root - INFO - CF Training: Epoch 0001 Iter 2277 / 2325 | Time 0.1s | Iter Loss 0.3670 | Iter Mean Loss 0.4657\n",
      "2025-11-10 13:46:21,775 - root - INFO - CF Training: Epoch 0001 Iter 2280 / 2325 | Time 0.1s | Iter Loss 0.3709 | Iter Mean Loss 0.4656\n",
      "2025-11-10 13:46:21,969 - root - INFO - CF Training: Epoch 0001 Iter 2283 / 2325 | Time 0.1s | Iter Loss 0.3431 | Iter Mean Loss 0.4654\n",
      "2025-11-10 13:46:22,162 - root - INFO - CF Training: Epoch 0001 Iter 2286 / 2325 | Time 0.1s | Iter Loss 0.3587 | Iter Mean Loss 0.4653\n",
      "2025-11-10 13:46:22,357 - root - INFO - CF Training: Epoch 0001 Iter 2289 / 2325 | Time 0.1s | Iter Loss 0.3509 | Iter Mean Loss 0.4651\n",
      "2025-11-10 13:46:22,552 - root - INFO - CF Training: Epoch 0001 Iter 2292 / 2325 | Time 0.1s | Iter Loss 0.3548 | Iter Mean Loss 0.4650\n",
      "2025-11-10 13:46:22,745 - root - INFO - CF Training: Epoch 0001 Iter 2295 / 2325 | Time 0.1s | Iter Loss 0.3461 | Iter Mean Loss 0.4648\n",
      "2025-11-10 13:46:22,940 - root - INFO - CF Training: Epoch 0001 Iter 2298 / 2325 | Time 0.1s | Iter Loss 0.3515 | Iter Mean Loss 0.4647\n",
      "2025-11-10 13:46:23,136 - root - INFO - CF Training: Epoch 0001 Iter 2301 / 2325 | Time 0.1s | Iter Loss 0.3321 | Iter Mean Loss 0.4645\n",
      "2025-11-10 13:46:23,330 - root - INFO - CF Training: Epoch 0001 Iter 2304 / 2325 | Time 0.1s | Iter Loss 0.3507 | Iter Mean Loss 0.4644\n",
      "2025-11-10 13:46:23,523 - root - INFO - CF Training: Epoch 0001 Iter 2307 / 2325 | Time 0.1s | Iter Loss 0.3620 | Iter Mean Loss 0.4643\n",
      "2025-11-10 13:46:23,717 - root - INFO - CF Training: Epoch 0001 Iter 2310 / 2325 | Time 0.1s | Iter Loss 0.3351 | Iter Mean Loss 0.4641\n",
      "2025-11-10 13:46:23,911 - root - INFO - CF Training: Epoch 0001 Iter 2313 / 2325 | Time 0.1s | Iter Loss 0.3496 | Iter Mean Loss 0.4640\n",
      "2025-11-10 13:46:24,107 - root - INFO - CF Training: Epoch 0001 Iter 2316 / 2325 | Time 0.1s | Iter Loss 0.3554 | Iter Mean Loss 0.4638\n",
      "2025-11-10 13:46:24,301 - root - INFO - CF Training: Epoch 0001 Iter 2319 / 2325 | Time 0.1s | Iter Loss 0.3495 | Iter Mean Loss 0.4637\n",
      "2025-11-10 13:46:24,495 - root - INFO - CF Training: Epoch 0001 Iter 2322 / 2325 | Time 0.1s | Iter Loss 0.3755 | Iter Mean Loss 0.4635\n",
      "2025-11-10 13:46:24,689 - root - INFO - CF Training: Epoch 0001 Iter 2325 / 2325 | Time 0.1s | Iter Loss 0.3521 | Iter Mean Loss 0.4634\n",
      "2025-11-10 13:46:24,690 - root - INFO - CF Training: Epoch 0001 Total Iter 2325 | Total Time 151.9s | Iter Mean Loss 0.4634\n",
      "2025-11-10 13:46:24,882 - root - INFO - KG Training: Epoch 0001 Iter 0003 / 4823 | Time 0.0s | Iter Loss 0.6928 | Iter Mean Loss 0.6929\n",
      "2025-11-10 13:46:25,005 - root - INFO - KG Training: Epoch 0001 Iter 0006 / 4823 | Time 0.0s | Iter Loss 0.6921 | Iter Mean Loss 0.6926\n",
      "2025-11-10 13:46:25,160 - root - INFO - KG Training: Epoch 0001 Iter 0009 / 4823 | Time 0.1s | Iter Loss 0.6915 | Iter Mean Loss 0.6923\n",
      "2025-11-10 13:46:25,371 - root - INFO - KG Training: Epoch 0001 Iter 0012 / 4823 | Time 0.0s | Iter Loss 0.6909 | Iter Mean Loss 0.6920\n",
      "2025-11-10 13:46:25,519 - root - INFO - KG Training: Epoch 0001 Iter 0015 / 4823 | Time 0.0s | Iter Loss 0.6904 | Iter Mean Loss 0.6917\n",
      "2025-11-10 13:46:25,643 - root - INFO - KG Training: Epoch 0001 Iter 0018 / 4823 | Time 0.0s | Iter Loss 0.6896 | Iter Mean Loss 0.6914\n",
      "2025-11-10 13:46:26,008 - root - INFO - KG Training: Epoch 0001 Iter 0021 / 4823 | Time 0.0s | Iter Loss 0.6892 | Iter Mean Loss 0.6911\n",
      "2025-11-10 13:46:26,134 - root - INFO - KG Training: Epoch 0001 Iter 0024 / 4823 | Time 0.0s | Iter Loss 0.6883 | Iter Mean Loss 0.6908\n",
      "2025-11-10 13:46:26,523 - root - INFO - KG Training: Epoch 0001 Iter 0027 / 4823 | Time 0.3s | Iter Loss 0.6873 | Iter Mean Loss 0.6904\n",
      "2025-11-10 13:46:26,652 - root - INFO - KG Training: Epoch 0001 Iter 0030 / 4823 | Time 0.0s | Iter Loss 0.6866 | Iter Mean Loss 0.6901\n",
      "2025-11-10 13:46:26,860 - root - INFO - KG Training: Epoch 0001 Iter 0033 / 4823 | Time 0.0s | Iter Loss 0.6864 | Iter Mean Loss 0.6897\n",
      "2025-11-10 13:46:26,984 - root - INFO - KG Training: Epoch 0001 Iter 0036 / 4823 | Time 0.0s | Iter Loss 0.6850 | Iter Mean Loss 0.6894\n",
      "2025-11-10 13:46:27,119 - root - INFO - KG Training: Epoch 0001 Iter 0039 / 4823 | Time 0.0s | Iter Loss 0.6838 | Iter Mean Loss 0.6890\n",
      "2025-11-10 13:46:27,249 - root - INFO - KG Training: Epoch 0001 Iter 0042 / 4823 | Time 0.0s | Iter Loss 0.6831 | Iter Mean Loss 0.6885\n",
      "2025-11-10 13:46:27,381 - root - INFO - KG Training: Epoch 0001 Iter 0045 / 4823 | Time 0.0s | Iter Loss 0.6823 | Iter Mean Loss 0.6881\n",
      "2025-11-10 13:46:27,552 - root - INFO - KG Training: Epoch 0001 Iter 0048 / 4823 | Time 0.1s | Iter Loss 0.6810 | Iter Mean Loss 0.6877\n",
      "2025-11-10 13:46:27,681 - root - INFO - KG Training: Epoch 0001 Iter 0051 / 4823 | Time 0.0s | Iter Loss 0.6798 | Iter Mean Loss 0.6873\n",
      "2025-11-10 13:46:27,804 - root - INFO - KG Training: Epoch 0001 Iter 0054 / 4823 | Time 0.0s | Iter Loss 0.6788 | Iter Mean Loss 0.6868\n",
      "2025-11-10 13:46:27,931 - root - INFO - KG Training: Epoch 0001 Iter 0057 / 4823 | Time 0.0s | Iter Loss 0.6770 | Iter Mean Loss 0.6863\n",
      "2025-11-10 13:46:28,052 - root - INFO - KG Training: Epoch 0001 Iter 0060 / 4823 | Time 0.0s | Iter Loss 0.6759 | Iter Mean Loss 0.6858\n",
      "2025-11-10 13:46:28,178 - root - INFO - KG Training: Epoch 0001 Iter 0063 / 4823 | Time 0.0s | Iter Loss 0.6744 | Iter Mean Loss 0.6853\n",
      "2025-11-10 13:46:28,301 - root - INFO - KG Training: Epoch 0001 Iter 0066 / 4823 | Time 0.0s | Iter Loss 0.6731 | Iter Mean Loss 0.6847\n",
      "2025-11-10 13:46:28,422 - root - INFO - KG Training: Epoch 0001 Iter 0069 / 4823 | Time 0.0s | Iter Loss 0.6719 | Iter Mean Loss 0.6842\n",
      "2025-11-10 13:46:28,546 - root - INFO - KG Training: Epoch 0001 Iter 0072 / 4823 | Time 0.0s | Iter Loss 0.6682 | Iter Mean Loss 0.6836\n",
      "2025-11-10 13:46:28,862 - root - INFO - KG Training: Epoch 0001 Iter 0075 / 4823 | Time 0.0s | Iter Loss 0.6663 | Iter Mean Loss 0.6830\n",
      "2025-11-10 13:46:29,101 - root - INFO - KG Training: Epoch 0001 Iter 0078 / 4823 | Time 0.2s | Iter Loss 0.6653 | Iter Mean Loss 0.6823\n",
      "2025-11-10 13:46:29,296 - root - INFO - KG Training: Epoch 0001 Iter 0081 / 4823 | Time 0.0s | Iter Loss 0.6645 | Iter Mean Loss 0.6817\n",
      "2025-11-10 13:46:29,612 - root - INFO - KG Training: Epoch 0001 Iter 0084 / 4823 | Time 0.0s | Iter Loss 0.6631 | Iter Mean Loss 0.6810\n",
      "2025-11-10 13:46:29,737 - root - INFO - KG Training: Epoch 0001 Iter 0087 / 4823 | Time 0.0s | Iter Loss 0.6614 | Iter Mean Loss 0.6804\n",
      "2025-11-10 13:46:29,868 - root - INFO - KG Training: Epoch 0001 Iter 0090 / 4823 | Time 0.0s | Iter Loss 0.6578 | Iter Mean Loss 0.6796\n",
      "2025-11-10 13:46:29,993 - root - INFO - KG Training: Epoch 0001 Iter 0093 / 4823 | Time 0.0s | Iter Loss 0.6560 | Iter Mean Loss 0.6789\n",
      "2025-11-10 13:46:30,118 - root - INFO - KG Training: Epoch 0001 Iter 0096 / 4823 | Time 0.0s | Iter Loss 0.6537 | Iter Mean Loss 0.6781\n",
      "2025-11-10 13:46:30,313 - root - INFO - KG Training: Epoch 0001 Iter 0099 / 4823 | Time 0.0s | Iter Loss 0.6522 | Iter Mean Loss 0.6773\n",
      "2025-11-10 13:46:30,436 - root - INFO - KG Training: Epoch 0001 Iter 0102 / 4823 | Time 0.0s | Iter Loss 0.6489 | Iter Mean Loss 0.6765\n",
      "2025-11-10 13:46:30,560 - root - INFO - KG Training: Epoch 0001 Iter 0105 / 4823 | Time 0.0s | Iter Loss 0.6448 | Iter Mean Loss 0.6756\n",
      "2025-11-10 13:46:31,074 - root - INFO - KG Training: Epoch 0001 Iter 0108 / 4823 | Time 0.0s | Iter Loss 0.6423 | Iter Mean Loss 0.6747\n",
      "2025-11-10 13:46:31,205 - root - INFO - KG Training: Epoch 0001 Iter 0111 / 4823 | Time 0.0s | Iter Loss 0.6393 | Iter Mean Loss 0.6738\n",
      "2025-11-10 13:46:31,329 - root - INFO - KG Training: Epoch 0001 Iter 0114 / 4823 | Time 0.0s | Iter Loss 0.6381 | Iter Mean Loss 0.6729\n",
      "2025-11-10 13:46:31,454 - root - INFO - KG Training: Epoch 0001 Iter 0117 / 4823 | Time 0.0s | Iter Loss 0.6341 | Iter Mean Loss 0.6720\n",
      "2025-11-10 13:46:31,702 - root - INFO - KG Training: Epoch 0001 Iter 0120 / 4823 | Time 0.0s | Iter Loss 0.6333 | Iter Mean Loss 0.6710\n",
      "2025-11-10 13:46:31,830 - root - INFO - KG Training: Epoch 0001 Iter 0123 / 4823 | Time 0.0s | Iter Loss 0.6315 | Iter Mean Loss 0.6701\n",
      "2025-11-10 13:46:31,955 - root - INFO - KG Training: Epoch 0001 Iter 0126 / 4823 | Time 0.0s | Iter Loss 0.6264 | Iter Mean Loss 0.6691\n",
      "2025-11-10 13:46:32,177 - root - INFO - KG Training: Epoch 0001 Iter 0129 / 4823 | Time 0.0s | Iter Loss 0.6241 | Iter Mean Loss 0.6681\n",
      "2025-11-10 13:46:32,301 - root - INFO - KG Training: Epoch 0001 Iter 0132 / 4823 | Time 0.0s | Iter Loss 0.6207 | Iter Mean Loss 0.6670\n",
      "2025-11-10 13:46:32,429 - root - INFO - KG Training: Epoch 0001 Iter 0135 / 4823 | Time 0.0s | Iter Loss 0.6201 | Iter Mean Loss 0.6660\n",
      "2025-11-10 13:46:32,550 - root - INFO - KG Training: Epoch 0001 Iter 0138 / 4823 | Time 0.0s | Iter Loss 0.6156 | Iter Mean Loss 0.6649\n",
      "2025-11-10 13:46:32,674 - root - INFO - KG Training: Epoch 0001 Iter 0141 / 4823 | Time 0.0s | Iter Loss 0.6163 | Iter Mean Loss 0.6639\n",
      "2025-11-10 13:46:32,802 - root - INFO - KG Training: Epoch 0001 Iter 0144 / 4823 | Time 0.0s | Iter Loss 0.6083 | Iter Mean Loss 0.6628\n",
      "2025-11-10 13:46:32,928 - root - INFO - KG Training: Epoch 0001 Iter 0147 / 4823 | Time 0.0s | Iter Loss 0.6069 | Iter Mean Loss 0.6617\n",
      "2025-11-10 13:46:33,056 - root - INFO - KG Training: Epoch 0001 Iter 0150 / 4823 | Time 0.0s | Iter Loss 0.6053 | Iter Mean Loss 0.6605\n",
      "2025-11-10 13:46:33,204 - root - INFO - KG Training: Epoch 0001 Iter 0153 / 4823 | Time 0.0s | Iter Loss 0.5972 | Iter Mean Loss 0.6594\n",
      "2025-11-10 13:46:33,332 - root - INFO - KG Training: Epoch 0001 Iter 0156 / 4823 | Time 0.0s | Iter Loss 0.5965 | Iter Mean Loss 0.6582\n",
      "2025-11-10 13:46:33,614 - root - INFO - KG Training: Epoch 0001 Iter 0159 / 4823 | Time 0.2s | Iter Loss 0.5948 | Iter Mean Loss 0.6570\n",
      "2025-11-10 13:46:33,748 - root - INFO - KG Training: Epoch 0001 Iter 0162 / 4823 | Time 0.0s | Iter Loss 0.5928 | Iter Mean Loss 0.6558\n",
      "2025-11-10 13:46:33,879 - root - INFO - KG Training: Epoch 0001 Iter 0165 / 4823 | Time 0.0s | Iter Loss 0.5873 | Iter Mean Loss 0.6546\n",
      "2025-11-10 13:46:34,002 - root - INFO - KG Training: Epoch 0001 Iter 0168 / 4823 | Time 0.0s | Iter Loss 0.5836 | Iter Mean Loss 0.6534\n",
      "2025-11-10 13:46:34,131 - root - INFO - KG Training: Epoch 0001 Iter 0171 / 4823 | Time 0.0s | Iter Loss 0.5842 | Iter Mean Loss 0.6522\n",
      "2025-11-10 13:46:34,258 - root - INFO - KG Training: Epoch 0001 Iter 0174 / 4823 | Time 0.0s | Iter Loss 0.5770 | Iter Mean Loss 0.6509\n",
      "2025-11-10 13:46:34,383 - root - INFO - KG Training: Epoch 0001 Iter 0177 / 4823 | Time 0.0s | Iter Loss 0.5773 | Iter Mean Loss 0.6497\n",
      "2025-11-10 13:46:34,509 - root - INFO - KG Training: Epoch 0001 Iter 0180 / 4823 | Time 0.0s | Iter Loss 0.5743 | Iter Mean Loss 0.6484\n",
      "2025-11-10 13:46:34,633 - root - INFO - KG Training: Epoch 0001 Iter 0183 / 4823 | Time 0.0s | Iter Loss 0.5742 | Iter Mean Loss 0.6472\n",
      "2025-11-10 13:46:34,760 - root - INFO - KG Training: Epoch 0001 Iter 0186 / 4823 | Time 0.0s | Iter Loss 0.5671 | Iter Mean Loss 0.6459\n",
      "2025-11-10 13:46:34,882 - root - INFO - KG Training: Epoch 0001 Iter 0189 / 4823 | Time 0.0s | Iter Loss 0.5643 | Iter Mean Loss 0.6446\n",
      "2025-11-10 13:46:35,005 - root - INFO - KG Training: Epoch 0001 Iter 0192 / 4823 | Time 0.0s | Iter Loss 0.5586 | Iter Mean Loss 0.6433\n",
      "2025-11-10 13:46:35,131 - root - INFO - KG Training: Epoch 0001 Iter 0195 / 4823 | Time 0.0s | Iter Loss 0.5557 | Iter Mean Loss 0.6420\n",
      "2025-11-10 13:46:35,253 - root - INFO - KG Training: Epoch 0001 Iter 0198 / 4823 | Time 0.0s | Iter Loss 0.5585 | Iter Mean Loss 0.6407\n",
      "2025-11-10 13:46:35,448 - root - INFO - KG Training: Epoch 0001 Iter 0201 / 4823 | Time 0.1s | Iter Loss 0.5543 | Iter Mean Loss 0.6394\n",
      "2025-11-10 13:46:35,571 - root - INFO - KG Training: Epoch 0001 Iter 0204 / 4823 | Time 0.0s | Iter Loss 0.5429 | Iter Mean Loss 0.6380\n",
      "2025-11-10 13:46:35,694 - root - INFO - KG Training: Epoch 0001 Iter 0207 / 4823 | Time 0.0s | Iter Loss 0.5420 | Iter Mean Loss 0.6366\n",
      "2025-11-10 13:46:35,883 - root - INFO - KG Training: Epoch 0001 Iter 0210 / 4823 | Time 0.1s | Iter Loss 0.5387 | Iter Mean Loss 0.6353\n",
      "2025-11-10 13:46:36,008 - root - INFO - KG Training: Epoch 0001 Iter 0213 / 4823 | Time 0.0s | Iter Loss 0.5349 | Iter Mean Loss 0.6339\n",
      "2025-11-10 13:46:36,202 - root - INFO - KG Training: Epoch 0001 Iter 0216 / 4823 | Time 0.1s | Iter Loss 0.5359 | Iter Mean Loss 0.6326\n",
      "2025-11-10 13:46:36,326 - root - INFO - KG Training: Epoch 0001 Iter 0219 / 4823 | Time 0.0s | Iter Loss 0.5327 | Iter Mean Loss 0.6313\n",
      "2025-11-10 13:46:36,453 - root - INFO - KG Training: Epoch 0001 Iter 0222 / 4823 | Time 0.0s | Iter Loss 0.5251 | Iter Mean Loss 0.6299\n",
      "2025-11-10 13:46:36,577 - root - INFO - KG Training: Epoch 0001 Iter 0225 / 4823 | Time 0.0s | Iter Loss 0.5264 | Iter Mean Loss 0.6285\n",
      "2025-11-10 13:46:36,702 - root - INFO - KG Training: Epoch 0001 Iter 0228 / 4823 | Time 0.0s | Iter Loss 0.5342 | Iter Mean Loss 0.6272\n",
      "2025-11-10 13:46:36,829 - root - INFO - KG Training: Epoch 0001 Iter 0231 / 4823 | Time 0.0s | Iter Loss 0.5264 | Iter Mean Loss 0.6259\n",
      "2025-11-10 13:46:36,955 - root - INFO - KG Training: Epoch 0001 Iter 0234 / 4823 | Time 0.0s | Iter Loss 0.5163 | Iter Mean Loss 0.6245\n",
      "2025-11-10 13:46:37,078 - root - INFO - KG Training: Epoch 0001 Iter 0237 / 4823 | Time 0.0s | Iter Loss 0.5183 | Iter Mean Loss 0.6232\n",
      "2025-11-10 13:46:37,224 - root - INFO - KG Training: Epoch 0001 Iter 0240 / 4823 | Time 0.0s | Iter Loss 0.5148 | Iter Mean Loss 0.6218\n",
      "2025-11-10 13:46:37,435 - root - INFO - KG Training: Epoch 0001 Iter 0243 / 4823 | Time 0.0s | Iter Loss 0.5087 | Iter Mean Loss 0.6204\n",
      "2025-11-10 13:46:37,561 - root - INFO - KG Training: Epoch 0001 Iter 0246 / 4823 | Time 0.0s | Iter Loss 0.5102 | Iter Mean Loss 0.6191\n",
      "2025-11-10 13:46:37,689 - root - INFO - KG Training: Epoch 0001 Iter 0249 / 4823 | Time 0.0s | Iter Loss 0.5070 | Iter Mean Loss 0.6177\n",
      "2025-11-10 13:46:37,812 - root - INFO - KG Training: Epoch 0001 Iter 0252 / 4823 | Time 0.0s | Iter Loss 0.5064 | Iter Mean Loss 0.6165\n",
      "2025-11-10 13:46:38,005 - root - INFO - KG Training: Epoch 0001 Iter 0255 / 4823 | Time 0.0s | Iter Loss 0.4984 | Iter Mean Loss 0.6151\n",
      "2025-11-10 13:46:38,252 - root - INFO - KG Training: Epoch 0001 Iter 0258 / 4823 | Time 0.2s | Iter Loss 0.4944 | Iter Mean Loss 0.6137\n",
      "2025-11-10 13:46:38,379 - root - INFO - KG Training: Epoch 0001 Iter 0261 / 4823 | Time 0.0s | Iter Loss 0.4957 | Iter Mean Loss 0.6124\n",
      "2025-11-10 13:46:38,503 - root - INFO - KG Training: Epoch 0001 Iter 0264 / 4823 | Time 0.0s | Iter Loss 0.4954 | Iter Mean Loss 0.6110\n",
      "2025-11-10 13:46:38,625 - root - INFO - KG Training: Epoch 0001 Iter 0267 / 4823 | Time 0.0s | Iter Loss 0.4951 | Iter Mean Loss 0.6097\n",
      "2025-11-10 13:46:38,750 - root - INFO - KG Training: Epoch 0001 Iter 0270 / 4823 | Time 0.0s | Iter Loss 0.4867 | Iter Mean Loss 0.6084\n",
      "2025-11-10 13:46:38,876 - root - INFO - KG Training: Epoch 0001 Iter 0273 / 4823 | Time 0.0s | Iter Loss 0.4910 | Iter Mean Loss 0.6071\n",
      "2025-11-10 13:46:39,000 - root - INFO - KG Training: Epoch 0001 Iter 0276 / 4823 | Time 0.0s | Iter Loss 0.4872 | Iter Mean Loss 0.6058\n",
      "2025-11-10 13:46:39,122 - root - INFO - KG Training: Epoch 0001 Iter 0279 / 4823 | Time 0.0s | Iter Loss 0.4863 | Iter Mean Loss 0.6045\n",
      "2025-11-10 13:46:39,247 - root - INFO - KG Training: Epoch 0001 Iter 0282 / 4823 | Time 0.0s | Iter Loss 0.4755 | Iter Mean Loss 0.6031\n",
      "2025-11-10 13:46:39,375 - root - INFO - KG Training: Epoch 0001 Iter 0285 / 4823 | Time 0.0s | Iter Loss 0.4758 | Iter Mean Loss 0.6018\n",
      "2025-11-10 13:46:39,502 - root - INFO - KG Training: Epoch 0001 Iter 0288 / 4823 | Time 0.0s | Iter Loss 0.4783 | Iter Mean Loss 0.6005\n",
      "2025-11-10 13:46:39,628 - root - INFO - KG Training: Epoch 0001 Iter 0291 / 4823 | Time 0.0s | Iter Loss 0.4817 | Iter Mean Loss 0.5992\n",
      "2025-11-10 13:46:39,752 - root - INFO - KG Training: Epoch 0001 Iter 0294 / 4823 | Time 0.0s | Iter Loss 0.4703 | Iter Mean Loss 0.5979\n",
      "2025-11-10 13:46:39,878 - root - INFO - KG Training: Epoch 0001 Iter 0297 / 4823 | Time 0.0s | Iter Loss 0.4778 | Iter Mean Loss 0.5967\n",
      "2025-11-10 13:46:40,002 - root - INFO - KG Training: Epoch 0001 Iter 0300 / 4823 | Time 0.0s | Iter Loss 0.4615 | Iter Mean Loss 0.5953\n",
      "2025-11-10 13:46:40,506 - root - INFO - KG Training: Epoch 0001 Iter 0303 / 4823 | Time 0.1s | Iter Loss 0.4711 | Iter Mean Loss 0.5941\n",
      "2025-11-10 13:46:40,636 - root - INFO - KG Training: Epoch 0001 Iter 0306 / 4823 | Time 0.0s | Iter Loss 0.4663 | Iter Mean Loss 0.5928\n",
      "2025-11-10 13:46:40,759 - root - INFO - KG Training: Epoch 0001 Iter 0309 / 4823 | Time 0.0s | Iter Loss 0.4624 | Iter Mean Loss 0.5915\n",
      "2025-11-10 13:46:40,950 - root - INFO - KG Training: Epoch 0001 Iter 0312 / 4823 | Time 0.0s | Iter Loss 0.4615 | Iter Mean Loss 0.5902\n",
      "2025-11-10 13:46:41,074 - root - INFO - KG Training: Epoch 0001 Iter 0315 / 4823 | Time 0.0s | Iter Loss 0.4544 | Iter Mean Loss 0.5890\n",
      "2025-11-10 13:46:41,196 - root - INFO - KG Training: Epoch 0001 Iter 0318 / 4823 | Time 0.0s | Iter Loss 0.4600 | Iter Mean Loss 0.5878\n",
      "2025-11-10 13:46:41,322 - root - INFO - KG Training: Epoch 0001 Iter 0321 / 4823 | Time 0.0s | Iter Loss 0.4591 | Iter Mean Loss 0.5865\n",
      "2025-11-10 13:46:41,450 - root - INFO - KG Training: Epoch 0001 Iter 0324 / 4823 | Time 0.0s | Iter Loss 0.4523 | Iter Mean Loss 0.5853\n",
      "2025-11-10 13:46:41,579 - root - INFO - KG Training: Epoch 0001 Iter 0327 / 4823 | Time 0.0s | Iter Loss 0.4555 | Iter Mean Loss 0.5841\n",
      "2025-11-10 13:46:41,703 - root - INFO - KG Training: Epoch 0001 Iter 0330 / 4823 | Time 0.0s | Iter Loss 0.4588 | Iter Mean Loss 0.5829\n",
      "2025-11-10 13:46:41,894 - root - INFO - KG Training: Epoch 0001 Iter 0333 / 4823 | Time 0.1s | Iter Loss 0.4527 | Iter Mean Loss 0.5817\n",
      "2025-11-10 13:46:42,124 - root - INFO - KG Training: Epoch 0001 Iter 0336 / 4823 | Time 0.0s | Iter Loss 0.4400 | Iter Mean Loss 0.5805\n",
      "2025-11-10 13:46:42,248 - root - INFO - KG Training: Epoch 0001 Iter 0339 / 4823 | Time 0.0s | Iter Loss 0.4506 | Iter Mean Loss 0.5793\n",
      "2025-11-10 13:46:42,372 - root - INFO - KG Training: Epoch 0001 Iter 0342 / 4823 | Time 0.0s | Iter Loss 0.4332 | Iter Mean Loss 0.5780\n",
      "2025-11-10 13:46:42,497 - root - INFO - KG Training: Epoch 0001 Iter 0345 / 4823 | Time 0.0s | Iter Loss 0.4459 | Iter Mean Loss 0.5769\n",
      "2025-11-10 13:46:42,893 - root - INFO - KG Training: Epoch 0001 Iter 0348 / 4823 | Time 0.0s | Iter Loss 0.4395 | Iter Mean Loss 0.5756\n",
      "2025-11-10 13:46:43,016 - root - INFO - KG Training: Epoch 0001 Iter 0351 / 4823 | Time 0.0s | Iter Loss 0.4268 | Iter Mean Loss 0.5744\n",
      "2025-11-10 13:46:43,307 - root - INFO - KG Training: Epoch 0001 Iter 0354 / 4823 | Time 0.0s | Iter Loss 0.4314 | Iter Mean Loss 0.5732\n",
      "2025-11-10 13:46:43,516 - root - INFO - KG Training: Epoch 0001 Iter 0357 / 4823 | Time 0.1s | Iter Loss 0.4389 | Iter Mean Loss 0.5721\n",
      "2025-11-10 13:46:43,639 - root - INFO - KG Training: Epoch 0001 Iter 0360 / 4823 | Time 0.0s | Iter Loss 0.4272 | Iter Mean Loss 0.5709\n",
      "2025-11-10 13:46:43,768 - root - INFO - KG Training: Epoch 0001 Iter 0363 / 4823 | Time 0.0s | Iter Loss 0.4235 | Iter Mean Loss 0.5697\n",
      "2025-11-10 13:46:43,895 - root - INFO - KG Training: Epoch 0001 Iter 0366 / 4823 | Time 0.0s | Iter Loss 0.4350 | Iter Mean Loss 0.5686\n",
      "2025-11-10 13:46:44,021 - root - INFO - KG Training: Epoch 0001 Iter 0369 / 4823 | Time 0.0s | Iter Loss 0.4340 | Iter Mean Loss 0.5675\n",
      "2025-11-10 13:46:44,145 - root - INFO - KG Training: Epoch 0001 Iter 0372 / 4823 | Time 0.0s | Iter Loss 0.4219 | Iter Mean Loss 0.5663\n",
      "2025-11-10 13:46:44,269 - root - INFO - KG Training: Epoch 0001 Iter 0375 / 4823 | Time 0.0s | Iter Loss 0.4219 | Iter Mean Loss 0.5652\n",
      "2025-11-10 13:46:44,393 - root - INFO - KG Training: Epoch 0001 Iter 0378 / 4823 | Time 0.0s | Iter Loss 0.4218 | Iter Mean Loss 0.5640\n",
      "2025-11-10 13:46:44,519 - root - INFO - KG Training: Epoch 0001 Iter 0381 / 4823 | Time 0.0s | Iter Loss 0.4261 | Iter Mean Loss 0.5629\n",
      "2025-11-10 13:46:44,642 - root - INFO - KG Training: Epoch 0001 Iter 0384 / 4823 | Time 0.0s | Iter Loss 0.4182 | Iter Mean Loss 0.5618\n",
      "2025-11-10 13:46:44,768 - root - INFO - KG Training: Epoch 0001 Iter 0387 / 4823 | Time 0.0s | Iter Loss 0.4259 | Iter Mean Loss 0.5607\n",
      "2025-11-10 13:46:44,895 - root - INFO - KG Training: Epoch 0001 Iter 0390 / 4823 | Time 0.0s | Iter Loss 0.4185 | Iter Mean Loss 0.5596\n",
      "2025-11-10 13:46:45,169 - root - INFO - KG Training: Epoch 0001 Iter 0393 / 4823 | Time 0.0s | Iter Loss 0.4218 | Iter Mean Loss 0.5584\n",
      "2025-11-10 13:46:45,295 - root - INFO - KG Training: Epoch 0001 Iter 0396 / 4823 | Time 0.0s | Iter Loss 0.4028 | Iter Mean Loss 0.5573\n",
      "2025-11-10 13:46:45,486 - root - INFO - KG Training: Epoch 0001 Iter 0399 / 4823 | Time 0.0s | Iter Loss 0.4088 | Iter Mean Loss 0.5562\n",
      "2025-11-10 13:46:45,705 - root - INFO - KG Training: Epoch 0001 Iter 0402 / 4823 | Time 0.0s | Iter Loss 0.4213 | Iter Mean Loss 0.5552\n",
      "2025-11-10 13:46:45,834 - root - INFO - KG Training: Epoch 0001 Iter 0405 / 4823 | Time 0.0s | Iter Loss 0.4058 | Iter Mean Loss 0.5541\n",
      "2025-11-10 13:46:45,965 - root - INFO - KG Training: Epoch 0001 Iter 0408 / 4823 | Time 0.0s | Iter Loss 0.4052 | Iter Mean Loss 0.5530\n",
      "2025-11-10 13:46:46,160 - root - INFO - KG Training: Epoch 0001 Iter 0411 / 4823 | Time 0.1s | Iter Loss 0.4146 | Iter Mean Loss 0.5519\n",
      "2025-11-10 13:46:46,294 - root - INFO - KG Training: Epoch 0001 Iter 0414 / 4823 | Time 0.0s | Iter Loss 0.4060 | Iter Mean Loss 0.5508\n",
      "2025-11-10 13:46:46,418 - root - INFO - KG Training: Epoch 0001 Iter 0417 / 4823 | Time 0.0s | Iter Loss 0.4133 | Iter Mean Loss 0.5498\n",
      "2025-11-10 13:46:46,610 - root - INFO - KG Training: Epoch 0001 Iter 0420 / 4823 | Time 0.0s | Iter Loss 0.3944 | Iter Mean Loss 0.5487\n",
      "2025-11-10 13:46:46,736 - root - INFO - KG Training: Epoch 0001 Iter 0423 / 4823 | Time 0.0s | Iter Loss 0.3935 | Iter Mean Loss 0.5476\n",
      "2025-11-10 13:46:46,924 - root - INFO - KG Training: Epoch 0001 Iter 0426 / 4823 | Time 0.0s | Iter Loss 0.3982 | Iter Mean Loss 0.5466\n",
      "2025-11-10 13:46:47,049 - root - INFO - KG Training: Epoch 0001 Iter 0429 / 4823 | Time 0.0s | Iter Loss 0.3984 | Iter Mean Loss 0.5456\n",
      "2025-11-10 13:46:47,244 - root - INFO - KG Training: Epoch 0001 Iter 0432 / 4823 | Time 0.0s | Iter Loss 0.4027 | Iter Mean Loss 0.5446\n",
      "2025-11-10 13:46:47,369 - root - INFO - KG Training: Epoch 0001 Iter 0435 / 4823 | Time 0.0s | Iter Loss 0.4052 | Iter Mean Loss 0.5436\n",
      "2025-11-10 13:46:47,496 - root - INFO - KG Training: Epoch 0001 Iter 0438 / 4823 | Time 0.0s | Iter Loss 0.3943 | Iter Mean Loss 0.5426\n",
      "2025-11-10 13:46:47,619 - root - INFO - KG Training: Epoch 0001 Iter 0441 / 4823 | Time 0.0s | Iter Loss 0.3978 | Iter Mean Loss 0.5416\n",
      "2025-11-10 13:46:47,751 - root - INFO - KG Training: Epoch 0001 Iter 0444 / 4823 | Time 0.0s | Iter Loss 0.3906 | Iter Mean Loss 0.5406\n",
      "2025-11-10 13:46:47,969 - root - INFO - KG Training: Epoch 0001 Iter 0447 / 4823 | Time 0.1s | Iter Loss 0.3844 | Iter Mean Loss 0.5396\n",
      "2025-11-10 13:46:48,094 - root - INFO - KG Training: Epoch 0001 Iter 0450 / 4823 | Time 0.0s | Iter Loss 0.3831 | Iter Mean Loss 0.5386\n",
      "2025-11-10 13:46:48,220 - root - INFO - KG Training: Epoch 0001 Iter 0453 / 4823 | Time 0.0s | Iter Loss 0.3953 | Iter Mean Loss 0.5376\n",
      "2025-11-10 13:46:48,349 - root - INFO - KG Training: Epoch 0001 Iter 0456 / 4823 | Time 0.0s | Iter Loss 0.3902 | Iter Mean Loss 0.5366\n",
      "2025-11-10 13:46:48,477 - root - INFO - KG Training: Epoch 0001 Iter 0459 / 4823 | Time 0.0s | Iter Loss 0.3867 | Iter Mean Loss 0.5356\n",
      "2025-11-10 13:46:48,600 - root - INFO - KG Training: Epoch 0001 Iter 0462 / 4823 | Time 0.0s | Iter Loss 0.3799 | Iter Mean Loss 0.5347\n",
      "2025-11-10 13:46:48,728 - root - INFO - KG Training: Epoch 0001 Iter 0465 / 4823 | Time 0.0s | Iter Loss 0.3756 | Iter Mean Loss 0.5337\n",
      "2025-11-10 13:46:48,919 - root - INFO - KG Training: Epoch 0001 Iter 0468 / 4823 | Time 0.0s | Iter Loss 0.3696 | Iter Mean Loss 0.5327\n",
      "2025-11-10 13:46:49,044 - root - INFO - KG Training: Epoch 0001 Iter 0471 / 4823 | Time 0.0s | Iter Loss 0.3890 | Iter Mean Loss 0.5318\n",
      "2025-11-10 13:46:49,277 - root - INFO - KG Training: Epoch 0001 Iter 0474 / 4823 | Time 0.0s | Iter Loss 0.3693 | Iter Mean Loss 0.5308\n",
      "2025-11-10 13:46:49,406 - root - INFO - KG Training: Epoch 0001 Iter 0477 / 4823 | Time 0.0s | Iter Loss 0.3930 | Iter Mean Loss 0.5299\n",
      "2025-11-10 13:46:49,533 - root - INFO - KG Training: Epoch 0001 Iter 0480 / 4823 | Time 0.0s | Iter Loss 0.3782 | Iter Mean Loss 0.5290\n",
      "2025-11-10 13:46:49,660 - root - INFO - KG Training: Epoch 0001 Iter 0483 / 4823 | Time 0.0s | Iter Loss 0.3721 | Iter Mean Loss 0.5280\n",
      "2025-11-10 13:46:49,791 - root - INFO - KG Training: Epoch 0001 Iter 0486 / 4823 | Time 0.0s | Iter Loss 0.3704 | Iter Mean Loss 0.5270\n",
      "2025-11-10 13:46:50,080 - root - INFO - KG Training: Epoch 0001 Iter 0489 / 4823 | Time 0.0s | Iter Loss 0.3650 | Iter Mean Loss 0.5261\n",
      "2025-11-10 13:46:50,207 - root - INFO - KG Training: Epoch 0001 Iter 0492 / 4823 | Time 0.0s | Iter Loss 0.3691 | Iter Mean Loss 0.5251\n",
      "2025-11-10 13:46:50,330 - root - INFO - KG Training: Epoch 0001 Iter 0495 / 4823 | Time 0.0s | Iter Loss 0.3679 | Iter Mean Loss 0.5242\n",
      "2025-11-10 13:46:50,455 - root - INFO - KG Training: Epoch 0001 Iter 0498 / 4823 | Time 0.0s | Iter Loss 0.3730 | Iter Mean Loss 0.5233\n",
      "2025-11-10 13:46:50,584 - root - INFO - KG Training: Epoch 0001 Iter 0501 / 4823 | Time 0.0s | Iter Loss 0.3724 | Iter Mean Loss 0.5224\n",
      "2025-11-10 13:46:50,708 - root - INFO - KG Training: Epoch 0001 Iter 0504 / 4823 | Time 0.0s | Iter Loss 0.3694 | Iter Mean Loss 0.5215\n",
      "2025-11-10 13:46:50,832 - root - INFO - KG Training: Epoch 0001 Iter 0507 / 4823 | Time 0.0s | Iter Loss 0.3682 | Iter Mean Loss 0.5206\n",
      "2025-11-10 13:46:50,956 - root - INFO - KG Training: Epoch 0001 Iter 0510 / 4823 | Time 0.0s | Iter Loss 0.3578 | Iter Mean Loss 0.5197\n",
      "2025-11-10 13:46:51,338 - root - INFO - KG Training: Epoch 0001 Iter 0513 / 4823 | Time 0.3s | Iter Loss 0.3703 | Iter Mean Loss 0.5188\n",
      "2025-11-10 13:46:51,538 - root - INFO - KG Training: Epoch 0001 Iter 0516 / 4823 | Time 0.0s | Iter Loss 0.3678 | Iter Mean Loss 0.5179\n",
      "2025-11-10 13:46:51,685 - root - INFO - KG Training: Epoch 0001 Iter 0519 / 4823 | Time 0.0s | Iter Loss 0.3670 | Iter Mean Loss 0.5170\n",
      "2025-11-10 13:46:51,808 - root - INFO - KG Training: Epoch 0001 Iter 0522 / 4823 | Time 0.0s | Iter Loss 0.3689 | Iter Mean Loss 0.5161\n",
      "2025-11-10 13:46:51,933 - root - INFO - KG Training: Epoch 0001 Iter 0525 / 4823 | Time 0.0s | Iter Loss 0.3679 | Iter Mean Loss 0.5153\n",
      "2025-11-10 13:46:52,058 - root - INFO - KG Training: Epoch 0001 Iter 0528 / 4823 | Time 0.0s | Iter Loss 0.3660 | Iter Mean Loss 0.5144\n",
      "2025-11-10 13:46:52,181 - root - INFO - KG Training: Epoch 0001 Iter 0531 / 4823 | Time 0.0s | Iter Loss 0.3619 | Iter Mean Loss 0.5136\n",
      "2025-11-10 13:46:52,307 - root - INFO - KG Training: Epoch 0001 Iter 0534 / 4823 | Time 0.0s | Iter Loss 0.3755 | Iter Mean Loss 0.5127\n",
      "2025-11-10 13:46:52,436 - root - INFO - KG Training: Epoch 0001 Iter 0537 / 4823 | Time 0.0s | Iter Loss 0.3593 | Iter Mean Loss 0.5119\n",
      "2025-11-10 13:46:52,562 - root - INFO - KG Training: Epoch 0001 Iter 0540 / 4823 | Time 0.0s | Iter Loss 0.3670 | Iter Mean Loss 0.5110\n",
      "2025-11-10 13:46:52,690 - root - INFO - KG Training: Epoch 0001 Iter 0543 / 4823 | Time 0.0s | Iter Loss 0.3562 | Iter Mean Loss 0.5102\n",
      "2025-11-10 13:46:52,813 - root - INFO - KG Training: Epoch 0001 Iter 0546 / 4823 | Time 0.0s | Iter Loss 0.3607 | Iter Mean Loss 0.5094\n",
      "2025-11-10 13:46:52,945 - root - INFO - KG Training: Epoch 0001 Iter 0549 / 4823 | Time 0.0s | Iter Loss 0.3453 | Iter Mean Loss 0.5085\n",
      "2025-11-10 13:46:53,071 - root - INFO - KG Training: Epoch 0001 Iter 0552 / 4823 | Time 0.0s | Iter Loss 0.3539 | Iter Mean Loss 0.5077\n",
      "2025-11-10 13:46:53,380 - root - INFO - KG Training: Epoch 0001 Iter 0555 / 4823 | Time 0.1s | Iter Loss 0.3513 | Iter Mean Loss 0.5069\n",
      "2025-11-10 13:46:53,507 - root - INFO - KG Training: Epoch 0001 Iter 0558 / 4823 | Time 0.0s | Iter Loss 0.3494 | Iter Mean Loss 0.5060\n",
      "2025-11-10 13:46:53,632 - root - INFO - KG Training: Epoch 0001 Iter 0561 / 4823 | Time 0.0s | Iter Loss 0.3585 | Iter Mean Loss 0.5052\n",
      "2025-11-10 13:46:53,756 - root - INFO - KG Training: Epoch 0001 Iter 0564 / 4823 | Time 0.0s | Iter Loss 0.3510 | Iter Mean Loss 0.5044\n",
      "2025-11-10 13:46:53,881 - root - INFO - KG Training: Epoch 0001 Iter 0567 / 4823 | Time 0.0s | Iter Loss 0.3486 | Iter Mean Loss 0.5036\n",
      "2025-11-10 13:46:54,004 - root - INFO - KG Training: Epoch 0001 Iter 0570 / 4823 | Time 0.0s | Iter Loss 0.3391 | Iter Mean Loss 0.5028\n",
      "2025-11-10 13:46:54,128 - root - INFO - KG Training: Epoch 0001 Iter 0573 / 4823 | Time 0.0s | Iter Loss 0.3563 | Iter Mean Loss 0.5020\n",
      "2025-11-10 13:46:54,465 - root - INFO - KG Training: Epoch 0001 Iter 0576 / 4823 | Time 0.3s | Iter Loss 0.3348 | Iter Mean Loss 0.5012\n",
      "2025-11-10 13:46:54,594 - root - INFO - KG Training: Epoch 0001 Iter 0579 / 4823 | Time 0.0s | Iter Loss 0.3527 | Iter Mean Loss 0.5004\n",
      "2025-11-10 13:46:54,719 - root - INFO - KG Training: Epoch 0001 Iter 0582 / 4823 | Time 0.0s | Iter Loss 0.3525 | Iter Mean Loss 0.4996\n",
      "2025-11-10 13:46:54,843 - root - INFO - KG Training: Epoch 0001 Iter 0585 / 4823 | Time 0.0s | Iter Loss 0.3403 | Iter Mean Loss 0.4988\n",
      "2025-11-10 13:46:54,969 - root - INFO - KG Training: Epoch 0001 Iter 0588 / 4823 | Time 0.0s | Iter Loss 0.3405 | Iter Mean Loss 0.4980\n",
      "2025-11-10 13:46:55,097 - root - INFO - KG Training: Epoch 0001 Iter 0591 / 4823 | Time 0.0s | Iter Loss 0.3403 | Iter Mean Loss 0.4972\n",
      "2025-11-10 13:46:55,405 - root - INFO - KG Training: Epoch 0001 Iter 0594 / 4823 | Time 0.2s | Iter Loss 0.3292 | Iter Mean Loss 0.4964\n",
      "2025-11-10 13:46:55,539 - root - INFO - KG Training: Epoch 0001 Iter 0597 / 4823 | Time 0.0s | Iter Loss 0.3486 | Iter Mean Loss 0.4956\n",
      "2025-11-10 13:46:55,667 - root - INFO - KG Training: Epoch 0001 Iter 0600 / 4823 | Time 0.0s | Iter Loss 0.3538 | Iter Mean Loss 0.4949\n",
      "2025-11-10 13:46:55,793 - root - INFO - KG Training: Epoch 0001 Iter 0603 / 4823 | Time 0.0s | Iter Loss 0.3561 | Iter Mean Loss 0.4941\n",
      "2025-11-10 13:46:55,918 - root - INFO - KG Training: Epoch 0001 Iter 0606 / 4823 | Time 0.0s | Iter Loss 0.3467 | Iter Mean Loss 0.4934\n",
      "2025-11-10 13:46:56,041 - root - INFO - KG Training: Epoch 0001 Iter 0609 / 4823 | Time 0.0s | Iter Loss 0.3307 | Iter Mean Loss 0.4926\n",
      "2025-11-10 13:46:56,165 - root - INFO - KG Training: Epoch 0001 Iter 0612 / 4823 | Time 0.0s | Iter Loss 0.3363 | Iter Mean Loss 0.4918\n",
      "2025-11-10 13:46:56,353 - root - INFO - KG Training: Epoch 0001 Iter 0615 / 4823 | Time 0.0s | Iter Loss 0.3347 | Iter Mean Loss 0.4911\n",
      "2025-11-10 13:46:56,477 - root - INFO - KG Training: Epoch 0001 Iter 0618 / 4823 | Time 0.0s | Iter Loss 0.3365 | Iter Mean Loss 0.4904\n",
      "2025-11-10 13:46:56,611 - root - INFO - KG Training: Epoch 0001 Iter 0621 / 4823 | Time 0.0s | Iter Loss 0.3422 | Iter Mean Loss 0.4896\n",
      "2025-11-10 13:46:57,214 - root - INFO - KG Training: Epoch 0001 Iter 0624 / 4823 | Time 0.0s | Iter Loss 0.3432 | Iter Mean Loss 0.4889\n",
      "2025-11-10 13:46:57,489 - root - INFO - KG Training: Epoch 0001 Iter 0627 / 4823 | Time 0.2s | Iter Loss 0.3375 | Iter Mean Loss 0.4882\n",
      "2025-11-10 13:46:57,614 - root - INFO - KG Training: Epoch 0001 Iter 0630 / 4823 | Time 0.0s | Iter Loss 0.3368 | Iter Mean Loss 0.4874\n",
      "2025-11-10 13:46:57,736 - root - INFO - KG Training: Epoch 0001 Iter 0633 / 4823 | Time 0.0s | Iter Loss 0.3344 | Iter Mean Loss 0.4867\n",
      "2025-11-10 13:46:57,862 - root - INFO - KG Training: Epoch 0001 Iter 0636 / 4823 | Time 0.0s | Iter Loss 0.3331 | Iter Mean Loss 0.4860\n",
      "2025-11-10 13:46:57,985 - root - INFO - KG Training: Epoch 0001 Iter 0639 / 4823 | Time 0.0s | Iter Loss 0.3276 | Iter Mean Loss 0.4852\n",
      "2025-11-10 13:46:58,108 - root - INFO - KG Training: Epoch 0001 Iter 0642 / 4823 | Time 0.0s | Iter Loss 0.3289 | Iter Mean Loss 0.4845\n",
      "2025-11-10 13:46:58,296 - root - INFO - KG Training: Epoch 0001 Iter 0645 / 4823 | Time 0.1s | Iter Loss 0.3349 | Iter Mean Loss 0.4838\n",
      "2025-11-10 13:46:58,422 - root - INFO - KG Training: Epoch 0001 Iter 0648 / 4823 | Time 0.0s | Iter Loss 0.3274 | Iter Mean Loss 0.4831\n",
      "2025-11-10 13:46:58,545 - root - INFO - KG Training: Epoch 0001 Iter 0651 / 4823 | Time 0.0s | Iter Loss 0.3213 | Iter Mean Loss 0.4823\n",
      "2025-11-10 13:46:58,933 - root - INFO - KG Training: Epoch 0001 Iter 0654 / 4823 | Time 0.0s | Iter Loss 0.3170 | Iter Mean Loss 0.4816\n",
      "2025-11-10 13:46:59,189 - root - INFO - KG Training: Epoch 0001 Iter 0657 / 4823 | Time 0.0s | Iter Loss 0.3248 | Iter Mean Loss 0.4809\n",
      "2025-11-10 13:46:59,314 - root - INFO - KG Training: Epoch 0001 Iter 0660 / 4823 | Time 0.0s | Iter Loss 0.3163 | Iter Mean Loss 0.4802\n",
      "2025-11-10 13:46:59,438 - root - INFO - KG Training: Epoch 0001 Iter 0663 / 4823 | Time 0.0s | Iter Loss 0.3245 | Iter Mean Loss 0.4794\n",
      "2025-11-10 13:46:59,564 - root - INFO - KG Training: Epoch 0001 Iter 0666 / 4823 | Time 0.0s | Iter Loss 0.3143 | Iter Mean Loss 0.4787\n",
      "2025-11-10 13:46:59,693 - root - INFO - KG Training: Epoch 0001 Iter 0669 / 4823 | Time 0.0s | Iter Loss 0.3210 | Iter Mean Loss 0.4780\n",
      "2025-11-10 13:46:59,819 - root - INFO - KG Training: Epoch 0001 Iter 0672 / 4823 | Time 0.0s | Iter Loss 0.3214 | Iter Mean Loss 0.4773\n",
      "2025-11-10 13:46:59,941 - root - INFO - KG Training: Epoch 0001 Iter 0675 / 4823 | Time 0.0s | Iter Loss 0.3362 | Iter Mean Loss 0.4766\n",
      "2025-11-10 13:47:00,141 - root - INFO - KG Training: Epoch 0001 Iter 0678 / 4823 | Time 0.1s | Iter Loss 0.3261 | Iter Mean Loss 0.4759\n",
      "2025-11-10 13:47:00,270 - root - INFO - KG Training: Epoch 0001 Iter 0681 / 4823 | Time 0.0s | Iter Loss 0.3138 | Iter Mean Loss 0.4752\n",
      "2025-11-10 13:47:00,396 - root - INFO - KG Training: Epoch 0001 Iter 0684 / 4823 | Time 0.0s | Iter Loss 0.3139 | Iter Mean Loss 0.4746\n",
      "2025-11-10 13:47:00,522 - root - INFO - KG Training: Epoch 0001 Iter 0687 / 4823 | Time 0.0s | Iter Loss 0.3105 | Iter Mean Loss 0.4739\n",
      "2025-11-10 13:47:00,653 - root - INFO - KG Training: Epoch 0001 Iter 0690 / 4823 | Time 0.0s | Iter Loss 0.3120 | Iter Mean Loss 0.4732\n",
      "2025-11-10 13:47:00,778 - root - INFO - KG Training: Epoch 0001 Iter 0693 / 4823 | Time 0.0s | Iter Loss 0.3248 | Iter Mean Loss 0.4725\n",
      "2025-11-10 13:47:00,900 - root - INFO - KG Training: Epoch 0001 Iter 0696 / 4823 | Time 0.0s | Iter Loss 0.3219 | Iter Mean Loss 0.4719\n",
      "2025-11-10 13:47:01,125 - root - INFO - KG Training: Epoch 0001 Iter 0699 / 4823 | Time 0.1s | Iter Loss 0.3223 | Iter Mean Loss 0.4712\n",
      "2025-11-10 13:47:01,254 - root - INFO - KG Training: Epoch 0001 Iter 0702 / 4823 | Time 0.0s | Iter Loss 0.3111 | Iter Mean Loss 0.4706\n",
      "2025-11-10 13:47:01,376 - root - INFO - KG Training: Epoch 0001 Iter 0705 / 4823 | Time 0.0s | Iter Loss 0.3112 | Iter Mean Loss 0.4699\n",
      "2025-11-10 13:47:01,660 - root - INFO - KG Training: Epoch 0001 Iter 0708 / 4823 | Time 0.0s | Iter Loss 0.3141 | Iter Mean Loss 0.4692\n",
      "2025-11-10 13:47:01,787 - root - INFO - KG Training: Epoch 0001 Iter 0711 / 4823 | Time 0.0s | Iter Loss 0.3124 | Iter Mean Loss 0.4685\n",
      "2025-11-10 13:47:01,913 - root - INFO - KG Training: Epoch 0001 Iter 0714 / 4823 | Time 0.0s | Iter Loss 0.3246 | Iter Mean Loss 0.4679\n",
      "2025-11-10 13:47:02,036 - root - INFO - KG Training: Epoch 0001 Iter 0717 / 4823 | Time 0.0s | Iter Loss 0.3145 | Iter Mean Loss 0.4673\n",
      "2025-11-10 13:47:02,160 - root - INFO - KG Training: Epoch 0001 Iter 0720 / 4823 | Time 0.0s | Iter Loss 0.3224 | Iter Mean Loss 0.4666\n",
      "2025-11-10 13:47:02,349 - root - INFO - KG Training: Epoch 0001 Iter 0723 / 4823 | Time 0.0s | Iter Loss 0.3044 | Iter Mean Loss 0.4660\n",
      "2025-11-10 13:47:02,472 - root - INFO - KG Training: Epoch 0001 Iter 0726 / 4823 | Time 0.0s | Iter Loss 0.3212 | Iter Mean Loss 0.4653\n",
      "2025-11-10 13:47:02,595 - root - INFO - KG Training: Epoch 0001 Iter 0729 / 4823 | Time 0.0s | Iter Loss 0.3094 | Iter Mean Loss 0.4647\n",
      "2025-11-10 13:47:02,788 - root - INFO - KG Training: Epoch 0001 Iter 0732 / 4823 | Time 0.0s | Iter Loss 0.3252 | Iter Mean Loss 0.4641\n",
      "2025-11-10 13:47:03,087 - root - INFO - KG Training: Epoch 0001 Iter 0735 / 4823 | Time 0.0s | Iter Loss 0.3207 | Iter Mean Loss 0.4635\n",
      "2025-11-10 13:47:03,213 - root - INFO - KG Training: Epoch 0001 Iter 0738 / 4823 | Time 0.0s | Iter Loss 0.3076 | Iter Mean Loss 0.4629\n",
      "2025-11-10 13:47:03,337 - root - INFO - KG Training: Epoch 0001 Iter 0741 / 4823 | Time 0.0s | Iter Loss 0.3054 | Iter Mean Loss 0.4623\n",
      "2025-11-10 13:47:03,464 - root - INFO - KG Training: Epoch 0001 Iter 0744 / 4823 | Time 0.0s | Iter Loss 0.3102 | Iter Mean Loss 0.4616\n",
      "2025-11-10 13:47:03,594 - root - INFO - KG Training: Epoch 0001 Iter 0747 / 4823 | Time 0.0s | Iter Loss 0.3029 | Iter Mean Loss 0.4610\n",
      "2025-11-10 13:47:03,716 - root - INFO - KG Training: Epoch 0001 Iter 0750 / 4823 | Time 0.0s | Iter Loss 0.3099 | Iter Mean Loss 0.4604\n",
      "2025-11-10 13:47:03,844 - root - INFO - KG Training: Epoch 0001 Iter 0753 / 4823 | Time 0.0s | Iter Loss 0.2991 | Iter Mean Loss 0.4597\n",
      "2025-11-10 13:47:03,968 - root - INFO - KG Training: Epoch 0001 Iter 0756 / 4823 | Time 0.0s | Iter Loss 0.2911 | Iter Mean Loss 0.4591\n",
      "2025-11-10 13:47:04,219 - root - INFO - KG Training: Epoch 0001 Iter 0759 / 4823 | Time 0.0s | Iter Loss 0.2991 | Iter Mean Loss 0.4585\n",
      "2025-11-10 13:47:04,440 - root - INFO - KG Training: Epoch 0001 Iter 0762 / 4823 | Time 0.0s | Iter Loss 0.2971 | Iter Mean Loss 0.4578\n",
      "2025-11-10 13:47:04,571 - root - INFO - KG Training: Epoch 0001 Iter 0765 / 4823 | Time 0.0s | Iter Loss 0.3244 | Iter Mean Loss 0.4573\n",
      "2025-11-10 13:47:04,698 - root - INFO - KG Training: Epoch 0001 Iter 0768 / 4823 | Time 0.0s | Iter Loss 0.2905 | Iter Mean Loss 0.4566\n",
      "2025-11-10 13:47:04,822 - root - INFO - KG Training: Epoch 0001 Iter 0771 / 4823 | Time 0.0s | Iter Loss 0.3005 | Iter Mean Loss 0.4560\n",
      "2025-11-10 13:47:04,950 - root - INFO - KG Training: Epoch 0001 Iter 0774 / 4823 | Time 0.0s | Iter Loss 0.3031 | Iter Mean Loss 0.4554\n",
      "2025-11-10 13:47:05,077 - root - INFO - KG Training: Epoch 0001 Iter 0777 / 4823 | Time 0.0s | Iter Loss 0.3009 | Iter Mean Loss 0.4548\n",
      "2025-11-10 13:47:05,201 - root - INFO - KG Training: Epoch 0001 Iter 0780 / 4823 | Time 0.0s | Iter Loss 0.2889 | Iter Mean Loss 0.4542\n",
      "2025-11-10 13:47:05,324 - root - INFO - KG Training: Epoch 0001 Iter 0783 / 4823 | Time 0.0s | Iter Loss 0.2874 | Iter Mean Loss 0.4536\n",
      "2025-11-10 13:47:05,513 - root - INFO - KG Training: Epoch 0001 Iter 0786 / 4823 | Time 0.1s | Iter Loss 0.2853 | Iter Mean Loss 0.4530\n",
      "2025-11-10 13:47:05,740 - root - INFO - KG Training: Epoch 0001 Iter 0789 / 4823 | Time 0.0s | Iter Loss 0.2894 | Iter Mean Loss 0.4524\n",
      "2025-11-10 13:47:05,869 - root - INFO - KG Training: Epoch 0001 Iter 0792 / 4823 | Time 0.0s | Iter Loss 0.2975 | Iter Mean Loss 0.4518\n",
      "2025-11-10 13:47:05,993 - root - INFO - KG Training: Epoch 0001 Iter 0795 / 4823 | Time 0.0s | Iter Loss 0.2896 | Iter Mean Loss 0.4512\n",
      "2025-11-10 13:47:06,117 - root - INFO - KG Training: Epoch 0001 Iter 0798 / 4823 | Time 0.0s | Iter Loss 0.2899 | Iter Mean Loss 0.4506\n",
      "2025-11-10 13:47:06,396 - root - INFO - KG Training: Epoch 0001 Iter 0801 / 4823 | Time 0.0s | Iter Loss 0.2952 | Iter Mean Loss 0.4500\n",
      "2025-11-10 13:47:06,521 - root - INFO - KG Training: Epoch 0001 Iter 0804 / 4823 | Time 0.0s | Iter Loss 0.2887 | Iter Mean Loss 0.4494\n",
      "2025-11-10 13:47:06,647 - root - INFO - KG Training: Epoch 0001 Iter 0807 / 4823 | Time 0.0s | Iter Loss 0.3046 | Iter Mean Loss 0.4489\n",
      "2025-11-10 13:47:06,771 - root - INFO - KG Training: Epoch 0001 Iter 0810 / 4823 | Time 0.0s | Iter Loss 0.3055 | Iter Mean Loss 0.4483\n",
      "2025-11-10 13:47:06,900 - root - INFO - KG Training: Epoch 0001 Iter 0813 / 4823 | Time 0.0s | Iter Loss 0.2946 | Iter Mean Loss 0.4477\n",
      "2025-11-10 13:47:07,093 - root - INFO - KG Training: Epoch 0001 Iter 0816 / 4823 | Time 0.0s | Iter Loss 0.2845 | Iter Mean Loss 0.4471\n",
      "2025-11-10 13:47:07,226 - root - INFO - KG Training: Epoch 0001 Iter 0819 / 4823 | Time 0.0s | Iter Loss 0.2887 | Iter Mean Loss 0.4466\n",
      "2025-11-10 13:47:07,350 - root - INFO - KG Training: Epoch 0001 Iter 0822 / 4823 | Time 0.0s | Iter Loss 0.2799 | Iter Mean Loss 0.4460\n",
      "2025-11-10 13:47:07,476 - root - INFO - KG Training: Epoch 0001 Iter 0825 / 4823 | Time 0.0s | Iter Loss 0.2839 | Iter Mean Loss 0.4454\n",
      "2025-11-10 13:47:07,598 - root - INFO - KG Training: Epoch 0001 Iter 0828 / 4823 | Time 0.0s | Iter Loss 0.2915 | Iter Mean Loss 0.4448\n",
      "2025-11-10 13:47:07,721 - root - INFO - KG Training: Epoch 0001 Iter 0831 / 4823 | Time 0.0s | Iter Loss 0.2849 | Iter Mean Loss 0.4443\n",
      "2025-11-10 13:47:07,844 - root - INFO - KG Training: Epoch 0001 Iter 0834 / 4823 | Time 0.0s | Iter Loss 0.2957 | Iter Mean Loss 0.4437\n",
      "2025-11-10 13:47:08,401 - root - INFO - KG Training: Epoch 0001 Iter 0837 / 4823 | Time 0.5s | Iter Loss 0.2922 | Iter Mean Loss 0.4432\n",
      "2025-11-10 13:47:08,617 - root - INFO - KG Training: Epoch 0001 Iter 0840 / 4823 | Time 0.0s | Iter Loss 0.2895 | Iter Mean Loss 0.4426\n",
      "2025-11-10 13:47:08,746 - root - INFO - KG Training: Epoch 0001 Iter 0843 / 4823 | Time 0.0s | Iter Loss 0.2860 | Iter Mean Loss 0.4420\n",
      "2025-11-10 13:47:08,940 - root - INFO - KG Training: Epoch 0001 Iter 0846 / 4823 | Time 0.0s | Iter Loss 0.2903 | Iter Mean Loss 0.4415\n",
      "2025-11-10 13:47:09,064 - root - INFO - KG Training: Epoch 0001 Iter 0849 / 4823 | Time 0.0s | Iter Loss 0.2855 | Iter Mean Loss 0.4410\n",
      "2025-11-10 13:47:09,192 - root - INFO - KG Training: Epoch 0001 Iter 0852 / 4823 | Time 0.0s | Iter Loss 0.2785 | Iter Mean Loss 0.4404\n",
      "2025-11-10 13:47:09,315 - root - INFO - KG Training: Epoch 0001 Iter 0855 / 4823 | Time 0.0s | Iter Loss 0.2727 | Iter Mean Loss 0.4399\n",
      "2025-11-10 13:47:09,526 - root - INFO - KG Training: Epoch 0001 Iter 0858 / 4823 | Time 0.1s | Iter Loss 0.2794 | Iter Mean Loss 0.4393\n",
      "2025-11-10 13:47:09,654 - root - INFO - KG Training: Epoch 0001 Iter 0861 / 4823 | Time 0.0s | Iter Loss 0.2823 | Iter Mean Loss 0.4388\n",
      "2025-11-10 13:47:09,783 - root - INFO - KG Training: Epoch 0001 Iter 0864 / 4823 | Time 0.0s | Iter Loss 0.2796 | Iter Mean Loss 0.4382\n",
      "2025-11-10 13:47:09,990 - root - INFO - KG Training: Epoch 0001 Iter 0867 / 4823 | Time 0.0s | Iter Loss 0.2715 | Iter Mean Loss 0.4377\n",
      "2025-11-10 13:47:10,121 - root - INFO - KG Training: Epoch 0001 Iter 0870 / 4823 | Time 0.0s | Iter Loss 0.2888 | Iter Mean Loss 0.4371\n",
      "2025-11-10 13:47:10,248 - root - INFO - KG Training: Epoch 0001 Iter 0873 / 4823 | Time 0.0s | Iter Loss 0.2757 | Iter Mean Loss 0.4366\n",
      "2025-11-10 13:47:10,375 - root - INFO - KG Training: Epoch 0001 Iter 0876 / 4823 | Time 0.0s | Iter Loss 0.2688 | Iter Mean Loss 0.4360\n",
      "2025-11-10 13:47:10,502 - root - INFO - KG Training: Epoch 0001 Iter 0879 / 4823 | Time 0.0s | Iter Loss 0.2758 | Iter Mean Loss 0.4355\n",
      "2025-11-10 13:47:10,730 - root - INFO - KG Training: Epoch 0001 Iter 0882 / 4823 | Time 0.0s | Iter Loss 0.2727 | Iter Mean Loss 0.4350\n",
      "2025-11-10 13:47:10,860 - root - INFO - KG Training: Epoch 0001 Iter 0885 / 4823 | Time 0.0s | Iter Loss 0.2697 | Iter Mean Loss 0.4344\n",
      "2025-11-10 13:47:10,985 - root - INFO - KG Training: Epoch 0001 Iter 0888 / 4823 | Time 0.0s | Iter Loss 0.2846 | Iter Mean Loss 0.4339\n",
      "2025-11-10 13:47:11,115 - root - INFO - KG Training: Epoch 0001 Iter 0891 / 4823 | Time 0.0s | Iter Loss 0.2831 | Iter Mean Loss 0.4334\n",
      "2025-11-10 13:47:11,241 - root - INFO - KG Training: Epoch 0001 Iter 0894 / 4823 | Time 0.0s | Iter Loss 0.2718 | Iter Mean Loss 0.4329\n",
      "2025-11-10 13:47:11,370 - root - INFO - KG Training: Epoch 0001 Iter 0897 / 4823 | Time 0.0s | Iter Loss 0.2683 | Iter Mean Loss 0.4324\n",
      "2025-11-10 13:47:11,560 - root - INFO - KG Training: Epoch 0001 Iter 0900 / 4823 | Time 0.0s | Iter Loss 0.2680 | Iter Mean Loss 0.4318\n",
      "2025-11-10 13:47:11,782 - root - INFO - KG Training: Epoch 0001 Iter 0903 / 4823 | Time 0.1s | Iter Loss 0.2788 | Iter Mean Loss 0.4313\n",
      "2025-11-10 13:47:11,937 - root - INFO - KG Training: Epoch 0001 Iter 0906 / 4823 | Time 0.0s | Iter Loss 0.2875 | Iter Mean Loss 0.4308\n",
      "2025-11-10 13:47:12,074 - root - INFO - KG Training: Epoch 0001 Iter 0909 / 4823 | Time 0.0s | Iter Loss 0.2701 | Iter Mean Loss 0.4303\n",
      "2025-11-10 13:47:12,212 - root - INFO - KG Training: Epoch 0001 Iter 0912 / 4823 | Time 0.0s | Iter Loss 0.2674 | Iter Mean Loss 0.4298\n",
      "2025-11-10 13:47:12,346 - root - INFO - KG Training: Epoch 0001 Iter 0915 / 4823 | Time 0.0s | Iter Loss 0.2827 | Iter Mean Loss 0.4293\n",
      "2025-11-10 13:47:12,546 - root - INFO - KG Training: Epoch 0001 Iter 0918 / 4823 | Time 0.0s | Iter Loss 0.2824 | Iter Mean Loss 0.4288\n",
      "2025-11-10 13:47:12,786 - root - INFO - KG Training: Epoch 0001 Iter 0921 / 4823 | Time 0.0s | Iter Loss 0.2778 | Iter Mean Loss 0.4283\n",
      "2025-11-10 13:47:13,031 - root - INFO - KG Training: Epoch 0001 Iter 0924 / 4823 | Time 0.2s | Iter Loss 0.2802 | Iter Mean Loss 0.4278\n",
      "2025-11-10 13:47:13,240 - root - INFO - KG Training: Epoch 0001 Iter 0927 / 4823 | Time 0.0s | Iter Loss 0.2716 | Iter Mean Loss 0.4273\n",
      "2025-11-10 13:47:13,368 - root - INFO - KG Training: Epoch 0001 Iter 0930 / 4823 | Time 0.0s | Iter Loss 0.2517 | Iter Mean Loss 0.4268\n",
      "2025-11-10 13:47:13,503 - root - INFO - KG Training: Epoch 0001 Iter 0933 / 4823 | Time 0.0s | Iter Loss 0.2726 | Iter Mean Loss 0.4263\n",
      "2025-11-10 13:47:14,194 - root - INFO - KG Training: Epoch 0001 Iter 0936 / 4823 | Time 0.1s | Iter Loss 0.2721 | Iter Mean Loss 0.4258\n",
      "2025-11-10 13:47:14,329 - root - INFO - KG Training: Epoch 0001 Iter 0939 / 4823 | Time 0.0s | Iter Loss 0.2830 | Iter Mean Loss 0.4253\n",
      "2025-11-10 13:47:14,454 - root - INFO - KG Training: Epoch 0001 Iter 0942 / 4823 | Time 0.0s | Iter Loss 0.2775 | Iter Mean Loss 0.4248\n",
      "2025-11-10 13:47:14,585 - root - INFO - KG Training: Epoch 0001 Iter 0945 / 4823 | Time 0.0s | Iter Loss 0.2570 | Iter Mean Loss 0.4243\n",
      "2025-11-10 13:47:14,712 - root - INFO - KG Training: Epoch 0001 Iter 0948 / 4823 | Time 0.0s | Iter Loss 0.2642 | Iter Mean Loss 0.4238\n",
      "2025-11-10 13:47:14,835 - root - INFO - KG Training: Epoch 0001 Iter 0951 / 4823 | Time 0.0s | Iter Loss 0.2840 | Iter Mean Loss 0.4233\n",
      "2025-11-10 13:47:14,959 - root - INFO - KG Training: Epoch 0001 Iter 0954 / 4823 | Time 0.0s | Iter Loss 0.2688 | Iter Mean Loss 0.4229\n",
      "2025-11-10 13:47:15,084 - root - INFO - KG Training: Epoch 0001 Iter 0957 / 4823 | Time 0.0s | Iter Loss 0.2575 | Iter Mean Loss 0.4224\n",
      "2025-11-10 13:47:15,208 - root - INFO - KG Training: Epoch 0001 Iter 0960 / 4823 | Time 0.0s | Iter Loss 0.2665 | Iter Mean Loss 0.4219\n",
      "2025-11-10 13:47:15,515 - root - INFO - KG Training: Epoch 0001 Iter 0963 / 4823 | Time 0.0s | Iter Loss 0.2578 | Iter Mean Loss 0.4214\n",
      "2025-11-10 13:47:15,706 - root - INFO - KG Training: Epoch 0001 Iter 0966 / 4823 | Time 0.1s | Iter Loss 0.2565 | Iter Mean Loss 0.4209\n",
      "2025-11-10 13:47:15,831 - root - INFO - KG Training: Epoch 0001 Iter 0969 / 4823 | Time 0.0s | Iter Loss 0.2612 | Iter Mean Loss 0.4204\n",
      "2025-11-10 13:47:16,112 - root - INFO - KG Training: Epoch 0001 Iter 0972 / 4823 | Time 0.0s | Iter Loss 0.2711 | Iter Mean Loss 0.4199\n",
      "2025-11-10 13:47:16,242 - root - INFO - KG Training: Epoch 0001 Iter 0975 / 4823 | Time 0.0s | Iter Loss 0.2571 | Iter Mean Loss 0.4195\n",
      "2025-11-10 13:47:16,365 - root - INFO - KG Training: Epoch 0001 Iter 0978 / 4823 | Time 0.0s | Iter Loss 0.2598 | Iter Mean Loss 0.4190\n",
      "2025-11-10 13:47:16,487 - root - INFO - KG Training: Epoch 0001 Iter 0981 / 4823 | Time 0.0s | Iter Loss 0.2620 | Iter Mean Loss 0.4185\n",
      "2025-11-10 13:47:16,611 - root - INFO - KG Training: Epoch 0001 Iter 0984 / 4823 | Time 0.0s | Iter Loss 0.2602 | Iter Mean Loss 0.4181\n",
      "2025-11-10 13:47:16,735 - root - INFO - KG Training: Epoch 0001 Iter 0987 / 4823 | Time 0.0s | Iter Loss 0.2634 | Iter Mean Loss 0.4176\n",
      "2025-11-10 13:47:17,036 - root - INFO - KG Training: Epoch 0001 Iter 0990 / 4823 | Time 0.0s | Iter Loss 0.2603 | Iter Mean Loss 0.4172\n",
      "2025-11-10 13:47:17,165 - root - INFO - KG Training: Epoch 0001 Iter 0993 / 4823 | Time 0.0s | Iter Loss 0.2567 | Iter Mean Loss 0.4167\n",
      "2025-11-10 13:47:17,321 - root - INFO - KG Training: Epoch 0001 Iter 0996 / 4823 | Time 0.1s | Iter Loss 0.2550 | Iter Mean Loss 0.4162\n",
      "2025-11-10 13:47:17,446 - root - INFO - KG Training: Epoch 0001 Iter 0999 / 4823 | Time 0.0s | Iter Loss 0.2719 | Iter Mean Loss 0.4157\n",
      "2025-11-10 13:47:17,570 - root - INFO - KG Training: Epoch 0001 Iter 1002 / 4823 | Time 0.0s | Iter Loss 0.2575 | Iter Mean Loss 0.4152\n",
      "2025-11-10 13:47:17,694 - root - INFO - KG Training: Epoch 0001 Iter 1005 / 4823 | Time 0.0s | Iter Loss 0.2654 | Iter Mean Loss 0.4148\n",
      "2025-11-10 13:47:17,818 - root - INFO - KG Training: Epoch 0001 Iter 1008 / 4823 | Time 0.0s | Iter Loss 0.2462 | Iter Mean Loss 0.4143\n",
      "2025-11-10 13:47:18,114 - root - INFO - KG Training: Epoch 0001 Iter 1011 / 4823 | Time 0.0s | Iter Loss 0.2576 | Iter Mean Loss 0.4138\n",
      "2025-11-10 13:47:18,236 - root - INFO - KG Training: Epoch 0001 Iter 1014 / 4823 | Time 0.0s | Iter Loss 0.2628 | Iter Mean Loss 0.4134\n",
      "2025-11-10 13:47:18,368 - root - INFO - KG Training: Epoch 0001 Iter 1017 / 4823 | Time 0.0s | Iter Loss 0.2566 | Iter Mean Loss 0.4129\n",
      "2025-11-10 13:47:18,490 - root - INFO - KG Training: Epoch 0001 Iter 1020 / 4823 | Time 0.0s | Iter Loss 0.2410 | Iter Mean Loss 0.4124\n",
      "2025-11-10 13:47:18,685 - root - INFO - KG Training: Epoch 0001 Iter 1023 / 4823 | Time 0.0s | Iter Loss 0.2583 | Iter Mean Loss 0.4120\n",
      "2025-11-10 13:47:18,874 - root - INFO - KG Training: Epoch 0001 Iter 1026 / 4823 | Time 0.1s | Iter Loss 0.2665 | Iter Mean Loss 0.4115\n",
      "2025-11-10 13:47:18,998 - root - INFO - KG Training: Epoch 0001 Iter 1029 / 4823 | Time 0.0s | Iter Loss 0.2514 | Iter Mean Loss 0.4111\n",
      "2025-11-10 13:47:19,121 - root - INFO - KG Training: Epoch 0001 Iter 1032 / 4823 | Time 0.0s | Iter Loss 0.2553 | Iter Mean Loss 0.4106\n",
      "2025-11-10 13:47:19,244 - root - INFO - KG Training: Epoch 0001 Iter 1035 / 4823 | Time 0.0s | Iter Loss 0.2433 | Iter Mean Loss 0.4102\n",
      "2025-11-10 13:47:19,368 - root - INFO - KG Training: Epoch 0001 Iter 1038 / 4823 | Time 0.0s | Iter Loss 0.2539 | Iter Mean Loss 0.4097\n",
      "2025-11-10 13:47:19,496 - root - INFO - KG Training: Epoch 0001 Iter 1041 / 4823 | Time 0.0s | Iter Loss 0.2515 | Iter Mean Loss 0.4093\n",
      "2025-11-10 13:47:19,685 - root - INFO - KG Training: Epoch 0001 Iter 1044 / 4823 | Time 0.0s | Iter Loss 0.2685 | Iter Mean Loss 0.4089\n",
      "2025-11-10 13:47:20,028 - root - INFO - KG Training: Epoch 0001 Iter 1047 / 4823 | Time 0.0s | Iter Loss 0.2486 | Iter Mean Loss 0.4084\n",
      "2025-11-10 13:47:20,152 - root - INFO - KG Training: Epoch 0001 Iter 1050 / 4823 | Time 0.0s | Iter Loss 0.2490 | Iter Mean Loss 0.4079\n",
      "2025-11-10 13:47:20,277 - root - INFO - KG Training: Epoch 0001 Iter 1053 / 4823 | Time 0.0s | Iter Loss 0.2436 | Iter Mean Loss 0.4075\n",
      "2025-11-10 13:47:20,401 - root - INFO - KG Training: Epoch 0001 Iter 1056 / 4823 | Time 0.0s | Iter Loss 0.2550 | Iter Mean Loss 0.4071\n",
      "2025-11-10 13:47:20,525 - root - INFO - KG Training: Epoch 0001 Iter 1059 / 4823 | Time 0.0s | Iter Loss 0.2510 | Iter Mean Loss 0.4067\n",
      "2025-11-10 13:47:20,655 - root - INFO - KG Training: Epoch 0001 Iter 1062 / 4823 | Time 0.0s | Iter Loss 0.2609 | Iter Mean Loss 0.4062\n",
      "2025-11-10 13:47:20,884 - root - INFO - KG Training: Epoch 0001 Iter 1065 / 4823 | Time 0.0s | Iter Loss 0.2582 | Iter Mean Loss 0.4058\n",
      "2025-11-10 13:47:21,014 - root - INFO - KG Training: Epoch 0001 Iter 1068 / 4823 | Time 0.0s | Iter Loss 0.2491 | Iter Mean Loss 0.4054\n",
      "2025-11-10 13:47:21,142 - root - INFO - KG Training: Epoch 0001 Iter 1071 / 4823 | Time 0.0s | Iter Loss 0.2449 | Iter Mean Loss 0.4049\n",
      "2025-11-10 13:47:21,334 - root - INFO - KG Training: Epoch 0001 Iter 1074 / 4823 | Time 0.0s | Iter Loss 0.2634 | Iter Mean Loss 0.4045\n",
      "2025-11-10 13:47:21,457 - root - INFO - KG Training: Epoch 0001 Iter 1077 / 4823 | Time 0.0s | Iter Loss 0.2524 | Iter Mean Loss 0.4041\n",
      "2025-11-10 13:47:21,589 - root - INFO - KG Training: Epoch 0001 Iter 1080 / 4823 | Time 0.0s | Iter Loss 0.2485 | Iter Mean Loss 0.4037\n",
      "2025-11-10 13:47:21,715 - root - INFO - KG Training: Epoch 0001 Iter 1083 / 4823 | Time 0.0s | Iter Loss 0.2473 | Iter Mean Loss 0.4032\n",
      "2025-11-10 13:47:21,837 - root - INFO - KG Training: Epoch 0001 Iter 1086 / 4823 | Time 0.0s | Iter Loss 0.2457 | Iter Mean Loss 0.4028\n",
      "2025-11-10 13:47:21,963 - root - INFO - KG Training: Epoch 0001 Iter 1089 / 4823 | Time 0.0s | Iter Loss 0.2500 | Iter Mean Loss 0.4024\n",
      "2025-11-10 13:47:22,087 - root - INFO - KG Training: Epoch 0001 Iter 1092 / 4823 | Time 0.0s | Iter Loss 0.2461 | Iter Mean Loss 0.4020\n",
      "2025-11-10 13:47:22,215 - root - INFO - KG Training: Epoch 0001 Iter 1095 / 4823 | Time 0.0s | Iter Loss 0.2541 | Iter Mean Loss 0.4016\n",
      "2025-11-10 13:47:22,425 - root - INFO - KG Training: Epoch 0001 Iter 1098 / 4823 | Time 0.0s | Iter Loss 0.2499 | Iter Mean Loss 0.4012\n",
      "2025-11-10 13:47:22,549 - root - INFO - KG Training: Epoch 0001 Iter 1101 / 4823 | Time 0.0s | Iter Loss 0.2562 | Iter Mean Loss 0.4007\n",
      "2025-11-10 13:47:22,672 - root - INFO - KG Training: Epoch 0001 Iter 1104 / 4823 | Time 0.0s | Iter Loss 0.2374 | Iter Mean Loss 0.4003\n",
      "2025-11-10 13:47:22,800 - root - INFO - KG Training: Epoch 0001 Iter 1107 / 4823 | Time 0.0s | Iter Loss 0.2460 | Iter Mean Loss 0.3999\n",
      "2025-11-10 13:47:22,988 - root - INFO - KG Training: Epoch 0001 Iter 1110 / 4823 | Time 0.0s | Iter Loss 0.2403 | Iter Mean Loss 0.3995\n",
      "2025-11-10 13:47:23,235 - root - INFO - KG Training: Epoch 0001 Iter 1113 / 4823 | Time 0.0s | Iter Loss 0.2546 | Iter Mean Loss 0.3991\n",
      "2025-11-10 13:47:23,398 - root - INFO - KG Training: Epoch 0001 Iter 1116 / 4823 | Time 0.0s | Iter Loss 0.2434 | Iter Mean Loss 0.3987\n",
      "2025-11-10 13:47:23,524 - root - INFO - KG Training: Epoch 0001 Iter 1119 / 4823 | Time 0.0s | Iter Loss 0.2422 | Iter Mean Loss 0.3983\n",
      "2025-11-10 13:47:23,718 - root - INFO - KG Training: Epoch 0001 Iter 1122 / 4823 | Time 0.0s | Iter Loss 0.2345 | Iter Mean Loss 0.3978\n",
      "2025-11-10 13:47:23,848 - root - INFO - KG Training: Epoch 0001 Iter 1125 / 4823 | Time 0.0s | Iter Loss 0.2436 | Iter Mean Loss 0.3974\n",
      "2025-11-10 13:47:24,209 - root - INFO - KG Training: Epoch 0001 Iter 1128 / 4823 | Time 0.1s | Iter Loss 0.2428 | Iter Mean Loss 0.3970\n",
      "2025-11-10 13:47:24,414 - root - INFO - KG Training: Epoch 0001 Iter 1131 / 4823 | Time 0.0s | Iter Loss 0.2370 | Iter Mean Loss 0.3966\n",
      "2025-11-10 13:47:24,539 - root - INFO - KG Training: Epoch 0001 Iter 1134 / 4823 | Time 0.0s | Iter Loss 0.2579 | Iter Mean Loss 0.3962\n",
      "2025-11-10 13:47:24,663 - root - INFO - KG Training: Epoch 0001 Iter 1137 / 4823 | Time 0.0s | Iter Loss 0.2363 | Iter Mean Loss 0.3957\n",
      "2025-11-10 13:47:24,791 - root - INFO - KG Training: Epoch 0001 Iter 1140 / 4823 | Time 0.0s | Iter Loss 0.2471 | Iter Mean Loss 0.3953\n",
      "2025-11-10 13:47:24,919 - root - INFO - KG Training: Epoch 0001 Iter 1143 / 4823 | Time 0.0s | Iter Loss 0.2501 | Iter Mean Loss 0.3949\n",
      "2025-11-10 13:47:25,046 - root - INFO - KG Training: Epoch 0001 Iter 1146 / 4823 | Time 0.0s | Iter Loss 0.2424 | Iter Mean Loss 0.3946\n",
      "2025-11-10 13:47:25,170 - root - INFO - KG Training: Epoch 0001 Iter 1149 / 4823 | Time 0.0s | Iter Loss 0.2483 | Iter Mean Loss 0.3941\n",
      "2025-11-10 13:47:25,294 - root - INFO - KG Training: Epoch 0001 Iter 1152 / 4823 | Time 0.0s | Iter Loss 0.2376 | Iter Mean Loss 0.3938\n",
      "2025-11-10 13:47:25,420 - root - INFO - KG Training: Epoch 0001 Iter 1155 / 4823 | Time 0.0s | Iter Loss 0.2519 | Iter Mean Loss 0.3934\n",
      "2025-11-10 13:47:25,541 - root - INFO - KG Training: Epoch 0001 Iter 1158 / 4823 | Time 0.0s | Iter Loss 0.2294 | Iter Mean Loss 0.3930\n",
      "2025-11-10 13:47:25,731 - root - INFO - KG Training: Epoch 0001 Iter 1161 / 4823 | Time 0.0s | Iter Loss 0.2412 | Iter Mean Loss 0.3925\n",
      "2025-11-10 13:47:25,944 - root - INFO - KG Training: Epoch 0001 Iter 1164 / 4823 | Time 0.0s | Iter Loss 0.2451 | Iter Mean Loss 0.3922\n",
      "2025-11-10 13:47:26,073 - root - INFO - KG Training: Epoch 0001 Iter 1167 / 4823 | Time 0.0s | Iter Loss 0.2263 | Iter Mean Loss 0.3918\n",
      "2025-11-10 13:47:26,196 - root - INFO - KG Training: Epoch 0001 Iter 1170 / 4823 | Time 0.0s | Iter Loss 0.2523 | Iter Mean Loss 0.3914\n",
      "2025-11-10 13:47:26,319 - root - INFO - KG Training: Epoch 0001 Iter 1173 / 4823 | Time 0.0s | Iter Loss 0.2429 | Iter Mean Loss 0.3910\n",
      "2025-11-10 13:47:26,655 - root - INFO - KG Training: Epoch 0001 Iter 1176 / 4823 | Time 0.0s | Iter Loss 0.2382 | Iter Mean Loss 0.3906\n",
      "2025-11-10 13:47:26,781 - root - INFO - KG Training: Epoch 0001 Iter 1179 / 4823 | Time 0.0s | Iter Loss 0.2415 | Iter Mean Loss 0.3902\n",
      "2025-11-10 13:47:27,080 - root - INFO - KG Training: Epoch 0001 Iter 1182 / 4823 | Time 0.1s | Iter Loss 0.2285 | Iter Mean Loss 0.3898\n",
      "2025-11-10 13:47:27,295 - root - INFO - KG Training: Epoch 0001 Iter 1185 / 4823 | Time 0.0s | Iter Loss 0.2427 | Iter Mean Loss 0.3895\n",
      "2025-11-10 13:47:27,420 - root - INFO - KG Training: Epoch 0001 Iter 1188 / 4823 | Time 0.0s | Iter Loss 0.2366 | Iter Mean Loss 0.3891\n",
      "2025-11-10 13:47:27,550 - root - INFO - KG Training: Epoch 0001 Iter 1191 / 4823 | Time 0.0s | Iter Loss 0.2288 | Iter Mean Loss 0.3887\n",
      "2025-11-10 13:47:27,671 - root - INFO - KG Training: Epoch 0001 Iter 1194 / 4823 | Time 0.0s | Iter Loss 0.2321 | Iter Mean Loss 0.3883\n",
      "2025-11-10 13:47:27,801 - root - INFO - KG Training: Epoch 0001 Iter 1197 / 4823 | Time 0.0s | Iter Loss 0.2439 | Iter Mean Loss 0.3879\n",
      "2025-11-10 13:47:27,989 - root - INFO - KG Training: Epoch 0001 Iter 1200 / 4823 | Time 0.0s | Iter Loss 0.2392 | Iter Mean Loss 0.3876\n",
      "2025-11-10 13:47:28,112 - root - INFO - KG Training: Epoch 0001 Iter 1203 / 4823 | Time 0.0s | Iter Loss 0.2344 | Iter Mean Loss 0.3872\n",
      "2025-11-10 13:47:28,236 - root - INFO - KG Training: Epoch 0001 Iter 1206 / 4823 | Time 0.0s | Iter Loss 0.2489 | Iter Mean Loss 0.3868\n",
      "2025-11-10 13:47:28,361 - root - INFO - KG Training: Epoch 0001 Iter 1209 / 4823 | Time 0.0s | Iter Loss 0.2444 | Iter Mean Loss 0.3864\n",
      "2025-11-10 13:47:28,634 - root - INFO - KG Training: Epoch 0001 Iter 1212 / 4823 | Time 0.1s | Iter Loss 0.2250 | Iter Mean Loss 0.3861\n",
      "2025-11-10 13:47:29,083 - root - INFO - KG Training: Epoch 0001 Iter 1215 / 4823 | Time 0.0s | Iter Loss 0.2390 | Iter Mean Loss 0.3857\n",
      "2025-11-10 13:47:29,215 - root - INFO - KG Training: Epoch 0001 Iter 1218 / 4823 | Time 0.0s | Iter Loss 0.2479 | Iter Mean Loss 0.3853\n",
      "2025-11-10 13:47:29,343 - root - INFO - KG Training: Epoch 0001 Iter 1221 / 4823 | Time 0.0s | Iter Loss 0.2240 | Iter Mean Loss 0.3849\n",
      "2025-11-10 13:47:29,468 - root - INFO - KG Training: Epoch 0001 Iter 1224 / 4823 | Time 0.0s | Iter Loss 0.2394 | Iter Mean Loss 0.3846\n",
      "2025-11-10 13:47:29,590 - root - INFO - KG Training: Epoch 0001 Iter 1227 / 4823 | Time 0.0s | Iter Loss 0.2279 | Iter Mean Loss 0.3842\n",
      "2025-11-10 13:47:29,746 - root - INFO - KG Training: Epoch 0001 Iter 1230 / 4823 | Time 0.0s | Iter Loss 0.2324 | Iter Mean Loss 0.3838\n",
      "2025-11-10 13:47:29,894 - root - INFO - KG Training: Epoch 0001 Iter 1233 / 4823 | Time 0.1s | Iter Loss 0.2427 | Iter Mean Loss 0.3834\n",
      "2025-11-10 13:47:30,200 - root - INFO - KG Training: Epoch 0001 Iter 1236 / 4823 | Time 0.2s | Iter Loss 0.2212 | Iter Mean Loss 0.3831\n",
      "2025-11-10 13:47:30,332 - root - INFO - KG Training: Epoch 0001 Iter 1239 / 4823 | Time 0.0s | Iter Loss 0.2431 | Iter Mean Loss 0.3827\n",
      "2025-11-10 13:47:30,455 - root - INFO - KG Training: Epoch 0001 Iter 1242 / 4823 | Time 0.0s | Iter Loss 0.2260 | Iter Mean Loss 0.3824\n",
      "2025-11-10 13:47:30,645 - root - INFO - KG Training: Epoch 0001 Iter 1245 / 4823 | Time 0.0s | Iter Loss 0.2292 | Iter Mean Loss 0.3820\n",
      "2025-11-10 13:47:30,767 - root - INFO - KG Training: Epoch 0001 Iter 1248 / 4823 | Time 0.0s | Iter Loss 0.2126 | Iter Mean Loss 0.3816\n",
      "2025-11-10 13:47:30,891 - root - INFO - KG Training: Epoch 0001 Iter 1251 / 4823 | Time 0.0s | Iter Loss 0.2292 | Iter Mean Loss 0.3813\n",
      "2025-11-10 13:47:31,022 - root - INFO - KG Training: Epoch 0001 Iter 1254 / 4823 | Time 0.0s | Iter Loss 0.2309 | Iter Mean Loss 0.3809\n",
      "2025-11-10 13:47:31,145 - root - INFO - KG Training: Epoch 0001 Iter 1257 / 4823 | Time 0.0s | Iter Loss 0.2229 | Iter Mean Loss 0.3806\n",
      "2025-11-10 13:47:31,496 - root - INFO - KG Training: Epoch 0001 Iter 1260 / 4823 | Time 0.1s | Iter Loss 0.2247 | Iter Mean Loss 0.3802\n",
      "2025-11-10 13:47:31,622 - root - INFO - KG Training: Epoch 0001 Iter 1263 / 4823 | Time 0.0s | Iter Loss 0.2374 | Iter Mean Loss 0.3798\n",
      "2025-11-10 13:47:31,915 - root - INFO - KG Training: Epoch 0001 Iter 1266 / 4823 | Time 0.0s | Iter Loss 0.2272 | Iter Mean Loss 0.3795\n",
      "2025-11-10 13:47:32,038 - root - INFO - KG Training: Epoch 0001 Iter 1269 / 4823 | Time 0.0s | Iter Loss 0.2276 | Iter Mean Loss 0.3792\n",
      "2025-11-10 13:47:32,161 - root - INFO - KG Training: Epoch 0001 Iter 1272 / 4823 | Time 0.0s | Iter Loss 0.2327 | Iter Mean Loss 0.3788\n",
      "2025-11-10 13:47:32,351 - root - INFO - KG Training: Epoch 0001 Iter 1275 / 4823 | Time 0.0s | Iter Loss 0.2361 | Iter Mean Loss 0.3785\n",
      "2025-11-10 13:47:32,474 - root - INFO - KG Training: Epoch 0001 Iter 1278 / 4823 | Time 0.0s | Iter Loss 0.2229 | Iter Mean Loss 0.3781\n",
      "2025-11-10 13:47:32,598 - root - INFO - KG Training: Epoch 0001 Iter 1281 / 4823 | Time 0.0s | Iter Loss 0.2340 | Iter Mean Loss 0.3778\n",
      "2025-11-10 13:47:32,721 - root - INFO - KG Training: Epoch 0001 Iter 1284 / 4823 | Time 0.0s | Iter Loss 0.2305 | Iter Mean Loss 0.3774\n",
      "2025-11-10 13:47:32,847 - root - INFO - KG Training: Epoch 0001 Iter 1287 / 4823 | Time 0.0s | Iter Loss 0.2325 | Iter Mean Loss 0.3771\n",
      "2025-11-10 13:47:32,971 - root - INFO - KG Training: Epoch 0001 Iter 1290 / 4823 | Time 0.0s | Iter Loss 0.2313 | Iter Mean Loss 0.3767\n",
      "2025-11-10 13:47:33,096 - root - INFO - KG Training: Epoch 0001 Iter 1293 / 4823 | Time 0.0s | Iter Loss 0.2218 | Iter Mean Loss 0.3764\n",
      "2025-11-10 13:47:33,223 - root - INFO - KG Training: Epoch 0001 Iter 1296 / 4823 | Time 0.0s | Iter Loss 0.2278 | Iter Mean Loss 0.3760\n",
      "2025-11-10 13:47:33,348 - root - INFO - KG Training: Epoch 0001 Iter 1299 / 4823 | Time 0.0s | Iter Loss 0.2088 | Iter Mean Loss 0.3757\n",
      "2025-11-10 13:47:33,625 - root - INFO - KG Training: Epoch 0001 Iter 1302 / 4823 | Time 0.0s | Iter Loss 0.2245 | Iter Mean Loss 0.3753\n",
      "2025-11-10 13:47:33,753 - root - INFO - KG Training: Epoch 0001 Iter 1305 / 4823 | Time 0.0s | Iter Loss 0.2336 | Iter Mean Loss 0.3750\n",
      "2025-11-10 13:47:33,873 - root - INFO - KG Training: Epoch 0001 Iter 1308 / 4823 | Time 0.0s | Iter Loss 0.2232 | Iter Mean Loss 0.3746\n",
      "2025-11-10 13:47:33,997 - root - INFO - KG Training: Epoch 0001 Iter 1311 / 4823 | Time 0.0s | Iter Loss 0.2152 | Iter Mean Loss 0.3743\n",
      "2025-11-10 13:47:34,476 - root - INFO - KG Training: Epoch 0001 Iter 1314 / 4823 | Time 0.4s | Iter Loss 0.2182 | Iter Mean Loss 0.3739\n",
      "2025-11-10 13:47:34,604 - root - INFO - KG Training: Epoch 0001 Iter 1317 / 4823 | Time 0.0s | Iter Loss 0.2309 | Iter Mean Loss 0.3736\n",
      "2025-11-10 13:47:34,729 - root - INFO - KG Training: Epoch 0001 Iter 1320 / 4823 | Time 0.0s | Iter Loss 0.2209 | Iter Mean Loss 0.3732\n",
      "2025-11-10 13:47:34,856 - root - INFO - KG Training: Epoch 0001 Iter 1323 / 4823 | Time 0.0s | Iter Loss 0.2313 | Iter Mean Loss 0.3729\n",
      "2025-11-10 13:47:35,017 - root - INFO - KG Training: Epoch 0001 Iter 1326 / 4823 | Time 0.1s | Iter Loss 0.2106 | Iter Mean Loss 0.3726\n",
      "2025-11-10 13:47:35,144 - root - INFO - KG Training: Epoch 0001 Iter 1329 / 4823 | Time 0.0s | Iter Loss 0.2258 | Iter Mean Loss 0.3722\n",
      "2025-11-10 13:47:35,268 - root - INFO - KG Training: Epoch 0001 Iter 1332 / 4823 | Time 0.0s | Iter Loss 0.2173 | Iter Mean Loss 0.3719\n",
      "2025-11-10 13:47:35,390 - root - INFO - KG Training: Epoch 0001 Iter 1335 / 4823 | Time 0.0s | Iter Loss 0.2137 | Iter Mean Loss 0.3716\n",
      "2025-11-10 13:47:35,511 - root - INFO - KG Training: Epoch 0001 Iter 1338 / 4823 | Time 0.0s | Iter Loss 0.2189 | Iter Mean Loss 0.3712\n",
      "2025-11-10 13:47:35,638 - root - INFO - KG Training: Epoch 0001 Iter 1341 / 4823 | Time 0.0s | Iter Loss 0.2182 | Iter Mean Loss 0.3709\n",
      "2025-11-10 13:47:35,772 - root - INFO - KG Training: Epoch 0001 Iter 1344 / 4823 | Time 0.0s | Iter Loss 0.2180 | Iter Mean Loss 0.3706\n",
      "2025-11-10 13:47:35,897 - root - INFO - KG Training: Epoch 0001 Iter 1347 / 4823 | Time 0.0s | Iter Loss 0.2238 | Iter Mean Loss 0.3703\n",
      "2025-11-10 13:47:36,019 - root - INFO - KG Training: Epoch 0001 Iter 1350 / 4823 | Time 0.0s | Iter Loss 0.2128 | Iter Mean Loss 0.3699\n",
      "2025-11-10 13:47:36,467 - root - INFO - KG Training: Epoch 0001 Iter 1353 / 4823 | Time 0.0s | Iter Loss 0.2137 | Iter Mean Loss 0.3696\n",
      "2025-11-10 13:47:36,596 - root - INFO - KG Training: Epoch 0001 Iter 1356 / 4823 | Time 0.0s | Iter Loss 0.2333 | Iter Mean Loss 0.3693\n",
      "2025-11-10 13:47:36,844 - root - INFO - KG Training: Epoch 0001 Iter 1359 / 4823 | Time 0.0s | Iter Loss 0.2167 | Iter Mean Loss 0.3689\n",
      "2025-11-10 13:47:36,975 - root - INFO - KG Training: Epoch 0001 Iter 1362 / 4823 | Time 0.0s | Iter Loss 0.2114 | Iter Mean Loss 0.3686\n",
      "2025-11-10 13:47:37,101 - root - INFO - KG Training: Epoch 0001 Iter 1365 / 4823 | Time 0.0s | Iter Loss 0.2219 | Iter Mean Loss 0.3683\n",
      "2025-11-10 13:47:37,309 - root - INFO - KG Training: Epoch 0001 Iter 1368 / 4823 | Time 0.1s | Iter Loss 0.2167 | Iter Mean Loss 0.3680\n",
      "2025-11-10 13:47:37,435 - root - INFO - KG Training: Epoch 0001 Iter 1371 / 4823 | Time 0.0s | Iter Loss 0.2184 | Iter Mean Loss 0.3676\n",
      "2025-11-10 13:47:37,624 - root - INFO - KG Training: Epoch 0001 Iter 1374 / 4823 | Time 0.0s | Iter Loss 0.2199 | Iter Mean Loss 0.3673\n",
      "2025-11-10 13:47:37,749 - root - INFO - KG Training: Epoch 0001 Iter 1377 / 4823 | Time 0.0s | Iter Loss 0.2345 | Iter Mean Loss 0.3670\n",
      "2025-11-10 13:47:37,873 - root - INFO - KG Training: Epoch 0001 Iter 1380 / 4823 | Time 0.0s | Iter Loss 0.2286 | Iter Mean Loss 0.3667\n",
      "2025-11-10 13:47:38,000 - root - INFO - KG Training: Epoch 0001 Iter 1383 / 4823 | Time 0.0s | Iter Loss 0.2097 | Iter Mean Loss 0.3664\n",
      "2025-11-10 13:47:38,127 - root - INFO - KG Training: Epoch 0001 Iter 1386 / 4823 | Time 0.0s | Iter Loss 0.2289 | Iter Mean Loss 0.3661\n",
      "2025-11-10 13:47:38,257 - root - INFO - KG Training: Epoch 0001 Iter 1389 / 4823 | Time 0.0s | Iter Loss 0.2321 | Iter Mean Loss 0.3657\n",
      "2025-11-10 13:47:38,383 - root - INFO - KG Training: Epoch 0001 Iter 1392 / 4823 | Time 0.0s | Iter Loss 0.2140 | Iter Mean Loss 0.3654\n",
      "2025-11-10 13:47:38,612 - root - INFO - KG Training: Epoch 0001 Iter 1395 / 4823 | Time 0.0s | Iter Loss 0.2076 | Iter Mean Loss 0.3651\n",
      "2025-11-10 13:47:38,738 - root - INFO - KG Training: Epoch 0001 Iter 1398 / 4823 | Time 0.0s | Iter Loss 0.2056 | Iter Mean Loss 0.3648\n",
      "2025-11-10 13:47:38,997 - root - INFO - KG Training: Epoch 0001 Iter 1401 / 4823 | Time 0.0s | Iter Loss 0.2199 | Iter Mean Loss 0.3645\n",
      "2025-11-10 13:47:39,125 - root - INFO - KG Training: Epoch 0001 Iter 1404 / 4823 | Time 0.0s | Iter Loss 0.2143 | Iter Mean Loss 0.3641\n",
      "2025-11-10 13:47:39,248 - root - INFO - KG Training: Epoch 0001 Iter 1407 / 4823 | Time 0.0s | Iter Loss 0.2182 | Iter Mean Loss 0.3638\n",
      "2025-11-10 13:47:39,374 - root - INFO - KG Training: Epoch 0001 Iter 1410 / 4823 | Time 0.0s | Iter Loss 0.2116 | Iter Mean Loss 0.3635\n",
      "2025-11-10 13:47:39,496 - root - INFO - KG Training: Epoch 0001 Iter 1413 / 4823 | Time 0.0s | Iter Loss 0.2166 | Iter Mean Loss 0.3632\n",
      "2025-11-10 13:47:39,714 - root - INFO - KG Training: Epoch 0001 Iter 1416 / 4823 | Time 0.0s | Iter Loss 0.2088 | Iter Mean Loss 0.3629\n",
      "2025-11-10 13:47:39,843 - root - INFO - KG Training: Epoch 0001 Iter 1419 / 4823 | Time 0.0s | Iter Loss 0.2181 | Iter Mean Loss 0.3626\n",
      "2025-11-10 13:47:39,968 - root - INFO - KG Training: Epoch 0001 Iter 1422 / 4823 | Time 0.0s | Iter Loss 0.2236 | Iter Mean Loss 0.3623\n",
      "2025-11-10 13:47:40,092 - root - INFO - KG Training: Epoch 0001 Iter 1425 / 4823 | Time 0.0s | Iter Loss 0.2102 | Iter Mean Loss 0.3620\n",
      "2025-11-10 13:47:40,217 - root - INFO - KG Training: Epoch 0001 Iter 1428 / 4823 | Time 0.0s | Iter Loss 0.2053 | Iter Mean Loss 0.3616\n",
      "2025-11-10 13:47:40,340 - root - INFO - KG Training: Epoch 0001 Iter 1431 / 4823 | Time 0.0s | Iter Loss 0.2010 | Iter Mean Loss 0.3613\n",
      "2025-11-10 13:47:40,464 - root - INFO - KG Training: Epoch 0001 Iter 1434 / 4823 | Time 0.0s | Iter Loss 0.2212 | Iter Mean Loss 0.3610\n",
      "2025-11-10 13:47:40,588 - root - INFO - KG Training: Epoch 0001 Iter 1437 / 4823 | Time 0.0s | Iter Loss 0.2161 | Iter Mean Loss 0.3607\n",
      "2025-11-10 13:47:40,711 - root - INFO - KG Training: Epoch 0001 Iter 1440 / 4823 | Time 0.0s | Iter Loss 0.2051 | Iter Mean Loss 0.3604\n",
      "2025-11-10 13:47:40,837 - root - INFO - KG Training: Epoch 0001 Iter 1443 / 4823 | Time 0.0s | Iter Loss 0.2129 | Iter Mean Loss 0.3601\n",
      "2025-11-10 13:47:40,962 - root - INFO - KG Training: Epoch 0001 Iter 1446 / 4823 | Time 0.0s | Iter Loss 0.2265 | Iter Mean Loss 0.3598\n",
      "2025-11-10 13:47:41,086 - root - INFO - KG Training: Epoch 0001 Iter 1449 / 4823 | Time 0.0s | Iter Loss 0.2224 | Iter Mean Loss 0.3595\n",
      "2025-11-10 13:47:41,211 - root - INFO - KG Training: Epoch 0001 Iter 1452 / 4823 | Time 0.0s | Iter Loss 0.2103 | Iter Mean Loss 0.3592\n",
      "2025-11-10 13:47:41,334 - root - INFO - KG Training: Epoch 0001 Iter 1455 / 4823 | Time 0.0s | Iter Loss 0.2117 | Iter Mean Loss 0.3589\n",
      "2025-11-10 13:47:41,456 - root - INFO - KG Training: Epoch 0001 Iter 1458 / 4823 | Time 0.0s | Iter Loss 0.2006 | Iter Mean Loss 0.3586\n",
      "2025-11-10 13:47:41,582 - root - INFO - KG Training: Epoch 0001 Iter 1461 / 4823 | Time 0.0s | Iter Loss 0.2061 | Iter Mean Loss 0.3583\n",
      "2025-11-10 13:47:41,745 - root - INFO - KG Training: Epoch 0001 Iter 1464 / 4823 | Time 0.1s | Iter Loss 0.2135 | Iter Mean Loss 0.3580\n",
      "2025-11-10 13:47:41,874 - root - INFO - KG Training: Epoch 0001 Iter 1467 / 4823 | Time 0.0s | Iter Loss 0.2021 | Iter Mean Loss 0.3577\n",
      "2025-11-10 13:47:41,999 - root - INFO - KG Training: Epoch 0001 Iter 1470 / 4823 | Time 0.0s | Iter Loss 0.2158 | Iter Mean Loss 0.3574\n",
      "2025-11-10 13:47:42,291 - root - INFO - KG Training: Epoch 0001 Iter 1473 / 4823 | Time 0.0s | Iter Loss 0.1930 | Iter Mean Loss 0.3571\n",
      "2025-11-10 13:47:42,423 - root - INFO - KG Training: Epoch 0001 Iter 1476 / 4823 | Time 0.0s | Iter Loss 0.2085 | Iter Mean Loss 0.3568\n",
      "2025-11-10 13:47:42,549 - root - INFO - KG Training: Epoch 0001 Iter 1479 / 4823 | Time 0.0s | Iter Loss 0.1868 | Iter Mean Loss 0.3565\n",
      "2025-11-10 13:47:42,676 - root - INFO - KG Training: Epoch 0001 Iter 1482 / 4823 | Time 0.0s | Iter Loss 0.2166 | Iter Mean Loss 0.3562\n",
      "2025-11-10 13:47:42,979 - root - INFO - KG Training: Epoch 0001 Iter 1485 / 4823 | Time 0.0s | Iter Loss 0.2039 | Iter Mean Loss 0.3558\n",
      "2025-11-10 13:47:43,108 - root - INFO - KG Training: Epoch 0001 Iter 1488 / 4823 | Time 0.0s | Iter Loss 0.2068 | Iter Mean Loss 0.3555\n",
      "2025-11-10 13:47:43,234 - root - INFO - KG Training: Epoch 0001 Iter 1491 / 4823 | Time 0.0s | Iter Loss 0.2087 | Iter Mean Loss 0.3552\n",
      "2025-11-10 13:47:43,359 - root - INFO - KG Training: Epoch 0001 Iter 1494 / 4823 | Time 0.0s | Iter Loss 0.2099 | Iter Mean Loss 0.3550\n",
      "2025-11-10 13:47:43,619 - root - INFO - KG Training: Epoch 0001 Iter 1497 / 4823 | Time 0.0s | Iter Loss 0.2004 | Iter Mean Loss 0.3547\n",
      "2025-11-10 13:47:43,981 - root - INFO - KG Training: Epoch 0001 Iter 1500 / 4823 | Time 0.3s | Iter Loss 0.2092 | Iter Mean Loss 0.3544\n",
      "2025-11-10 13:47:44,113 - root - INFO - KG Training: Epoch 0001 Iter 1503 / 4823 | Time 0.0s | Iter Loss 0.2115 | Iter Mean Loss 0.3541\n",
      "2025-11-10 13:47:44,237 - root - INFO - KG Training: Epoch 0001 Iter 1506 / 4823 | Time 0.0s | Iter Loss 0.2059 | Iter Mean Loss 0.3538\n",
      "2025-11-10 13:47:44,361 - root - INFO - KG Training: Epoch 0001 Iter 1509 / 4823 | Time 0.0s | Iter Loss 0.2037 | Iter Mean Loss 0.3535\n",
      "2025-11-10 13:47:44,485 - root - INFO - KG Training: Epoch 0001 Iter 1512 / 4823 | Time 0.0s | Iter Loss 0.2111 | Iter Mean Loss 0.3532\n",
      "2025-11-10 13:47:44,607 - root - INFO - KG Training: Epoch 0001 Iter 1515 / 4823 | Time 0.0s | Iter Loss 0.1984 | Iter Mean Loss 0.3529\n",
      "2025-11-10 13:47:44,846 - root - INFO - KG Training: Epoch 0001 Iter 1518 / 4823 | Time 0.0s | Iter Loss 0.2088 | Iter Mean Loss 0.3526\n",
      "2025-11-10 13:47:44,970 - root - INFO - KG Training: Epoch 0001 Iter 1521 / 4823 | Time 0.0s | Iter Loss 0.2078 | Iter Mean Loss 0.3524\n",
      "2025-11-10 13:47:45,093 - root - INFO - KG Training: Epoch 0001 Iter 1524 / 4823 | Time 0.0s | Iter Loss 0.2033 | Iter Mean Loss 0.3521\n",
      "2025-11-10 13:47:45,215 - root - INFO - KG Training: Epoch 0001 Iter 1527 / 4823 | Time 0.0s | Iter Loss 0.1915 | Iter Mean Loss 0.3518\n",
      "2025-11-10 13:47:45,359 - root - INFO - KG Training: Epoch 0001 Iter 1530 / 4823 | Time 0.0s | Iter Loss 0.2174 | Iter Mean Loss 0.3515\n",
      "2025-11-10 13:47:45,481 - root - INFO - KG Training: Epoch 0001 Iter 1533 / 4823 | Time 0.0s | Iter Loss 0.2083 | Iter Mean Loss 0.3512\n",
      "2025-11-10 13:47:45,608 - root - INFO - KG Training: Epoch 0001 Iter 1536 / 4823 | Time 0.0s | Iter Loss 0.2165 | Iter Mean Loss 0.3509\n",
      "2025-11-10 13:47:45,735 - root - INFO - KG Training: Epoch 0001 Iter 1539 / 4823 | Time 0.0s | Iter Loss 0.2025 | Iter Mean Loss 0.3507\n",
      "2025-11-10 13:47:45,859 - root - INFO - KG Training: Epoch 0001 Iter 1542 / 4823 | Time 0.0s | Iter Loss 0.2026 | Iter Mean Loss 0.3504\n",
      "2025-11-10 13:47:45,985 - root - INFO - KG Training: Epoch 0001 Iter 1545 / 4823 | Time 0.0s | Iter Loss 0.2045 | Iter Mean Loss 0.3501\n",
      "2025-11-10 13:47:46,109 - root - INFO - KG Training: Epoch 0001 Iter 1548 / 4823 | Time 0.0s | Iter Loss 0.2118 | Iter Mean Loss 0.3498\n",
      "2025-11-10 13:47:46,239 - root - INFO - KG Training: Epoch 0001 Iter 1551 / 4823 | Time 0.0s | Iter Loss 0.2115 | Iter Mean Loss 0.3495\n",
      "2025-11-10 13:47:46,367 - root - INFO - KG Training: Epoch 0001 Iter 1554 / 4823 | Time 0.0s | Iter Loss 0.2055 | Iter Mean Loss 0.3493\n",
      "2025-11-10 13:47:46,614 - root - INFO - KG Training: Epoch 0001 Iter 1557 / 4823 | Time 0.0s | Iter Loss 0.1953 | Iter Mean Loss 0.3490\n",
      "2025-11-10 13:47:46,744 - root - INFO - KG Training: Epoch 0001 Iter 1560 / 4823 | Time 0.0s | Iter Loss 0.2179 | Iter Mean Loss 0.3487\n",
      "2025-11-10 13:47:46,868 - root - INFO - KG Training: Epoch 0001 Iter 1563 / 4823 | Time 0.0s | Iter Loss 0.2080 | Iter Mean Loss 0.3484\n",
      "2025-11-10 13:47:46,990 - root - INFO - KG Training: Epoch 0001 Iter 1566 / 4823 | Time 0.0s | Iter Loss 0.2014 | Iter Mean Loss 0.3481\n",
      "2025-11-10 13:47:47,115 - root - INFO - KG Training: Epoch 0001 Iter 1569 / 4823 | Time 0.0s | Iter Loss 0.1984 | Iter Mean Loss 0.3479\n",
      "2025-11-10 13:47:47,242 - root - INFO - KG Training: Epoch 0001 Iter 1572 / 4823 | Time 0.0s | Iter Loss 0.1966 | Iter Mean Loss 0.3476\n",
      "2025-11-10 13:47:47,600 - root - INFO - KG Training: Epoch 0001 Iter 1575 / 4823 | Time 0.0s | Iter Loss 0.2130 | Iter Mean Loss 0.3473\n",
      "2025-11-10 13:47:47,723 - root - INFO - KG Training: Epoch 0001 Iter 1578 / 4823 | Time 0.0s | Iter Loss 0.2092 | Iter Mean Loss 0.3470\n",
      "2025-11-10 13:47:47,850 - root - INFO - KG Training: Epoch 0001 Iter 1581 / 4823 | Time 0.0s | Iter Loss 0.2175 | Iter Mean Loss 0.3468\n",
      "2025-11-10 13:47:47,973 - root - INFO - KG Training: Epoch 0001 Iter 1584 / 4823 | Time 0.0s | Iter Loss 0.2116 | Iter Mean Loss 0.3465\n",
      "2025-11-10 13:47:48,095 - root - INFO - KG Training: Epoch 0001 Iter 1587 / 4823 | Time 0.0s | Iter Loss 0.2080 | Iter Mean Loss 0.3462\n",
      "2025-11-10 13:47:48,219 - root - INFO - KG Training: Epoch 0001 Iter 1590 / 4823 | Time 0.0s | Iter Loss 0.2126 | Iter Mean Loss 0.3459\n",
      "2025-11-10 13:47:48,344 - root - INFO - KG Training: Epoch 0001 Iter 1593 / 4823 | Time 0.0s | Iter Loss 0.1918 | Iter Mean Loss 0.3457\n",
      "2025-11-10 13:47:48,469 - root - INFO - KG Training: Epoch 0001 Iter 1596 / 4823 | Time 0.0s | Iter Loss 0.2033 | Iter Mean Loss 0.3454\n",
      "2025-11-10 13:47:48,592 - root - INFO - KG Training: Epoch 0001 Iter 1599 / 4823 | Time 0.0s | Iter Loss 0.2067 | Iter Mean Loss 0.3451\n",
      "2025-11-10 13:47:48,718 - root - INFO - KG Training: Epoch 0001 Iter 1602 / 4823 | Time 0.0s | Iter Loss 0.1980 | Iter Mean Loss 0.3449\n",
      "2025-11-10 13:47:48,841 - root - INFO - KG Training: Epoch 0001 Iter 1605 / 4823 | Time 0.0s | Iter Loss 0.1976 | Iter Mean Loss 0.3446\n",
      "2025-11-10 13:47:48,964 - root - INFO - KG Training: Epoch 0001 Iter 1608 / 4823 | Time 0.0s | Iter Loss 0.1927 | Iter Mean Loss 0.3443\n",
      "2025-11-10 13:47:49,086 - root - INFO - KG Training: Epoch 0001 Iter 1611 / 4823 | Time 0.0s | Iter Loss 0.1951 | Iter Mean Loss 0.3440\n",
      "2025-11-10 13:47:49,214 - root - INFO - KG Training: Epoch 0001 Iter 1614 / 4823 | Time 0.0s | Iter Loss 0.1986 | Iter Mean Loss 0.3438\n",
      "2025-11-10 13:47:49,336 - root - INFO - KG Training: Epoch 0001 Iter 1617 / 4823 | Time 0.0s | Iter Loss 0.1951 | Iter Mean Loss 0.3435\n",
      "2025-11-10 13:47:49,465 - root - INFO - KG Training: Epoch 0001 Iter 1620 / 4823 | Time 0.0s | Iter Loss 0.2097 | Iter Mean Loss 0.3432\n",
      "2025-11-10 13:47:49,674 - root - INFO - KG Training: Epoch 0001 Iter 1623 / 4823 | Time 0.0s | Iter Loss 0.2015 | Iter Mean Loss 0.3430\n",
      "2025-11-10 13:47:49,866 - root - INFO - KG Training: Epoch 0001 Iter 1626 / 4823 | Time 0.1s | Iter Loss 0.1898 | Iter Mean Loss 0.3427\n",
      "2025-11-10 13:47:50,090 - root - INFO - KG Training: Epoch 0001 Iter 1629 / 4823 | Time 0.0s | Iter Loss 0.2018 | Iter Mean Loss 0.3425\n",
      "2025-11-10 13:47:50,372 - root - INFO - KG Training: Epoch 0001 Iter 1632 / 4823 | Time 0.0s | Iter Loss 0.1983 | Iter Mean Loss 0.3422\n",
      "2025-11-10 13:47:50,494 - root - INFO - KG Training: Epoch 0001 Iter 1635 / 4823 | Time 0.0s | Iter Loss 0.1895 | Iter Mean Loss 0.3419\n",
      "2025-11-10 13:47:50,627 - root - INFO - KG Training: Epoch 0001 Iter 1638 / 4823 | Time 0.0s | Iter Loss 0.2033 | Iter Mean Loss 0.3416\n",
      "2025-11-10 13:47:50,765 - root - INFO - KG Training: Epoch 0001 Iter 1641 / 4823 | Time 0.0s | Iter Loss 0.1935 | Iter Mean Loss 0.3414\n",
      "2025-11-10 13:47:50,902 - root - INFO - KG Training: Epoch 0001 Iter 1644 / 4823 | Time 0.0s | Iter Loss 0.2024 | Iter Mean Loss 0.3411\n",
      "2025-11-10 13:47:51,026 - root - INFO - KG Training: Epoch 0001 Iter 1647 / 4823 | Time 0.0s | Iter Loss 0.2003 | Iter Mean Loss 0.3409\n",
      "2025-11-10 13:47:51,152 - root - INFO - KG Training: Epoch 0001 Iter 1650 / 4823 | Time 0.0s | Iter Loss 0.1986 | Iter Mean Loss 0.3406\n",
      "2025-11-10 13:47:51,276 - root - INFO - KG Training: Epoch 0001 Iter 1653 / 4823 | Time 0.0s | Iter Loss 0.1967 | Iter Mean Loss 0.3403\n",
      "2025-11-10 13:47:51,402 - root - INFO - KG Training: Epoch 0001 Iter 1656 / 4823 | Time 0.0s | Iter Loss 0.1998 | Iter Mean Loss 0.3401\n",
      "2025-11-10 13:47:51,529 - root - INFO - KG Training: Epoch 0001 Iter 1659 / 4823 | Time 0.0s | Iter Loss 0.1909 | Iter Mean Loss 0.3398\n",
      "2025-11-10 13:47:51,655 - root - INFO - KG Training: Epoch 0001 Iter 1662 / 4823 | Time 0.0s | Iter Loss 0.1931 | Iter Mean Loss 0.3396\n",
      "2025-11-10 13:47:51,846 - root - INFO - KG Training: Epoch 0001 Iter 1665 / 4823 | Time 0.1s | Iter Loss 0.1894 | Iter Mean Loss 0.3393\n",
      "2025-11-10 13:47:51,971 - root - INFO - KG Training: Epoch 0001 Iter 1668 / 4823 | Time 0.0s | Iter Loss 0.1952 | Iter Mean Loss 0.3390\n",
      "2025-11-10 13:47:52,096 - root - INFO - KG Training: Epoch 0001 Iter 1671 / 4823 | Time 0.0s | Iter Loss 0.1913 | Iter Mean Loss 0.3388\n",
      "2025-11-10 13:47:52,223 - root - INFO - KG Training: Epoch 0001 Iter 1674 / 4823 | Time 0.0s | Iter Loss 0.1992 | Iter Mean Loss 0.3385\n",
      "2025-11-10 13:47:52,349 - root - INFO - KG Training: Epoch 0001 Iter 1677 / 4823 | Time 0.0s | Iter Loss 0.1844 | Iter Mean Loss 0.3383\n",
      "2025-11-10 13:47:52,475 - root - INFO - KG Training: Epoch 0001 Iter 1680 / 4823 | Time 0.0s | Iter Loss 0.1934 | Iter Mean Loss 0.3380\n",
      "2025-11-10 13:47:52,598 - root - INFO - KG Training: Epoch 0001 Iter 1683 / 4823 | Time 0.0s | Iter Loss 0.1923 | Iter Mean Loss 0.3378\n",
      "2025-11-10 13:47:52,720 - root - INFO - KG Training: Epoch 0001 Iter 1686 / 4823 | Time 0.0s | Iter Loss 0.1864 | Iter Mean Loss 0.3375\n",
      "2025-11-10 13:47:52,847 - root - INFO - KG Training: Epoch 0001 Iter 1689 / 4823 | Time 0.0s | Iter Loss 0.1975 | Iter Mean Loss 0.3373\n",
      "2025-11-10 13:47:52,970 - root - INFO - KG Training: Epoch 0001 Iter 1692 / 4823 | Time 0.0s | Iter Loss 0.1877 | Iter Mean Loss 0.3370\n",
      "2025-11-10 13:47:53,099 - root - INFO - KG Training: Epoch 0001 Iter 1695 / 4823 | Time 0.0s | Iter Loss 0.1907 | Iter Mean Loss 0.3368\n",
      "2025-11-10 13:47:53,224 - root - INFO - KG Training: Epoch 0001 Iter 1698 / 4823 | Time 0.0s | Iter Loss 0.1834 | Iter Mean Loss 0.3365\n",
      "2025-11-10 13:47:53,350 - root - INFO - KG Training: Epoch 0001 Iter 1701 / 4823 | Time 0.0s | Iter Loss 0.2148 | Iter Mean Loss 0.3363\n",
      "2025-11-10 13:47:53,475 - root - INFO - KG Training: Epoch 0001 Iter 1704 / 4823 | Time 0.0s | Iter Loss 0.1868 | Iter Mean Loss 0.3360\n",
      "2025-11-10 13:47:53,601 - root - INFO - KG Training: Epoch 0001 Iter 1707 / 4823 | Time 0.0s | Iter Loss 0.1923 | Iter Mean Loss 0.3358\n",
      "2025-11-10 13:47:53,725 - root - INFO - KG Training: Epoch 0001 Iter 1710 / 4823 | Time 0.0s | Iter Loss 0.1855 | Iter Mean Loss 0.3355\n",
      "2025-11-10 13:47:53,850 - root - INFO - KG Training: Epoch 0001 Iter 1713 / 4823 | Time 0.0s | Iter Loss 0.1940 | Iter Mean Loss 0.3353\n",
      "2025-11-10 13:47:54,059 - root - INFO - KG Training: Epoch 0001 Iter 1716 / 4823 | Time 0.1s | Iter Loss 0.1961 | Iter Mean Loss 0.3350\n",
      "2025-11-10 13:47:54,268 - root - INFO - KG Training: Epoch 0001 Iter 1719 / 4823 | Time 0.1s | Iter Loss 0.2145 | Iter Mean Loss 0.3348\n",
      "2025-11-10 13:47:54,392 - root - INFO - KG Training: Epoch 0001 Iter 1722 / 4823 | Time 0.0s | Iter Loss 0.2051 | Iter Mean Loss 0.3345\n",
      "2025-11-10 13:47:54,515 - root - INFO - KG Training: Epoch 0001 Iter 1725 / 4823 | Time 0.0s | Iter Loss 0.1810 | Iter Mean Loss 0.3343\n",
      "2025-11-10 13:47:54,638 - root - INFO - KG Training: Epoch 0001 Iter 1728 / 4823 | Time 0.0s | Iter Loss 0.1946 | Iter Mean Loss 0.3340\n",
      "2025-11-10 13:47:54,764 - root - INFO - KG Training: Epoch 0001 Iter 1731 / 4823 | Time 0.0s | Iter Loss 0.1835 | Iter Mean Loss 0.3338\n",
      "2025-11-10 13:47:54,887 - root - INFO - KG Training: Epoch 0001 Iter 1734 / 4823 | Time 0.0s | Iter Loss 0.1939 | Iter Mean Loss 0.3335\n",
      "2025-11-10 13:47:55,016 - root - INFO - KG Training: Epoch 0001 Iter 1737 / 4823 | Time 0.0s | Iter Loss 0.1969 | Iter Mean Loss 0.3333\n",
      "2025-11-10 13:47:55,282 - root - INFO - KG Training: Epoch 0001 Iter 1740 / 4823 | Time 0.0s | Iter Loss 0.1857 | Iter Mean Loss 0.3330\n",
      "2025-11-10 13:47:55,407 - root - INFO - KG Training: Epoch 0001 Iter 1743 / 4823 | Time 0.0s | Iter Loss 0.1870 | Iter Mean Loss 0.3328\n",
      "2025-11-10 13:47:55,530 - root - INFO - KG Training: Epoch 0001 Iter 1746 / 4823 | Time 0.0s | Iter Loss 0.1922 | Iter Mean Loss 0.3326\n",
      "2025-11-10 13:47:55,653 - root - INFO - KG Training: Epoch 0001 Iter 1749 / 4823 | Time 0.0s | Iter Loss 0.2013 | Iter Mean Loss 0.3323\n",
      "2025-11-10 13:47:55,842 - root - INFO - KG Training: Epoch 0001 Iter 1752 / 4823 | Time 0.0s | Iter Loss 0.1935 | Iter Mean Loss 0.3321\n",
      "2025-11-10 13:47:55,967 - root - INFO - KG Training: Epoch 0001 Iter 1755 / 4823 | Time 0.0s | Iter Loss 0.2031 | Iter Mean Loss 0.3319\n",
      "2025-11-10 13:47:56,249 - root - INFO - KG Training: Epoch 0001 Iter 1758 / 4823 | Time 0.0s | Iter Loss 0.1935 | Iter Mean Loss 0.3316\n",
      "2025-11-10 13:47:56,516 - root - INFO - KG Training: Epoch 0001 Iter 1761 / 4823 | Time 0.0s | Iter Loss 0.1747 | Iter Mean Loss 0.3314\n",
      "2025-11-10 13:47:56,640 - root - INFO - KG Training: Epoch 0001 Iter 1764 / 4823 | Time 0.0s | Iter Loss 0.1968 | Iter Mean Loss 0.3311\n",
      "2025-11-10 13:47:57,038 - root - INFO - KG Training: Epoch 0001 Iter 1767 / 4823 | Time 0.0s | Iter Loss 0.1966 | Iter Mean Loss 0.3309\n",
      "2025-11-10 13:47:57,167 - root - INFO - KG Training: Epoch 0001 Iter 1770 / 4823 | Time 0.0s | Iter Loss 0.1875 | Iter Mean Loss 0.3307\n",
      "2025-11-10 13:47:57,291 - root - INFO - KG Training: Epoch 0001 Iter 1773 / 4823 | Time 0.0s | Iter Loss 0.1838 | Iter Mean Loss 0.3304\n",
      "2025-11-10 13:47:57,421 - root - INFO - KG Training: Epoch 0001 Iter 1776 / 4823 | Time 0.0s | Iter Loss 0.1915 | Iter Mean Loss 0.3302\n",
      "2025-11-10 13:47:57,544 - root - INFO - KG Training: Epoch 0001 Iter 1779 / 4823 | Time 0.0s | Iter Loss 0.1753 | Iter Mean Loss 0.3299\n",
      "2025-11-10 13:47:57,666 - root - INFO - KG Training: Epoch 0001 Iter 1782 / 4823 | Time 0.0s | Iter Loss 0.1956 | Iter Mean Loss 0.3297\n",
      "2025-11-10 13:47:57,795 - root - INFO - KG Training: Epoch 0001 Iter 1785 / 4823 | Time 0.0s | Iter Loss 0.1894 | Iter Mean Loss 0.3295\n",
      "2025-11-10 13:47:57,919 - root - INFO - KG Training: Epoch 0001 Iter 1788 / 4823 | Time 0.0s | Iter Loss 0.1873 | Iter Mean Loss 0.3292\n",
      "2025-11-10 13:47:58,045 - root - INFO - KG Training: Epoch 0001 Iter 1791 / 4823 | Time 0.0s | Iter Loss 0.1868 | Iter Mean Loss 0.3290\n",
      "2025-11-10 13:47:58,167 - root - INFO - KG Training: Epoch 0001 Iter 1794 / 4823 | Time 0.0s | Iter Loss 0.1969 | Iter Mean Loss 0.3288\n",
      "2025-11-10 13:47:58,292 - root - INFO - KG Training: Epoch 0001 Iter 1797 / 4823 | Time 0.0s | Iter Loss 0.1887 | Iter Mean Loss 0.3285\n",
      "2025-11-10 13:47:58,417 - root - INFO - KG Training: Epoch 0001 Iter 1800 / 4823 | Time 0.0s | Iter Loss 0.1816 | Iter Mean Loss 0.3283\n",
      "2025-11-10 13:47:58,539 - root - INFO - KG Training: Epoch 0001 Iter 1803 / 4823 | Time 0.0s | Iter Loss 0.1996 | Iter Mean Loss 0.3281\n",
      "2025-11-10 13:47:58,662 - root - INFO - KG Training: Epoch 0001 Iter 1806 / 4823 | Time 0.0s | Iter Loss 0.1999 | Iter Mean Loss 0.3278\n",
      "2025-11-10 13:47:58,786 - root - INFO - KG Training: Epoch 0001 Iter 1809 / 4823 | Time 0.0s | Iter Loss 0.1784 | Iter Mean Loss 0.3276\n",
      "2025-11-10 13:47:58,911 - root - INFO - KG Training: Epoch 0001 Iter 1812 / 4823 | Time 0.0s | Iter Loss 0.1932 | Iter Mean Loss 0.3274\n",
      "2025-11-10 13:47:59,145 - root - INFO - KG Training: Epoch 0001 Iter 1815 / 4823 | Time 0.2s | Iter Loss 0.1991 | Iter Mean Loss 0.3271\n",
      "2025-11-10 13:47:59,276 - root - INFO - KG Training: Epoch 0001 Iter 1818 / 4823 | Time 0.0s | Iter Loss 0.1930 | Iter Mean Loss 0.3269\n",
      "2025-11-10 13:47:59,408 - root - INFO - KG Training: Epoch 0001 Iter 1821 / 4823 | Time 0.0s | Iter Loss 0.2031 | Iter Mean Loss 0.3267\n",
      "2025-11-10 13:47:59,536 - root - INFO - KG Training: Epoch 0001 Iter 1824 / 4823 | Time 0.0s | Iter Loss 0.1889 | Iter Mean Loss 0.3265\n",
      "2025-11-10 13:47:59,667 - root - INFO - KG Training: Epoch 0001 Iter 1827 / 4823 | Time 0.0s | Iter Loss 0.1882 | Iter Mean Loss 0.3262\n",
      "2025-11-10 13:47:59,790 - root - INFO - KG Training: Epoch 0001 Iter 1830 / 4823 | Time 0.0s | Iter Loss 0.1713 | Iter Mean Loss 0.3260\n",
      "2025-11-10 13:47:59,987 - root - INFO - KG Training: Epoch 0001 Iter 1833 / 4823 | Time 0.0s | Iter Loss 0.1992 | Iter Mean Loss 0.3258\n",
      "2025-11-10 13:48:00,111 - root - INFO - KG Training: Epoch 0001 Iter 1836 / 4823 | Time 0.0s | Iter Loss 0.1782 | Iter Mean Loss 0.3256\n",
      "2025-11-10 13:48:00,241 - root - INFO - KG Training: Epoch 0001 Iter 1839 / 4823 | Time 0.0s | Iter Loss 0.1924 | Iter Mean Loss 0.3254\n",
      "2025-11-10 13:48:00,363 - root - INFO - KG Training: Epoch 0001 Iter 1842 / 4823 | Time 0.0s | Iter Loss 0.1663 | Iter Mean Loss 0.3251\n",
      "2025-11-10 13:48:00,570 - root - INFO - KG Training: Epoch 0001 Iter 1845 / 4823 | Time 0.0s | Iter Loss 0.1732 | Iter Mean Loss 0.3249\n",
      "2025-11-10 13:48:00,692 - root - INFO - KG Training: Epoch 0001 Iter 1848 / 4823 | Time 0.0s | Iter Loss 0.1877 | Iter Mean Loss 0.3247\n",
      "2025-11-10 13:48:00,821 - root - INFO - KG Training: Epoch 0001 Iter 1851 / 4823 | Time 0.0s | Iter Loss 0.1910 | Iter Mean Loss 0.3245\n",
      "2025-11-10 13:48:00,945 - root - INFO - KG Training: Epoch 0001 Iter 1854 / 4823 | Time 0.0s | Iter Loss 0.1847 | Iter Mean Loss 0.3242\n",
      "2025-11-10 13:48:01,245 - root - INFO - KG Training: Epoch 0001 Iter 1857 / 4823 | Time 0.0s | Iter Loss 0.1717 | Iter Mean Loss 0.3240\n",
      "2025-11-10 13:48:01,379 - root - INFO - KG Training: Epoch 0001 Iter 1860 / 4823 | Time 0.0s | Iter Loss 0.1811 | Iter Mean Loss 0.3238\n",
      "2025-11-10 13:48:01,505 - root - INFO - KG Training: Epoch 0001 Iter 1863 / 4823 | Time 0.0s | Iter Loss 0.1659 | Iter Mean Loss 0.3235\n",
      "2025-11-10 13:48:02,006 - root - INFO - KG Training: Epoch 0001 Iter 1866 / 4823 | Time 0.0s | Iter Loss 0.1761 | Iter Mean Loss 0.3233\n",
      "2025-11-10 13:48:02,131 - root - INFO - KG Training: Epoch 0001 Iter 1869 / 4823 | Time 0.0s | Iter Loss 0.1958 | Iter Mean Loss 0.3231\n",
      "2025-11-10 13:48:02,690 - root - INFO - KG Training: Epoch 0001 Iter 1872 / 4823 | Time 0.0s | Iter Loss 0.1883 | Iter Mean Loss 0.3229\n",
      "2025-11-10 13:48:02,888 - root - INFO - KG Training: Epoch 0001 Iter 1875 / 4823 | Time 0.0s | Iter Loss 0.1782 | Iter Mean Loss 0.3227\n",
      "2025-11-10 13:48:03,021 - root - INFO - KG Training: Epoch 0001 Iter 1878 / 4823 | Time 0.0s | Iter Loss 0.2003 | Iter Mean Loss 0.3224\n",
      "2025-11-10 13:48:03,146 - root - INFO - KG Training: Epoch 0001 Iter 1881 / 4823 | Time 0.0s | Iter Loss 0.1822 | Iter Mean Loss 0.3222\n",
      "2025-11-10 13:48:03,274 - root - INFO - KG Training: Epoch 0001 Iter 1884 / 4823 | Time 0.0s | Iter Loss 0.1884 | Iter Mean Loss 0.3220\n",
      "2025-11-10 13:48:03,666 - root - INFO - KG Training: Epoch 0001 Iter 1887 / 4823 | Time 0.0s | Iter Loss 0.1928 | Iter Mean Loss 0.3218\n",
      "2025-11-10 13:48:03,787 - root - INFO - KG Training: Epoch 0001 Iter 1890 / 4823 | Time 0.0s | Iter Loss 0.1950 | Iter Mean Loss 0.3216\n",
      "2025-11-10 13:48:03,908 - root - INFO - KG Training: Epoch 0001 Iter 1893 / 4823 | Time 0.0s | Iter Loss 0.1873 | Iter Mean Loss 0.3214\n",
      "2025-11-10 13:48:04,031 - root - INFO - KG Training: Epoch 0001 Iter 1896 / 4823 | Time 0.0s | Iter Loss 0.1847 | Iter Mean Loss 0.3212\n",
      "2025-11-10 13:48:04,153 - root - INFO - KG Training: Epoch 0001 Iter 1899 / 4823 | Time 0.0s | Iter Loss 0.2005 | Iter Mean Loss 0.3210\n",
      "2025-11-10 13:48:04,344 - root - INFO - KG Training: Epoch 0001 Iter 1902 / 4823 | Time 0.1s | Iter Loss 0.1976 | Iter Mean Loss 0.3207\n",
      "2025-11-10 13:48:04,469 - root - INFO - KG Training: Epoch 0001 Iter 1905 / 4823 | Time 0.0s | Iter Loss 0.1873 | Iter Mean Loss 0.3205\n",
      "2025-11-10 13:48:04,600 - root - INFO - KG Training: Epoch 0001 Iter 1908 / 4823 | Time 0.0s | Iter Loss 0.1885 | Iter Mean Loss 0.3203\n",
      "2025-11-10 13:48:04,725 - root - INFO - KG Training: Epoch 0001 Iter 1911 / 4823 | Time 0.0s | Iter Loss 0.1704 | Iter Mean Loss 0.3201\n",
      "2025-11-10 13:48:04,848 - root - INFO - KG Training: Epoch 0001 Iter 1914 / 4823 | Time 0.0s | Iter Loss 0.1785 | Iter Mean Loss 0.3199\n",
      "2025-11-10 13:48:04,975 - root - INFO - KG Training: Epoch 0001 Iter 1917 / 4823 | Time 0.0s | Iter Loss 0.1880 | Iter Mean Loss 0.3197\n",
      "2025-11-10 13:48:05,098 - root - INFO - KG Training: Epoch 0001 Iter 1920 / 4823 | Time 0.0s | Iter Loss 0.1866 | Iter Mean Loss 0.3195\n",
      "2025-11-10 13:48:05,225 - root - INFO - KG Training: Epoch 0001 Iter 1923 / 4823 | Time 0.0s | Iter Loss 0.1918 | Iter Mean Loss 0.3193\n",
      "2025-11-10 13:48:05,349 - root - INFO - KG Training: Epoch 0001 Iter 1926 / 4823 | Time 0.0s | Iter Loss 0.1963 | Iter Mean Loss 0.3191\n",
      "2025-11-10 13:48:05,473 - root - INFO - KG Training: Epoch 0001 Iter 1929 / 4823 | Time 0.0s | Iter Loss 0.1848 | Iter Mean Loss 0.3189\n",
      "2025-11-10 13:48:05,604 - root - INFO - KG Training: Epoch 0001 Iter 1932 / 4823 | Time 0.0s | Iter Loss 0.1772 | Iter Mean Loss 0.3187\n",
      "2025-11-10 13:48:05,729 - root - INFO - KG Training: Epoch 0001 Iter 1935 / 4823 | Time 0.0s | Iter Loss 0.1913 | Iter Mean Loss 0.3185\n",
      "2025-11-10 13:48:05,853 - root - INFO - KG Training: Epoch 0001 Iter 1938 / 4823 | Time 0.0s | Iter Loss 0.1818 | Iter Mean Loss 0.3182\n",
      "2025-11-10 13:48:05,977 - root - INFO - KG Training: Epoch 0001 Iter 1941 / 4823 | Time 0.0s | Iter Loss 0.1847 | Iter Mean Loss 0.3180\n",
      "2025-11-10 13:48:06,191 - root - INFO - KG Training: Epoch 0001 Iter 1944 / 4823 | Time 0.0s | Iter Loss 0.1836 | Iter Mean Loss 0.3178\n",
      "2025-11-10 13:48:06,380 - root - INFO - KG Training: Epoch 0001 Iter 1947 / 4823 | Time 0.0s | Iter Loss 0.1890 | Iter Mean Loss 0.3176\n",
      "2025-11-10 13:48:06,503 - root - INFO - KG Training: Epoch 0001 Iter 1950 / 4823 | Time 0.0s | Iter Loss 0.1970 | Iter Mean Loss 0.3174\n",
      "2025-11-10 13:48:06,628 - root - INFO - KG Training: Epoch 0001 Iter 1953 / 4823 | Time 0.0s | Iter Loss 0.1883 | Iter Mean Loss 0.3172\n",
      "2025-11-10 13:48:06,754 - root - INFO - KG Training: Epoch 0001 Iter 1956 / 4823 | Time 0.0s | Iter Loss 0.1592 | Iter Mean Loss 0.3170\n",
      "2025-11-10 13:48:06,879 - root - INFO - KG Training: Epoch 0001 Iter 1959 / 4823 | Time 0.0s | Iter Loss 0.1925 | Iter Mean Loss 0.3168\n",
      "2025-11-10 13:48:07,135 - root - INFO - KG Training: Epoch 0001 Iter 1962 / 4823 | Time 0.2s | Iter Loss 0.1729 | Iter Mean Loss 0.3166\n",
      "2025-11-10 13:48:07,266 - root - INFO - KG Training: Epoch 0001 Iter 1965 / 4823 | Time 0.0s | Iter Loss 0.1767 | Iter Mean Loss 0.3164\n",
      "2025-11-10 13:48:07,390 - root - INFO - KG Training: Epoch 0001 Iter 1968 / 4823 | Time 0.0s | Iter Loss 0.1713 | Iter Mean Loss 0.3162\n",
      "2025-11-10 13:48:07,512 - root - INFO - KG Training: Epoch 0001 Iter 1971 / 4823 | Time 0.0s | Iter Loss 0.1752 | Iter Mean Loss 0.3160\n",
      "2025-11-10 13:48:07,635 - root - INFO - KG Training: Epoch 0001 Iter 1974 / 4823 | Time 0.0s | Iter Loss 0.1779 | Iter Mean Loss 0.3158\n",
      "2025-11-10 13:48:07,762 - root - INFO - KG Training: Epoch 0001 Iter 1977 / 4823 | Time 0.0s | Iter Loss 0.1766 | Iter Mean Loss 0.3156\n",
      "2025-11-10 13:48:07,885 - root - INFO - KG Training: Epoch 0001 Iter 1980 / 4823 | Time 0.0s | Iter Loss 0.1776 | Iter Mean Loss 0.3153\n",
      "2025-11-10 13:48:08,009 - root - INFO - KG Training: Epoch 0001 Iter 1983 / 4823 | Time 0.0s | Iter Loss 0.1873 | Iter Mean Loss 0.3151\n",
      "2025-11-10 13:48:08,131 - root - INFO - KG Training: Epoch 0001 Iter 1986 / 4823 | Time 0.0s | Iter Loss 0.1800 | Iter Mean Loss 0.3149\n",
      "2025-11-10 13:48:08,599 - root - INFO - KG Training: Epoch 0001 Iter 1989 / 4823 | Time 0.3s | Iter Loss 0.1772 | Iter Mean Loss 0.3147\n",
      "2025-11-10 13:48:08,734 - root - INFO - KG Training: Epoch 0001 Iter 1992 / 4823 | Time 0.0s | Iter Loss 0.1777 | Iter Mean Loss 0.3145\n",
      "2025-11-10 13:48:08,860 - root - INFO - KG Training: Epoch 0001 Iter 1995 / 4823 | Time 0.0s | Iter Loss 0.1672 | Iter Mean Loss 0.3143\n",
      "2025-11-10 13:48:08,989 - root - INFO - KG Training: Epoch 0001 Iter 1998 / 4823 | Time 0.0s | Iter Loss 0.1791 | Iter Mean Loss 0.3141\n",
      "2025-11-10 13:48:09,258 - root - INFO - KG Training: Epoch 0001 Iter 2001 / 4823 | Time 0.0s | Iter Loss 0.1693 | Iter Mean Loss 0.3139\n",
      "2025-11-10 13:48:09,382 - root - INFO - KG Training: Epoch 0001 Iter 2004 / 4823 | Time 0.0s | Iter Loss 0.1759 | Iter Mean Loss 0.3137\n",
      "2025-11-10 13:48:09,504 - root - INFO - KG Training: Epoch 0001 Iter 2007 / 4823 | Time 0.0s | Iter Loss 0.1937 | Iter Mean Loss 0.3135\n",
      "2025-11-10 13:48:09,693 - root - INFO - KG Training: Epoch 0001 Iter 2010 / 4823 | Time 0.1s | Iter Loss 0.1756 | Iter Mean Loss 0.3133\n",
      "2025-11-10 13:48:09,817 - root - INFO - KG Training: Epoch 0001 Iter 2013 / 4823 | Time 0.0s | Iter Loss 0.1732 | Iter Mean Loss 0.3131\n",
      "2025-11-10 13:48:10,030 - root - INFO - KG Training: Epoch 0001 Iter 2016 / 4823 | Time 0.0s | Iter Loss 0.1664 | Iter Mean Loss 0.3129\n",
      "2025-11-10 13:48:10,269 - root - INFO - KG Training: Epoch 0001 Iter 2019 / 4823 | Time 0.0s | Iter Loss 0.1848 | Iter Mean Loss 0.3127\n",
      "2025-11-10 13:48:10,396 - root - INFO - KG Training: Epoch 0001 Iter 2022 / 4823 | Time 0.0s | Iter Loss 0.1846 | Iter Mean Loss 0.3125\n",
      "2025-11-10 13:48:10,602 - root - INFO - KG Training: Epoch 0001 Iter 2025 / 4823 | Time 0.1s | Iter Loss 0.1923 | Iter Mean Loss 0.3123\n",
      "2025-11-10 13:48:10,771 - root - INFO - KG Training: Epoch 0001 Iter 2028 / 4823 | Time 0.0s | Iter Loss 0.2093 | Iter Mean Loss 0.3122\n",
      "2025-11-10 13:48:10,897 - root - INFO - KG Training: Epoch 0001 Iter 2031 / 4823 | Time 0.0s | Iter Loss 0.1818 | Iter Mean Loss 0.3120\n",
      "2025-11-10 13:48:11,021 - root - INFO - KG Training: Epoch 0001 Iter 2034 / 4823 | Time 0.0s | Iter Loss 0.1745 | Iter Mean Loss 0.3118\n",
      "2025-11-10 13:48:11,146 - root - INFO - KG Training: Epoch 0001 Iter 2037 / 4823 | Time 0.0s | Iter Loss 0.1776 | Iter Mean Loss 0.3116\n",
      "2025-11-10 13:48:11,271 - root - INFO - KG Training: Epoch 0001 Iter 2040 / 4823 | Time 0.0s | Iter Loss 0.1654 | Iter Mean Loss 0.3114\n",
      "2025-11-10 13:48:11,397 - root - INFO - KG Training: Epoch 0001 Iter 2043 / 4823 | Time 0.0s | Iter Loss 0.1808 | Iter Mean Loss 0.3112\n",
      "2025-11-10 13:48:11,518 - root - INFO - KG Training: Epoch 0001 Iter 2046 / 4823 | Time 0.0s | Iter Loss 0.1726 | Iter Mean Loss 0.3110\n",
      "2025-11-10 13:48:11,645 - root - INFO - KG Training: Epoch 0001 Iter 2049 / 4823 | Time 0.0s | Iter Loss 0.1806 | Iter Mean Loss 0.3108\n",
      "2025-11-10 13:48:11,767 - root - INFO - KG Training: Epoch 0001 Iter 2052 / 4823 | Time 0.0s | Iter Loss 0.1684 | Iter Mean Loss 0.3106\n",
      "2025-11-10 13:48:11,956 - root - INFO - KG Training: Epoch 0001 Iter 2055 / 4823 | Time 0.0s | Iter Loss 0.1929 | Iter Mean Loss 0.3104\n",
      "2025-11-10 13:48:12,080 - root - INFO - KG Training: Epoch 0001 Iter 2058 / 4823 | Time 0.0s | Iter Loss 0.1686 | Iter Mean Loss 0.3102\n",
      "2025-11-10 13:48:12,271 - root - INFO - KG Training: Epoch 0001 Iter 2061 / 4823 | Time 0.1s | Iter Loss 0.1781 | Iter Mean Loss 0.3100\n",
      "2025-11-10 13:48:12,396 - root - INFO - KG Training: Epoch 0001 Iter 2064 / 4823 | Time 0.0s | Iter Loss 0.1780 | Iter Mean Loss 0.3098\n",
      "2025-11-10 13:48:12,520 - root - INFO - KG Training: Epoch 0001 Iter 2067 / 4823 | Time 0.0s | Iter Loss 0.1701 | Iter Mean Loss 0.3096\n",
      "2025-11-10 13:48:12,648 - root - INFO - KG Training: Epoch 0001 Iter 2070 / 4823 | Time 0.0s | Iter Loss 0.1646 | Iter Mean Loss 0.3094\n",
      "2025-11-10 13:48:12,775 - root - INFO - KG Training: Epoch 0001 Iter 2073 / 4823 | Time 0.0s | Iter Loss 0.1658 | Iter Mean Loss 0.3092\n",
      "2025-11-10 13:48:12,898 - root - INFO - KG Training: Epoch 0001 Iter 2076 / 4823 | Time 0.0s | Iter Loss 0.1814 | Iter Mean Loss 0.3090\n",
      "2025-11-10 13:48:13,024 - root - INFO - KG Training: Epoch 0001 Iter 2079 / 4823 | Time 0.0s | Iter Loss 0.1727 | Iter Mean Loss 0.3088\n",
      "2025-11-10 13:48:13,148 - root - INFO - KG Training: Epoch 0001 Iter 2082 / 4823 | Time 0.0s | Iter Loss 0.1766 | Iter Mean Loss 0.3087\n",
      "2025-11-10 13:48:13,277 - root - INFO - KG Training: Epoch 0001 Iter 2085 / 4823 | Time 0.0s | Iter Loss 0.1794 | Iter Mean Loss 0.3085\n",
      "2025-11-10 13:48:13,408 - root - INFO - KG Training: Epoch 0001 Iter 2088 / 4823 | Time 0.0s | Iter Loss 0.1631 | Iter Mean Loss 0.3083\n",
      "2025-11-10 13:48:13,533 - root - INFO - KG Training: Epoch 0001 Iter 2091 / 4823 | Time 0.0s | Iter Loss 0.1774 | Iter Mean Loss 0.3081\n",
      "2025-11-10 13:48:13,657 - root - INFO - KG Training: Epoch 0001 Iter 2094 / 4823 | Time 0.0s | Iter Loss 0.1779 | Iter Mean Loss 0.3079\n",
      "2025-11-10 13:48:13,781 - root - INFO - KG Training: Epoch 0001 Iter 2097 / 4823 | Time 0.0s | Iter Loss 0.1705 | Iter Mean Loss 0.3077\n",
      "2025-11-10 13:48:13,909 - root - INFO - KG Training: Epoch 0001 Iter 2100 / 4823 | Time 0.0s | Iter Loss 0.1661 | Iter Mean Loss 0.3075\n",
      "2025-11-10 13:48:14,034 - root - INFO - KG Training: Epoch 0001 Iter 2103 / 4823 | Time 0.0s | Iter Loss 0.1777 | Iter Mean Loss 0.3073\n",
      "2025-11-10 13:48:14,159 - root - INFO - KG Training: Epoch 0001 Iter 2106 / 4823 | Time 0.0s | Iter Loss 0.1671 | Iter Mean Loss 0.3071\n",
      "2025-11-10 13:48:14,283 - root - INFO - KG Training: Epoch 0001 Iter 2109 / 4823 | Time 0.0s | Iter Loss 0.1686 | Iter Mean Loss 0.3070\n",
      "2025-11-10 13:48:14,411 - root - INFO - KG Training: Epoch 0001 Iter 2112 / 4823 | Time 0.0s | Iter Loss 0.1778 | Iter Mean Loss 0.3068\n",
      "2025-11-10 13:48:14,533 - root - INFO - KG Training: Epoch 0001 Iter 2115 / 4823 | Time 0.0s | Iter Loss 0.1805 | Iter Mean Loss 0.3066\n",
      "2025-11-10 13:48:14,658 - root - INFO - KG Training: Epoch 0001 Iter 2118 / 4823 | Time 0.0s | Iter Loss 0.1708 | Iter Mean Loss 0.3064\n",
      "2025-11-10 13:48:14,783 - root - INFO - KG Training: Epoch 0001 Iter 2121 / 4823 | Time 0.0s | Iter Loss 0.1902 | Iter Mean Loss 0.3062\n",
      "2025-11-10 13:48:14,906 - root - INFO - KG Training: Epoch 0001 Iter 2124 / 4823 | Time 0.0s | Iter Loss 0.1681 | Iter Mean Loss 0.3060\n",
      "2025-11-10 13:48:15,029 - root - INFO - KG Training: Epoch 0001 Iter 2127 / 4823 | Time 0.0s | Iter Loss 0.1712 | Iter Mean Loss 0.3059\n",
      "2025-11-10 13:48:15,157 - root - INFO - KG Training: Epoch 0001 Iter 2130 / 4823 | Time 0.0s | Iter Loss 0.1694 | Iter Mean Loss 0.3057\n",
      "2025-11-10 13:48:15,283 - root - INFO - KG Training: Epoch 0001 Iter 2133 / 4823 | Time 0.0s | Iter Loss 0.1646 | Iter Mean Loss 0.3055\n",
      "2025-11-10 13:48:15,414 - root - INFO - KG Training: Epoch 0001 Iter 2136 / 4823 | Time 0.0s | Iter Loss 0.1603 | Iter Mean Loss 0.3053\n",
      "2025-11-10 13:48:15,535 - root - INFO - KG Training: Epoch 0001 Iter 2139 / 4823 | Time 0.0s | Iter Loss 0.1728 | Iter Mean Loss 0.3051\n",
      "2025-11-10 13:48:15,723 - root - INFO - KG Training: Epoch 0001 Iter 2142 / 4823 | Time 0.1s | Iter Loss 0.1673 | Iter Mean Loss 0.3049\n",
      "2025-11-10 13:48:15,850 - root - INFO - KG Training: Epoch 0001 Iter 2145 / 4823 | Time 0.0s | Iter Loss 0.1564 | Iter Mean Loss 0.3047\n",
      "2025-11-10 13:48:15,974 - root - INFO - KG Training: Epoch 0001 Iter 2148 / 4823 | Time 0.0s | Iter Loss 0.1659 | Iter Mean Loss 0.3045\n",
      "2025-11-10 13:48:16,098 - root - INFO - KG Training: Epoch 0001 Iter 2151 / 4823 | Time 0.0s | Iter Loss 0.1694 | Iter Mean Loss 0.3043\n",
      "2025-11-10 13:48:16,227 - root - INFO - KG Training: Epoch 0001 Iter 2154 / 4823 | Time 0.0s | Iter Loss 0.1638 | Iter Mean Loss 0.3041\n",
      "2025-11-10 13:48:16,681 - root - INFO - KG Training: Epoch 0001 Iter 2157 / 4823 | Time 0.0s | Iter Loss 0.1753 | Iter Mean Loss 0.3040\n",
      "2025-11-10 13:48:16,807 - root - INFO - KG Training: Epoch 0001 Iter 2160 / 4823 | Time 0.0s | Iter Loss 0.1813 | Iter Mean Loss 0.3038\n",
      "2025-11-10 13:48:16,930 - root - INFO - KG Training: Epoch 0001 Iter 2163 / 4823 | Time 0.0s | Iter Loss 0.1763 | Iter Mean Loss 0.3036\n",
      "2025-11-10 13:48:17,245 - root - INFO - KG Training: Epoch 0001 Iter 2166 / 4823 | Time 0.0s | Iter Loss 0.1724 | Iter Mean Loss 0.3034\n",
      "2025-11-10 13:48:17,379 - root - INFO - KG Training: Epoch 0001 Iter 2169 / 4823 | Time 0.0s | Iter Loss 0.1846 | Iter Mean Loss 0.3032\n",
      "2025-11-10 13:48:17,507 - root - INFO - KG Training: Epoch 0001 Iter 2172 / 4823 | Time 0.0s | Iter Loss 0.1686 | Iter Mean Loss 0.3031\n",
      "2025-11-10 13:48:17,730 - root - INFO - KG Training: Epoch 0001 Iter 2175 / 4823 | Time 0.0s | Iter Loss 0.1753 | Iter Mean Loss 0.3029\n",
      "2025-11-10 13:48:17,858 - root - INFO - KG Training: Epoch 0001 Iter 2178 / 4823 | Time 0.0s | Iter Loss 0.1748 | Iter Mean Loss 0.3027\n",
      "2025-11-10 13:48:17,982 - root - INFO - KG Training: Epoch 0001 Iter 2181 / 4823 | Time 0.0s | Iter Loss 0.1615 | Iter Mean Loss 0.3025\n",
      "2025-11-10 13:48:18,271 - root - INFO - KG Training: Epoch 0001 Iter 2184 / 4823 | Time 0.2s | Iter Loss 0.1816 | Iter Mean Loss 0.3023\n",
      "2025-11-10 13:48:18,549 - root - INFO - KG Training: Epoch 0001 Iter 2187 / 4823 | Time 0.0s | Iter Loss 0.1759 | Iter Mean Loss 0.3022\n",
      "2025-11-10 13:48:18,674 - root - INFO - KG Training: Epoch 0001 Iter 2190 / 4823 | Time 0.0s | Iter Loss 0.1707 | Iter Mean Loss 0.3020\n",
      "2025-11-10 13:48:18,824 - root - INFO - KG Training: Epoch 0001 Iter 2193 / 4823 | Time 0.0s | Iter Loss 0.1671 | Iter Mean Loss 0.3018\n",
      "2025-11-10 13:48:18,946 - root - INFO - KG Training: Epoch 0001 Iter 2196 / 4823 | Time 0.0s | Iter Loss 0.1771 | Iter Mean Loss 0.3016\n",
      "2025-11-10 13:48:19,069 - root - INFO - KG Training: Epoch 0001 Iter 2199 / 4823 | Time 0.0s | Iter Loss 0.1610 | Iter Mean Loss 0.3015\n",
      "2025-11-10 13:48:19,191 - root - INFO - KG Training: Epoch 0001 Iter 2202 / 4823 | Time 0.0s | Iter Loss 0.1623 | Iter Mean Loss 0.3013\n",
      "2025-11-10 13:48:19,608 - root - INFO - KG Training: Epoch 0001 Iter 2205 / 4823 | Time 0.0s | Iter Loss 0.1669 | Iter Mean Loss 0.3011\n",
      "2025-11-10 13:48:19,739 - root - INFO - KG Training: Epoch 0001 Iter 2208 / 4823 | Time 0.0s | Iter Loss 0.1679 | Iter Mean Loss 0.3009\n",
      "2025-11-10 13:48:19,862 - root - INFO - KG Training: Epoch 0001 Iter 2211 / 4823 | Time 0.0s | Iter Loss 0.1772 | Iter Mean Loss 0.3007\n",
      "2025-11-10 13:48:19,990 - root - INFO - KG Training: Epoch 0001 Iter 2214 / 4823 | Time 0.0s | Iter Loss 0.1742 | Iter Mean Loss 0.3006\n",
      "2025-11-10 13:48:20,113 - root - INFO - KG Training: Epoch 0001 Iter 2217 / 4823 | Time 0.0s | Iter Loss 0.1503 | Iter Mean Loss 0.3004\n",
      "2025-11-10 13:48:20,237 - root - INFO - KG Training: Epoch 0001 Iter 2220 / 4823 | Time 0.0s | Iter Loss 0.1618 | Iter Mean Loss 0.3002\n",
      "2025-11-10 13:48:20,361 - root - INFO - KG Training: Epoch 0001 Iter 2223 / 4823 | Time 0.0s | Iter Loss 0.1715 | Iter Mean Loss 0.3000\n",
      "2025-11-10 13:48:20,485 - root - INFO - KG Training: Epoch 0001 Iter 2226 / 4823 | Time 0.0s | Iter Loss 0.1686 | Iter Mean Loss 0.2999\n",
      "2025-11-10 13:48:20,609 - root - INFO - KG Training: Epoch 0001 Iter 2229 / 4823 | Time 0.0s | Iter Loss 0.1724 | Iter Mean Loss 0.2997\n",
      "2025-11-10 13:48:20,733 - root - INFO - KG Training: Epoch 0001 Iter 2232 / 4823 | Time 0.0s | Iter Loss 0.1860 | Iter Mean Loss 0.2995\n",
      "2025-11-10 13:48:20,856 - root - INFO - KG Training: Epoch 0001 Iter 2235 / 4823 | Time 0.0s | Iter Loss 0.1733 | Iter Mean Loss 0.2994\n",
      "2025-11-10 13:48:20,983 - root - INFO - KG Training: Epoch 0001 Iter 2238 / 4823 | Time 0.0s | Iter Loss 0.1834 | Iter Mean Loss 0.2992\n",
      "2025-11-10 13:48:21,106 - root - INFO - KG Training: Epoch 0001 Iter 2241 / 4823 | Time 0.0s | Iter Loss 0.1610 | Iter Mean Loss 0.2990\n",
      "2025-11-10 13:48:21,302 - root - INFO - KG Training: Epoch 0001 Iter 2244 / 4823 | Time 0.0s | Iter Loss 0.1741 | Iter Mean Loss 0.2989\n",
      "2025-11-10 13:48:21,425 - root - INFO - KG Training: Epoch 0001 Iter 2247 / 4823 | Time 0.0s | Iter Loss 0.1566 | Iter Mean Loss 0.2987\n",
      "2025-11-10 13:48:21,550 - root - INFO - KG Training: Epoch 0001 Iter 2250 / 4823 | Time 0.0s | Iter Loss 0.1707 | Iter Mean Loss 0.2985\n",
      "2025-11-10 13:48:21,674 - root - INFO - KG Training: Epoch 0001 Iter 2253 / 4823 | Time 0.0s | Iter Loss 0.1627 | Iter Mean Loss 0.2984\n",
      "2025-11-10 13:48:21,801 - root - INFO - KG Training: Epoch 0001 Iter 2256 / 4823 | Time 0.0s | Iter Loss 0.1687 | Iter Mean Loss 0.2982\n",
      "2025-11-10 13:48:21,924 - root - INFO - KG Training: Epoch 0001 Iter 2259 / 4823 | Time 0.0s | Iter Loss 0.1880 | Iter Mean Loss 0.2980\n",
      "2025-11-10 13:48:22,047 - root - INFO - KG Training: Epoch 0001 Iter 2262 / 4823 | Time 0.0s | Iter Loss 0.1707 | Iter Mean Loss 0.2979\n",
      "2025-11-10 13:48:22,203 - root - INFO - KG Training: Epoch 0001 Iter 2265 / 4823 | Time 0.1s | Iter Loss 0.1586 | Iter Mean Loss 0.2977\n",
      "2025-11-10 13:48:22,332 - root - INFO - KG Training: Epoch 0001 Iter 2268 / 4823 | Time 0.0s | Iter Loss 0.1879 | Iter Mean Loss 0.2975\n",
      "2025-11-10 13:48:22,523 - root - INFO - KG Training: Epoch 0001 Iter 2271 / 4823 | Time 0.0s | Iter Loss 0.1854 | Iter Mean Loss 0.2974\n",
      "2025-11-10 13:48:22,653 - root - INFO - KG Training: Epoch 0001 Iter 2274 / 4823 | Time 0.0s | Iter Loss 0.1640 | Iter Mean Loss 0.2972\n",
      "2025-11-10 13:48:22,778 - root - INFO - KG Training: Epoch 0001 Iter 2277 / 4823 | Time 0.0s | Iter Loss 0.1709 | Iter Mean Loss 0.2970\n",
      "2025-11-10 13:48:22,901 - root - INFO - KG Training: Epoch 0001 Iter 2280 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2969\n",
      "2025-11-10 13:48:23,026 - root - INFO - KG Training: Epoch 0001 Iter 2283 / 4823 | Time 0.0s | Iter Loss 0.1717 | Iter Mean Loss 0.2967\n",
      "2025-11-10 13:48:23,259 - root - INFO - KG Training: Epoch 0001 Iter 2286 / 4823 | Time 0.0s | Iter Loss 0.1750 | Iter Mean Loss 0.2965\n",
      "2025-11-10 13:48:23,384 - root - INFO - KG Training: Epoch 0001 Iter 2289 / 4823 | Time 0.0s | Iter Loss 0.1694 | Iter Mean Loss 0.2964\n",
      "2025-11-10 13:48:23,516 - root - INFO - KG Training: Epoch 0001 Iter 2292 / 4823 | Time 0.0s | Iter Loss 0.1875 | Iter Mean Loss 0.2962\n",
      "2025-11-10 13:48:23,638 - root - INFO - KG Training: Epoch 0001 Iter 2295 / 4823 | Time 0.0s | Iter Loss 0.1660 | Iter Mean Loss 0.2960\n",
      "2025-11-10 13:48:23,849 - root - INFO - KG Training: Epoch 0001 Iter 2298 / 4823 | Time 0.0s | Iter Loss 0.1657 | Iter Mean Loss 0.2959\n",
      "2025-11-10 13:48:23,983 - root - INFO - KG Training: Epoch 0001 Iter 2301 / 4823 | Time 0.0s | Iter Loss 0.1761 | Iter Mean Loss 0.2957\n",
      "2025-11-10 13:48:24,260 - root - INFO - KG Training: Epoch 0001 Iter 2304 / 4823 | Time 0.0s | Iter Loss 0.1768 | Iter Mean Loss 0.2955\n",
      "2025-11-10 13:48:24,392 - root - INFO - KG Training: Epoch 0001 Iter 2307 / 4823 | Time 0.0s | Iter Loss 0.1698 | Iter Mean Loss 0.2954\n",
      "2025-11-10 13:48:24,764 - root - INFO - KG Training: Epoch 0001 Iter 2310 / 4823 | Time 0.0s | Iter Loss 0.1665 | Iter Mean Loss 0.2952\n",
      "2025-11-10 13:48:24,892 - root - INFO - KG Training: Epoch 0001 Iter 2313 / 4823 | Time 0.0s | Iter Loss 0.1823 | Iter Mean Loss 0.2950\n",
      "2025-11-10 13:48:25,020 - root - INFO - KG Training: Epoch 0001 Iter 2316 / 4823 | Time 0.0s | Iter Loss 0.1668 | Iter Mean Loss 0.2949\n",
      "2025-11-10 13:48:25,229 - root - INFO - KG Training: Epoch 0001 Iter 2319 / 4823 | Time 0.1s | Iter Loss 0.1772 | Iter Mean Loss 0.2947\n",
      "2025-11-10 13:48:25,362 - root - INFO - KG Training: Epoch 0001 Iter 2322 / 4823 | Time 0.0s | Iter Loss 0.1619 | Iter Mean Loss 0.2946\n",
      "2025-11-10 13:48:25,487 - root - INFO - KG Training: Epoch 0001 Iter 2325 / 4823 | Time 0.0s | Iter Loss 0.1784 | Iter Mean Loss 0.2944\n",
      "2025-11-10 13:48:25,676 - root - INFO - KG Training: Epoch 0001 Iter 2328 / 4823 | Time 0.1s | Iter Loss 0.1644 | Iter Mean Loss 0.2943\n",
      "2025-11-10 13:48:25,978 - root - INFO - KG Training: Epoch 0001 Iter 2331 / 4823 | Time 0.0s | Iter Loss 0.1728 | Iter Mean Loss 0.2941\n",
      "2025-11-10 13:48:26,106 - root - INFO - KG Training: Epoch 0001 Iter 2334 / 4823 | Time 0.0s | Iter Loss 0.1657 | Iter Mean Loss 0.2939\n",
      "2025-11-10 13:48:26,231 - root - INFO - KG Training: Epoch 0001 Iter 2337 / 4823 | Time 0.0s | Iter Loss 0.1679 | Iter Mean Loss 0.2938\n",
      "2025-11-10 13:48:26,355 - root - INFO - KG Training: Epoch 0001 Iter 2340 / 4823 | Time 0.0s | Iter Loss 0.1616 | Iter Mean Loss 0.2936\n",
      "2025-11-10 13:48:26,484 - root - INFO - KG Training: Epoch 0001 Iter 2343 / 4823 | Time 0.0s | Iter Loss 0.1751 | Iter Mean Loss 0.2934\n",
      "2025-11-10 13:48:26,607 - root - INFO - KG Training: Epoch 0001 Iter 2346 / 4823 | Time 0.0s | Iter Loss 0.1668 | Iter Mean Loss 0.2933\n",
      "2025-11-10 13:48:26,884 - root - INFO - KG Training: Epoch 0001 Iter 2349 / 4823 | Time 0.0s | Iter Loss 0.1747 | Iter Mean Loss 0.2931\n",
      "2025-11-10 13:48:27,016 - root - INFO - KG Training: Epoch 0001 Iter 2352 / 4823 | Time 0.0s | Iter Loss 0.1639 | Iter Mean Loss 0.2930\n",
      "2025-11-10 13:48:27,141 - root - INFO - KG Training: Epoch 0001 Iter 2355 / 4823 | Time 0.0s | Iter Loss 0.1555 | Iter Mean Loss 0.2928\n",
      "2025-11-10 13:48:27,269 - root - INFO - KG Training: Epoch 0001 Iter 2358 / 4823 | Time 0.0s | Iter Loss 0.1711 | Iter Mean Loss 0.2927\n",
      "2025-11-10 13:48:27,391 - root - INFO - KG Training: Epoch 0001 Iter 2361 / 4823 | Time 0.0s | Iter Loss 0.1875 | Iter Mean Loss 0.2925\n",
      "2025-11-10 13:48:27,536 - root - INFO - KG Training: Epoch 0001 Iter 2364 / 4823 | Time 0.0s | Iter Loss 0.1636 | Iter Mean Loss 0.2924\n",
      "2025-11-10 13:48:27,663 - root - INFO - KG Training: Epoch 0001 Iter 2367 / 4823 | Time 0.0s | Iter Loss 0.1732 | Iter Mean Loss 0.2922\n",
      "2025-11-10 13:48:27,789 - root - INFO - KG Training: Epoch 0001 Iter 2370 / 4823 | Time 0.0s | Iter Loss 0.1779 | Iter Mean Loss 0.2920\n",
      "2025-11-10 13:48:27,913 - root - INFO - KG Training: Epoch 0001 Iter 2373 / 4823 | Time 0.0s | Iter Loss 0.1635 | Iter Mean Loss 0.2919\n",
      "2025-11-10 13:48:28,035 - root - INFO - KG Training: Epoch 0001 Iter 2376 / 4823 | Time 0.0s | Iter Loss 0.1755 | Iter Mean Loss 0.2917\n",
      "2025-11-10 13:48:28,158 - root - INFO - KG Training: Epoch 0001 Iter 2379 / 4823 | Time 0.0s | Iter Loss 0.1791 | Iter Mean Loss 0.2916\n",
      "2025-11-10 13:48:28,308 - root - INFO - KG Training: Epoch 0001 Iter 2382 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2914\n",
      "2025-11-10 13:48:28,430 - root - INFO - KG Training: Epoch 0001 Iter 2385 / 4823 | Time 0.0s | Iter Loss 0.1786 | Iter Mean Loss 0.2912\n",
      "2025-11-10 13:48:28,557 - root - INFO - KG Training: Epoch 0001 Iter 2388 / 4823 | Time 0.0s | Iter Loss 0.1606 | Iter Mean Loss 0.2911\n",
      "2025-11-10 13:48:28,772 - root - INFO - KG Training: Epoch 0001 Iter 2391 / 4823 | Time 0.0s | Iter Loss 0.1621 | Iter Mean Loss 0.2909\n",
      "2025-11-10 13:48:28,998 - root - INFO - KG Training: Epoch 0001 Iter 2394 / 4823 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.2908\n",
      "2025-11-10 13:48:29,122 - root - INFO - KG Training: Epoch 0001 Iter 2397 / 4823 | Time 0.0s | Iter Loss 0.1621 | Iter Mean Loss 0.2906\n",
      "2025-11-10 13:48:29,261 - root - INFO - KG Training: Epoch 0001 Iter 2400 / 4823 | Time 0.0s | Iter Loss 0.1646 | Iter Mean Loss 0.2905\n",
      "2025-11-10 13:48:29,548 - root - INFO - KG Training: Epoch 0001 Iter 2403 / 4823 | Time 0.0s | Iter Loss 0.1861 | Iter Mean Loss 0.2903\n",
      "2025-11-10 13:48:29,681 - root - INFO - KG Training: Epoch 0001 Iter 2406 / 4823 | Time 0.0s | Iter Loss 0.1671 | Iter Mean Loss 0.2902\n",
      "2025-11-10 13:48:29,819 - root - INFO - KG Training: Epoch 0001 Iter 2409 / 4823 | Time 0.0s | Iter Loss 0.1711 | Iter Mean Loss 0.2900\n",
      "2025-11-10 13:48:30,023 - root - INFO - KG Training: Epoch 0001 Iter 2412 / 4823 | Time 0.0s | Iter Loss 0.1761 | Iter Mean Loss 0.2898\n",
      "2025-11-10 13:48:30,260 - root - INFO - KG Training: Epoch 0001 Iter 2415 / 4823 | Time 0.1s | Iter Loss 0.1852 | Iter Mean Loss 0.2897\n",
      "2025-11-10 13:48:30,391 - root - INFO - KG Training: Epoch 0001 Iter 2418 / 4823 | Time 0.0s | Iter Loss 0.1671 | Iter Mean Loss 0.2895\n",
      "2025-11-10 13:48:30,529 - root - INFO - KG Training: Epoch 0001 Iter 2421 / 4823 | Time 0.0s | Iter Loss 0.1681 | Iter Mean Loss 0.2894\n",
      "2025-11-10 13:48:30,685 - root - INFO - KG Training: Epoch 0001 Iter 2424 / 4823 | Time 0.1s | Iter Loss 0.1729 | Iter Mean Loss 0.2892\n",
      "2025-11-10 13:48:30,816 - root - INFO - KG Training: Epoch 0001 Iter 2427 / 4823 | Time 0.0s | Iter Loss 0.1604 | Iter Mean Loss 0.2891\n",
      "2025-11-10 13:48:31,010 - root - INFO - KG Training: Epoch 0001 Iter 2430 / 4823 | Time 0.0s | Iter Loss 0.1720 | Iter Mean Loss 0.2889\n",
      "2025-11-10 13:48:31,137 - root - INFO - KG Training: Epoch 0001 Iter 2433 / 4823 | Time 0.0s | Iter Loss 0.1820 | Iter Mean Loss 0.2888\n",
      "2025-11-10 13:48:31,262 - root - INFO - KG Training: Epoch 0001 Iter 2436 / 4823 | Time 0.0s | Iter Loss 0.1635 | Iter Mean Loss 0.2886\n",
      "2025-11-10 13:48:31,453 - root - INFO - KG Training: Epoch 0001 Iter 2439 / 4823 | Time 0.1s | Iter Loss 0.1538 | Iter Mean Loss 0.2885\n",
      "2025-11-10 13:48:31,588 - root - INFO - KG Training: Epoch 0001 Iter 2442 / 4823 | Time 0.0s | Iter Loss 0.1721 | Iter Mean Loss 0.2883\n",
      "2025-11-10 13:48:31,967 - root - INFO - KG Training: Epoch 0001 Iter 2445 / 4823 | Time 0.3s | Iter Loss 0.1757 | Iter Mean Loss 0.2882\n",
      "2025-11-10 13:48:32,095 - root - INFO - KG Training: Epoch 0001 Iter 2448 / 4823 | Time 0.0s | Iter Loss 0.1664 | Iter Mean Loss 0.2880\n",
      "2025-11-10 13:48:32,218 - root - INFO - KG Training: Epoch 0001 Iter 2451 / 4823 | Time 0.0s | Iter Loss 0.1687 | Iter Mean Loss 0.2879\n",
      "2025-11-10 13:48:32,346 - root - INFO - KG Training: Epoch 0001 Iter 2454 / 4823 | Time 0.0s | Iter Loss 0.1625 | Iter Mean Loss 0.2877\n",
      "2025-11-10 13:48:32,473 - root - INFO - KG Training: Epoch 0001 Iter 2457 / 4823 | Time 0.0s | Iter Loss 0.1738 | Iter Mean Loss 0.2876\n",
      "2025-11-10 13:48:32,600 - root - INFO - KG Training: Epoch 0001 Iter 2460 / 4823 | Time 0.0s | Iter Loss 0.1656 | Iter Mean Loss 0.2875\n",
      "2025-11-10 13:48:32,947 - root - INFO - KG Training: Epoch 0001 Iter 2463 / 4823 | Time 0.0s | Iter Loss 0.1543 | Iter Mean Loss 0.2873\n",
      "2025-11-10 13:48:33,071 - root - INFO - KG Training: Epoch 0001 Iter 2466 / 4823 | Time 0.0s | Iter Loss 0.1638 | Iter Mean Loss 0.2871\n",
      "2025-11-10 13:48:33,194 - root - INFO - KG Training: Epoch 0001 Iter 2469 / 4823 | Time 0.0s | Iter Loss 0.1759 | Iter Mean Loss 0.2870\n",
      "2025-11-10 13:48:33,317 - root - INFO - KG Training: Epoch 0001 Iter 2472 / 4823 | Time 0.0s | Iter Loss 0.1833 | Iter Mean Loss 0.2869\n",
      "2025-11-10 13:48:33,441 - root - INFO - KG Training: Epoch 0001 Iter 2475 / 4823 | Time 0.0s | Iter Loss 0.1588 | Iter Mean Loss 0.2867\n",
      "2025-11-10 13:48:33,565 - root - INFO - KG Training: Epoch 0001 Iter 2478 / 4823 | Time 0.0s | Iter Loss 0.1658 | Iter Mean Loss 0.2866\n",
      "2025-11-10 13:48:33,786 - root - INFO - KG Training: Epoch 0001 Iter 2481 / 4823 | Time 0.1s | Iter Loss 0.1548 | Iter Mean Loss 0.2864\n",
      "2025-11-10 13:48:33,913 - root - INFO - KG Training: Epoch 0001 Iter 2484 / 4823 | Time 0.0s | Iter Loss 0.1808 | Iter Mean Loss 0.2863\n",
      "2025-11-10 13:48:34,035 - root - INFO - KG Training: Epoch 0001 Iter 2487 / 4823 | Time 0.0s | Iter Loss 0.1608 | Iter Mean Loss 0.2861\n",
      "2025-11-10 13:48:34,167 - root - INFO - KG Training: Epoch 0001 Iter 2490 / 4823 | Time 0.0s | Iter Loss 0.1558 | Iter Mean Loss 0.2860\n",
      "2025-11-10 13:48:34,311 - root - INFO - KG Training: Epoch 0001 Iter 2493 / 4823 | Time 0.0s | Iter Loss 0.1771 | Iter Mean Loss 0.2858\n",
      "2025-11-10 13:48:34,440 - root - INFO - KG Training: Epoch 0001 Iter 2496 / 4823 | Time 0.0s | Iter Loss 0.1685 | Iter Mean Loss 0.2857\n",
      "2025-11-10 13:48:34,572 - root - INFO - KG Training: Epoch 0001 Iter 2499 / 4823 | Time 0.0s | Iter Loss 0.1692 | Iter Mean Loss 0.2856\n",
      "2025-11-10 13:48:34,696 - root - INFO - KG Training: Epoch 0001 Iter 2502 / 4823 | Time 0.0s | Iter Loss 0.1601 | Iter Mean Loss 0.2854\n",
      "2025-11-10 13:48:34,819 - root - INFO - KG Training: Epoch 0001 Iter 2505 / 4823 | Time 0.0s | Iter Loss 0.1672 | Iter Mean Loss 0.2853\n",
      "2025-11-10 13:48:34,942 - root - INFO - KG Training: Epoch 0001 Iter 2508 / 4823 | Time 0.0s | Iter Loss 0.1691 | Iter Mean Loss 0.2851\n",
      "2025-11-10 13:48:35,134 - root - INFO - KG Training: Epoch 0001 Iter 2511 / 4823 | Time 0.1s | Iter Loss 0.1792 | Iter Mean Loss 0.2850\n",
      "2025-11-10 13:48:35,601 - root - INFO - KG Training: Epoch 0001 Iter 2514 / 4823 | Time 0.0s | Iter Loss 0.1644 | Iter Mean Loss 0.2848\n",
      "2025-11-10 13:48:35,735 - root - INFO - KG Training: Epoch 0001 Iter 2517 / 4823 | Time 0.0s | Iter Loss 0.1642 | Iter Mean Loss 0.2847\n",
      "2025-11-10 13:48:35,857 - root - INFO - KG Training: Epoch 0001 Iter 2520 / 4823 | Time 0.0s | Iter Loss 0.1732 | Iter Mean Loss 0.2846\n",
      "2025-11-10 13:48:35,985 - root - INFO - KG Training: Epoch 0001 Iter 2523 / 4823 | Time 0.0s | Iter Loss 0.1582 | Iter Mean Loss 0.2844\n",
      "2025-11-10 13:48:36,108 - root - INFO - KG Training: Epoch 0001 Iter 2526 / 4823 | Time 0.0s | Iter Loss 0.1545 | Iter Mean Loss 0.2843\n",
      "2025-11-10 13:48:36,234 - root - INFO - KG Training: Epoch 0001 Iter 2529 / 4823 | Time 0.0s | Iter Loss 0.1618 | Iter Mean Loss 0.2841\n",
      "2025-11-10 13:48:36,356 - root - INFO - KG Training: Epoch 0001 Iter 2532 / 4823 | Time 0.0s | Iter Loss 0.1534 | Iter Mean Loss 0.2840\n",
      "2025-11-10 13:48:36,571 - root - INFO - KG Training: Epoch 0001 Iter 2535 / 4823 | Time 0.0s | Iter Loss 0.1669 | Iter Mean Loss 0.2838\n",
      "2025-11-10 13:48:36,696 - root - INFO - KG Training: Epoch 0001 Iter 2538 / 4823 | Time 0.0s | Iter Loss 0.1772 | Iter Mean Loss 0.2837\n",
      "2025-11-10 13:48:37,218 - root - INFO - KG Training: Epoch 0001 Iter 2541 / 4823 | Time 0.1s | Iter Loss 0.1715 | Iter Mean Loss 0.2836\n",
      "2025-11-10 13:48:37,347 - root - INFO - KG Training: Epoch 0001 Iter 2544 / 4823 | Time 0.0s | Iter Loss 0.1560 | Iter Mean Loss 0.2834\n",
      "2025-11-10 13:48:37,509 - root - INFO - KG Training: Epoch 0001 Iter 2547 / 4823 | Time 0.0s | Iter Loss 0.1741 | Iter Mean Loss 0.2833\n",
      "2025-11-10 13:48:37,643 - root - INFO - KG Training: Epoch 0001 Iter 2550 / 4823 | Time 0.0s | Iter Loss 0.1629 | Iter Mean Loss 0.2831\n",
      "2025-11-10 13:48:37,769 - root - INFO - KG Training: Epoch 0001 Iter 2553 / 4823 | Time 0.0s | Iter Loss 0.1590 | Iter Mean Loss 0.2830\n",
      "2025-11-10 13:48:37,892 - root - INFO - KG Training: Epoch 0001 Iter 2556 / 4823 | Time 0.0s | Iter Loss 0.1527 | Iter Mean Loss 0.2829\n",
      "2025-11-10 13:48:38,017 - root - INFO - KG Training: Epoch 0001 Iter 2559 / 4823 | Time 0.0s | Iter Loss 0.1441 | Iter Mean Loss 0.2827\n",
      "2025-11-10 13:48:38,141 - root - INFO - KG Training: Epoch 0001 Iter 2562 / 4823 | Time 0.0s | Iter Loss 0.1563 | Iter Mean Loss 0.2826\n",
      "2025-11-10 13:48:38,263 - root - INFO - KG Training: Epoch 0001 Iter 2565 / 4823 | Time 0.0s | Iter Loss 0.1667 | Iter Mean Loss 0.2824\n",
      "2025-11-10 13:48:38,391 - root - INFO - KG Training: Epoch 0001 Iter 2568 / 4823 | Time 0.0s | Iter Loss 0.1516 | Iter Mean Loss 0.2823\n",
      "2025-11-10 13:48:38,519 - root - INFO - KG Training: Epoch 0001 Iter 2571 / 4823 | Time 0.0s | Iter Loss 0.1678 | Iter Mean Loss 0.2821\n",
      "2025-11-10 13:48:38,641 - root - INFO - KG Training: Epoch 0001 Iter 2574 / 4823 | Time 0.0s | Iter Loss 0.1632 | Iter Mean Loss 0.2820\n",
      "2025-11-10 13:48:38,766 - root - INFO - KG Training: Epoch 0001 Iter 2577 / 4823 | Time 0.0s | Iter Loss 0.1668 | Iter Mean Loss 0.2819\n",
      "2025-11-10 13:48:38,900 - root - INFO - KG Training: Epoch 0001 Iter 2580 / 4823 | Time 0.0s | Iter Loss 0.1639 | Iter Mean Loss 0.2817\n",
      "2025-11-10 13:48:39,024 - root - INFO - KG Training: Epoch 0001 Iter 2583 / 4823 | Time 0.0s | Iter Loss 0.1661 | Iter Mean Loss 0.2816\n",
      "2025-11-10 13:48:39,231 - root - INFO - KG Training: Epoch 0001 Iter 2586 / 4823 | Time 0.1s | Iter Loss 0.1657 | Iter Mean Loss 0.2814\n",
      "2025-11-10 13:48:39,362 - root - INFO - KG Training: Epoch 0001 Iter 2589 / 4823 | Time 0.0s | Iter Loss 0.1756 | Iter Mean Loss 0.2813\n",
      "2025-11-10 13:48:39,486 - root - INFO - KG Training: Epoch 0001 Iter 2592 / 4823 | Time 0.0s | Iter Loss 0.1563 | Iter Mean Loss 0.2812\n",
      "2025-11-10 13:48:39,695 - root - INFO - KG Training: Epoch 0001 Iter 2595 / 4823 | Time 0.0s | Iter Loss 0.1527 | Iter Mean Loss 0.2810\n",
      "2025-11-10 13:48:39,822 - root - INFO - KG Training: Epoch 0001 Iter 2598 / 4823 | Time 0.0s | Iter Loss 0.1608 | Iter Mean Loss 0.2809\n",
      "2025-11-10 13:48:40,029 - root - INFO - KG Training: Epoch 0001 Iter 2601 / 4823 | Time 0.0s | Iter Loss 0.1521 | Iter Mean Loss 0.2807\n",
      "2025-11-10 13:48:40,154 - root - INFO - KG Training: Epoch 0001 Iter 2604 / 4823 | Time 0.0s | Iter Loss 0.1621 | Iter Mean Loss 0.2806\n",
      "2025-11-10 13:48:40,277 - root - INFO - KG Training: Epoch 0001 Iter 2607 / 4823 | Time 0.0s | Iter Loss 0.1630 | Iter Mean Loss 0.2805\n",
      "2025-11-10 13:48:40,406 - root - INFO - KG Training: Epoch 0001 Iter 2610 / 4823 | Time 0.0s | Iter Loss 0.1677 | Iter Mean Loss 0.2803\n",
      "2025-11-10 13:48:40,598 - root - INFO - KG Training: Epoch 0001 Iter 2613 / 4823 | Time 0.0s | Iter Loss 0.1559 | Iter Mean Loss 0.2802\n",
      "2025-11-10 13:48:40,720 - root - INFO - KG Training: Epoch 0001 Iter 2616 / 4823 | Time 0.0s | Iter Loss 0.1618 | Iter Mean Loss 0.2801\n",
      "2025-11-10 13:48:40,849 - root - INFO - KG Training: Epoch 0001 Iter 2619 / 4823 | Time 0.0s | Iter Loss 0.1554 | Iter Mean Loss 0.2799\n",
      "2025-11-10 13:48:41,069 - root - INFO - KG Training: Epoch 0001 Iter 2622 / 4823 | Time 0.1s | Iter Loss 0.1513 | Iter Mean Loss 0.2798\n",
      "2025-11-10 13:48:41,199 - root - INFO - KG Training: Epoch 0001 Iter 2625 / 4823 | Time 0.0s | Iter Loss 0.1539 | Iter Mean Loss 0.2797\n",
      "2025-11-10 13:48:41,326 - root - INFO - KG Training: Epoch 0001 Iter 2628 / 4823 | Time 0.0s | Iter Loss 0.1738 | Iter Mean Loss 0.2795\n",
      "2025-11-10 13:48:41,450 - root - INFO - KG Training: Epoch 0001 Iter 2631 / 4823 | Time 0.0s | Iter Loss 0.1643 | Iter Mean Loss 0.2794\n",
      "2025-11-10 13:48:41,575 - root - INFO - KG Training: Epoch 0001 Iter 2634 / 4823 | Time 0.0s | Iter Loss 0.1588 | Iter Mean Loss 0.2793\n",
      "2025-11-10 13:48:41,700 - root - INFO - KG Training: Epoch 0001 Iter 2637 / 4823 | Time 0.0s | Iter Loss 0.1584 | Iter Mean Loss 0.2791\n",
      "2025-11-10 13:48:41,892 - root - INFO - KG Training: Epoch 0001 Iter 2640 / 4823 | Time 0.1s | Iter Loss 0.1703 | Iter Mean Loss 0.2790\n",
      "2025-11-10 13:48:42,016 - root - INFO - KG Training: Epoch 0001 Iter 2643 / 4823 | Time 0.0s | Iter Loss 0.1610 | Iter Mean Loss 0.2789\n",
      "2025-11-10 13:48:42,140 - root - INFO - KG Training: Epoch 0001 Iter 2646 / 4823 | Time 0.0s | Iter Loss 0.1513 | Iter Mean Loss 0.2787\n",
      "2025-11-10 13:48:42,330 - root - INFO - KG Training: Epoch 0001 Iter 2649 / 4823 | Time 0.1s | Iter Loss 0.1673 | Iter Mean Loss 0.2786\n",
      "2025-11-10 13:48:42,459 - root - INFO - KG Training: Epoch 0001 Iter 2652 / 4823 | Time 0.0s | Iter Loss 0.1503 | Iter Mean Loss 0.2784\n",
      "2025-11-10 13:48:42,590 - root - INFO - KG Training: Epoch 0001 Iter 2655 / 4823 | Time 0.0s | Iter Loss 0.1524 | Iter Mean Loss 0.2783\n",
      "2025-11-10 13:48:42,716 - root - INFO - KG Training: Epoch 0001 Iter 2658 / 4823 | Time 0.0s | Iter Loss 0.1632 | Iter Mean Loss 0.2782\n",
      "2025-11-10 13:48:42,840 - root - INFO - KG Training: Epoch 0001 Iter 2661 / 4823 | Time 0.0s | Iter Loss 0.1744 | Iter Mean Loss 0.2781\n",
      "2025-11-10 13:48:42,961 - root - INFO - KG Training: Epoch 0001 Iter 2664 / 4823 | Time 0.0s | Iter Loss 0.1648 | Iter Mean Loss 0.2779\n",
      "2025-11-10 13:48:43,084 - root - INFO - KG Training: Epoch 0001 Iter 2667 / 4823 | Time 0.0s | Iter Loss 0.1578 | Iter Mean Loss 0.2778\n",
      "2025-11-10 13:48:43,207 - root - INFO - KG Training: Epoch 0001 Iter 2670 / 4823 | Time 0.0s | Iter Loss 0.1553 | Iter Mean Loss 0.2776\n",
      "2025-11-10 13:48:43,332 - root - INFO - KG Training: Epoch 0001 Iter 2673 / 4823 | Time 0.0s | Iter Loss 0.1569 | Iter Mean Loss 0.2775\n",
      "2025-11-10 13:48:43,457 - root - INFO - KG Training: Epoch 0001 Iter 2676 / 4823 | Time 0.0s | Iter Loss 0.1538 | Iter Mean Loss 0.2774\n",
      "2025-11-10 13:48:43,581 - root - INFO - KG Training: Epoch 0001 Iter 2679 / 4823 | Time 0.0s | Iter Loss 0.1583 | Iter Mean Loss 0.2772\n",
      "2025-11-10 13:48:43,713 - root - INFO - KG Training: Epoch 0001 Iter 2682 / 4823 | Time 0.0s | Iter Loss 0.1545 | Iter Mean Loss 0.2771\n",
      "2025-11-10 13:48:43,840 - root - INFO - KG Training: Epoch 0001 Iter 2685 / 4823 | Time 0.0s | Iter Loss 0.1564 | Iter Mean Loss 0.2770\n",
      "2025-11-10 13:48:43,969 - root - INFO - KG Training: Epoch 0001 Iter 2688 / 4823 | Time 0.0s | Iter Loss 0.1590 | Iter Mean Loss 0.2768\n",
      "2025-11-10 13:48:44,095 - root - INFO - KG Training: Epoch 0001 Iter 2691 / 4823 | Time 0.0s | Iter Loss 0.1743 | Iter Mean Loss 0.2767\n",
      "2025-11-10 13:48:44,611 - root - INFO - KG Training: Epoch 0001 Iter 2694 / 4823 | Time 0.0s | Iter Loss 0.1633 | Iter Mean Loss 0.2766\n",
      "2025-11-10 13:48:44,736 - root - INFO - KG Training: Epoch 0001 Iter 2697 / 4823 | Time 0.0s | Iter Loss 0.1674 | Iter Mean Loss 0.2765\n",
      "2025-11-10 13:48:44,857 - root - INFO - KG Training: Epoch 0001 Iter 2700 / 4823 | Time 0.0s | Iter Loss 0.1592 | Iter Mean Loss 0.2763\n",
      "2025-11-10 13:48:44,980 - root - INFO - KG Training: Epoch 0001 Iter 2703 / 4823 | Time 0.0s | Iter Loss 0.1588 | Iter Mean Loss 0.2762\n",
      "2025-11-10 13:48:45,101 - root - INFO - KG Training: Epoch 0001 Iter 2706 / 4823 | Time 0.0s | Iter Loss 0.1760 | Iter Mean Loss 0.2761\n",
      "2025-11-10 13:48:45,226 - root - INFO - KG Training: Epoch 0001 Iter 2709 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2760\n",
      "2025-11-10 13:48:45,352 - root - INFO - KG Training: Epoch 0001 Iter 2712 / 4823 | Time 0.0s | Iter Loss 0.1620 | Iter Mean Loss 0.2758\n",
      "2025-11-10 13:48:45,475 - root - INFO - KG Training: Epoch 0001 Iter 2715 / 4823 | Time 0.0s | Iter Loss 0.1734 | Iter Mean Loss 0.2757\n",
      "2025-11-10 13:48:45,666 - root - INFO - KG Training: Epoch 0001 Iter 2718 / 4823 | Time 0.0s | Iter Loss 0.1543 | Iter Mean Loss 0.2756\n",
      "2025-11-10 13:48:45,792 - root - INFO - KG Training: Epoch 0001 Iter 2721 / 4823 | Time 0.0s | Iter Loss 0.1686 | Iter Mean Loss 0.2755\n",
      "2025-11-10 13:48:45,919 - root - INFO - KG Training: Epoch 0001 Iter 2724 / 4823 | Time 0.0s | Iter Loss 0.1811 | Iter Mean Loss 0.2753\n",
      "2025-11-10 13:48:46,044 - root - INFO - KG Training: Epoch 0001 Iter 2727 / 4823 | Time 0.0s | Iter Loss 0.1403 | Iter Mean Loss 0.2752\n",
      "2025-11-10 13:48:46,327 - root - INFO - KG Training: Epoch 0001 Iter 2730 / 4823 | Time 0.0s | Iter Loss 0.1584 | Iter Mean Loss 0.2751\n",
      "2025-11-10 13:48:46,547 - root - INFO - KG Training: Epoch 0001 Iter 2733 / 4823 | Time 0.0s | Iter Loss 0.1613 | Iter Mean Loss 0.2750\n",
      "2025-11-10 13:48:46,669 - root - INFO - KG Training: Epoch 0001 Iter 2736 / 4823 | Time 0.0s | Iter Loss 0.1590 | Iter Mean Loss 0.2748\n",
      "2025-11-10 13:48:46,799 - root - INFO - KG Training: Epoch 0001 Iter 2739 / 4823 | Time 0.0s | Iter Loss 0.1666 | Iter Mean Loss 0.2747\n",
      "2025-11-10 13:48:47,046 - root - INFO - KG Training: Epoch 0001 Iter 2742 / 4823 | Time 0.2s | Iter Loss 0.1511 | Iter Mean Loss 0.2746\n",
      "2025-11-10 13:48:47,173 - root - INFO - KG Training: Epoch 0001 Iter 2745 / 4823 | Time 0.0s | Iter Loss 0.1701 | Iter Mean Loss 0.2744\n",
      "2025-11-10 13:48:47,297 - root - INFO - KG Training: Epoch 0001 Iter 2748 / 4823 | Time 0.0s | Iter Loss 0.1573 | Iter Mean Loss 0.2743\n",
      "2025-11-10 13:48:47,490 - root - INFO - KG Training: Epoch 0001 Iter 2751 / 4823 | Time 0.0s | Iter Loss 0.1625 | Iter Mean Loss 0.2742\n",
      "2025-11-10 13:48:47,744 - root - INFO - KG Training: Epoch 0001 Iter 2754 / 4823 | Time 0.1s | Iter Loss 0.1590 | Iter Mean Loss 0.2741\n",
      "2025-11-10 13:48:47,875 - root - INFO - KG Training: Epoch 0001 Iter 2757 / 4823 | Time 0.0s | Iter Loss 0.1477 | Iter Mean Loss 0.2739\n",
      "2025-11-10 13:48:47,997 - root - INFO - KG Training: Epoch 0001 Iter 2760 / 4823 | Time 0.0s | Iter Loss 0.1526 | Iter Mean Loss 0.2738\n",
      "2025-11-10 13:48:48,121 - root - INFO - KG Training: Epoch 0001 Iter 2763 / 4823 | Time 0.0s | Iter Loss 0.1690 | Iter Mean Loss 0.2737\n",
      "2025-11-10 13:48:48,244 - root - INFO - KG Training: Epoch 0001 Iter 2766 / 4823 | Time 0.0s | Iter Loss 0.1573 | Iter Mean Loss 0.2736\n",
      "2025-11-10 13:48:48,461 - root - INFO - KG Training: Epoch 0001 Iter 2769 / 4823 | Time 0.0s | Iter Loss 0.1581 | Iter Mean Loss 0.2734\n",
      "2025-11-10 13:48:48,591 - root - INFO - KG Training: Epoch 0001 Iter 2772 / 4823 | Time 0.0s | Iter Loss 0.1738 | Iter Mean Loss 0.2733\n",
      "2025-11-10 13:48:48,717 - root - INFO - KG Training: Epoch 0001 Iter 2775 / 4823 | Time 0.0s | Iter Loss 0.1657 | Iter Mean Loss 0.2732\n",
      "2025-11-10 13:48:48,840 - root - INFO - KG Training: Epoch 0001 Iter 2778 / 4823 | Time 0.0s | Iter Loss 0.1663 | Iter Mean Loss 0.2731\n",
      "2025-11-10 13:48:48,972 - root - INFO - KG Training: Epoch 0001 Iter 2781 / 4823 | Time 0.0s | Iter Loss 0.1530 | Iter Mean Loss 0.2729\n",
      "2025-11-10 13:48:49,095 - root - INFO - KG Training: Epoch 0001 Iter 2784 / 4823 | Time 0.0s | Iter Loss 0.1557 | Iter Mean Loss 0.2728\n",
      "2025-11-10 13:48:49,285 - root - INFO - KG Training: Epoch 0001 Iter 2787 / 4823 | Time 0.0s | Iter Loss 0.1534 | Iter Mean Loss 0.2727\n",
      "2025-11-10 13:48:49,415 - root - INFO - KG Training: Epoch 0001 Iter 2790 / 4823 | Time 0.0s | Iter Loss 0.1634 | Iter Mean Loss 0.2726\n",
      "2025-11-10 13:48:49,538 - root - INFO - KG Training: Epoch 0001 Iter 2793 / 4823 | Time 0.0s | Iter Loss 0.1572 | Iter Mean Loss 0.2724\n",
      "2025-11-10 13:48:49,747 - root - INFO - KG Training: Epoch 0001 Iter 2796 / 4823 | Time 0.0s | Iter Loss 0.1474 | Iter Mean Loss 0.2723\n",
      "2025-11-10 13:48:50,129 - root - INFO - KG Training: Epoch 0001 Iter 2799 / 4823 | Time 0.0s | Iter Loss 0.1606 | Iter Mean Loss 0.2722\n",
      "2025-11-10 13:48:50,252 - root - INFO - KG Training: Epoch 0001 Iter 2802 / 4823 | Time 0.0s | Iter Loss 0.1683 | Iter Mean Loss 0.2721\n",
      "2025-11-10 13:48:50,440 - root - INFO - KG Training: Epoch 0001 Iter 2805 / 4823 | Time 0.0s | Iter Loss 0.1444 | Iter Mean Loss 0.2720\n",
      "2025-11-10 13:48:50,565 - root - INFO - KG Training: Epoch 0001 Iter 2808 / 4823 | Time 0.0s | Iter Loss 0.1367 | Iter Mean Loss 0.2718\n",
      "2025-11-10 13:48:51,061 - root - INFO - KG Training: Epoch 0001 Iter 2811 / 4823 | Time 0.0s | Iter Loss 0.1723 | Iter Mean Loss 0.2717\n",
      "2025-11-10 13:48:51,185 - root - INFO - KG Training: Epoch 0001 Iter 2814 / 4823 | Time 0.0s | Iter Loss 0.1504 | Iter Mean Loss 0.2716\n",
      "2025-11-10 13:48:51,307 - root - INFO - KG Training: Epoch 0001 Iter 2817 / 4823 | Time 0.0s | Iter Loss 0.1669 | Iter Mean Loss 0.2715\n",
      "2025-11-10 13:48:51,433 - root - INFO - KG Training: Epoch 0001 Iter 2820 / 4823 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.2713\n",
      "2025-11-10 13:48:51,556 - root - INFO - KG Training: Epoch 0001 Iter 2823 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2712\n",
      "2025-11-10 13:48:51,679 - root - INFO - KG Training: Epoch 0001 Iter 2826 / 4823 | Time 0.0s | Iter Loss 0.1497 | Iter Mean Loss 0.2711\n",
      "2025-11-10 13:48:51,948 - root - INFO - KG Training: Epoch 0001 Iter 2829 / 4823 | Time 0.2s | Iter Loss 0.1562 | Iter Mean Loss 0.2710\n",
      "2025-11-10 13:48:52,074 - root - INFO - KG Training: Epoch 0001 Iter 2832 / 4823 | Time 0.0s | Iter Loss 0.1518 | Iter Mean Loss 0.2708\n",
      "2025-11-10 13:48:52,202 - root - INFO - KG Training: Epoch 0001 Iter 2835 / 4823 | Time 0.0s | Iter Loss 0.1470 | Iter Mean Loss 0.2707\n",
      "2025-11-10 13:48:52,390 - root - INFO - KG Training: Epoch 0001 Iter 2838 / 4823 | Time 0.0s | Iter Loss 0.1645 | Iter Mean Loss 0.2706\n",
      "2025-11-10 13:48:52,645 - root - INFO - KG Training: Epoch 0001 Iter 2841 / 4823 | Time 0.0s | Iter Loss 0.1554 | Iter Mean Loss 0.2705\n",
      "2025-11-10 13:48:52,772 - root - INFO - KG Training: Epoch 0001 Iter 2844 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2704\n",
      "2025-11-10 13:48:52,895 - root - INFO - KG Training: Epoch 0001 Iter 2847 / 4823 | Time 0.0s | Iter Loss 0.1607 | Iter Mean Loss 0.2702\n",
      "2025-11-10 13:48:53,020 - root - INFO - KG Training: Epoch 0001 Iter 2850 / 4823 | Time 0.0s | Iter Loss 0.1561 | Iter Mean Loss 0.2701\n",
      "2025-11-10 13:48:53,146 - root - INFO - KG Training: Epoch 0001 Iter 2853 / 4823 | Time 0.0s | Iter Loss 0.1461 | Iter Mean Loss 0.2700\n",
      "2025-11-10 13:48:53,295 - root - INFO - KG Training: Epoch 0001 Iter 2856 / 4823 | Time 0.1s | Iter Loss 0.1564 | Iter Mean Loss 0.2699\n",
      "2025-11-10 13:48:53,524 - root - INFO - KG Training: Epoch 0001 Iter 2859 / 4823 | Time 0.0s | Iter Loss 0.1644 | Iter Mean Loss 0.2698\n",
      "2025-11-10 13:48:53,652 - root - INFO - KG Training: Epoch 0001 Iter 2862 / 4823 | Time 0.0s | Iter Loss 0.1418 | Iter Mean Loss 0.2696\n",
      "2025-11-10 13:48:53,775 - root - INFO - KG Training: Epoch 0001 Iter 2865 / 4823 | Time 0.0s | Iter Loss 0.1496 | Iter Mean Loss 0.2695\n",
      "2025-11-10 13:48:54,166 - root - INFO - KG Training: Epoch 0001 Iter 2868 / 4823 | Time 0.0s | Iter Loss 0.1480 | Iter Mean Loss 0.2694\n",
      "2025-11-10 13:48:54,292 - root - INFO - KG Training: Epoch 0001 Iter 2871 / 4823 | Time 0.0s | Iter Loss 0.1425 | Iter Mean Loss 0.2693\n",
      "2025-11-10 13:48:54,421 - root - INFO - KG Training: Epoch 0001 Iter 2874 / 4823 | Time 0.0s | Iter Loss 0.1505 | Iter Mean Loss 0.2692\n",
      "2025-11-10 13:48:54,564 - root - INFO - KG Training: Epoch 0001 Iter 2877 / 4823 | Time 0.0s | Iter Loss 0.1442 | Iter Mean Loss 0.2690\n",
      "2025-11-10 13:48:54,815 - root - INFO - KG Training: Epoch 0001 Iter 2880 / 4823 | Time 0.0s | Iter Loss 0.1536 | Iter Mean Loss 0.2689\n",
      "2025-11-10 13:48:54,944 - root - INFO - KG Training: Epoch 0001 Iter 2883 / 4823 | Time 0.0s | Iter Loss 0.1454 | Iter Mean Loss 0.2688\n",
      "2025-11-10 13:48:55,066 - root - INFO - KG Training: Epoch 0001 Iter 2886 / 4823 | Time 0.0s | Iter Loss 0.1534 | Iter Mean Loss 0.2687\n",
      "2025-11-10 13:48:55,190 - root - INFO - KG Training: Epoch 0001 Iter 2889 / 4823 | Time 0.0s | Iter Loss 0.1650 | Iter Mean Loss 0.2686\n",
      "2025-11-10 13:48:55,561 - root - INFO - KG Training: Epoch 0001 Iter 2892 / 4823 | Time 0.3s | Iter Loss 0.1476 | Iter Mean Loss 0.2685\n",
      "2025-11-10 13:48:55,689 - root - INFO - KG Training: Epoch 0001 Iter 2895 / 4823 | Time 0.0s | Iter Loss 0.1641 | Iter Mean Loss 0.2683\n",
      "2025-11-10 13:48:55,813 - root - INFO - KG Training: Epoch 0001 Iter 2898 / 4823 | Time 0.0s | Iter Loss 0.1594 | Iter Mean Loss 0.2682\n",
      "2025-11-10 13:48:55,938 - root - INFO - KG Training: Epoch 0001 Iter 2901 / 4823 | Time 0.0s | Iter Loss 0.1503 | Iter Mean Loss 0.2681\n",
      "2025-11-10 13:48:56,104 - root - INFO - KG Training: Epoch 0001 Iter 2904 / 4823 | Time 0.1s | Iter Loss 0.1670 | Iter Mean Loss 0.2680\n",
      "2025-11-10 13:48:56,228 - root - INFO - KG Training: Epoch 0001 Iter 2907 / 4823 | Time 0.0s | Iter Loss 0.1596 | Iter Mean Loss 0.2679\n",
      "2025-11-10 13:48:56,354 - root - INFO - KG Training: Epoch 0001 Iter 2910 / 4823 | Time 0.0s | Iter Loss 0.1623 | Iter Mean Loss 0.2678\n",
      "2025-11-10 13:48:56,674 - root - INFO - KG Training: Epoch 0001 Iter 2913 / 4823 | Time 0.0s | Iter Loss 0.1473 | Iter Mean Loss 0.2677\n",
      "2025-11-10 13:48:56,803 - root - INFO - KG Training: Epoch 0001 Iter 2916 / 4823 | Time 0.0s | Iter Loss 0.1549 | Iter Mean Loss 0.2675\n",
      "2025-11-10 13:48:56,925 - root - INFO - KG Training: Epoch 0001 Iter 2919 / 4823 | Time 0.0s | Iter Loss 0.1407 | Iter Mean Loss 0.2674\n",
      "2025-11-10 13:48:57,056 - root - INFO - KG Training: Epoch 0001 Iter 2922 / 4823 | Time 0.0s | Iter Loss 0.1684 | Iter Mean Loss 0.2673\n",
      "2025-11-10 13:48:57,196 - root - INFO - KG Training: Epoch 0001 Iter 2925 / 4823 | Time 0.0s | Iter Loss 0.1414 | Iter Mean Loss 0.2672\n",
      "2025-11-10 13:48:57,324 - root - INFO - KG Training: Epoch 0001 Iter 2928 / 4823 | Time 0.0s | Iter Loss 0.1481 | Iter Mean Loss 0.2670\n",
      "2025-11-10 13:48:57,452 - root - INFO - KG Training: Epoch 0001 Iter 2931 / 4823 | Time 0.0s | Iter Loss 0.1649 | Iter Mean Loss 0.2669\n",
      "2025-11-10 13:48:57,578 - root - INFO - KG Training: Epoch 0001 Iter 2934 / 4823 | Time 0.0s | Iter Loss 0.1480 | Iter Mean Loss 0.2668\n",
      "2025-11-10 13:48:57,706 - root - INFO - KG Training: Epoch 0001 Iter 2937 / 4823 | Time 0.0s | Iter Loss 0.1662 | Iter Mean Loss 0.2667\n",
      "2025-11-10 13:48:57,830 - root - INFO - KG Training: Epoch 0001 Iter 2940 / 4823 | Time 0.0s | Iter Loss 0.1559 | Iter Mean Loss 0.2666\n",
      "2025-11-10 13:48:57,951 - root - INFO - KG Training: Epoch 0001 Iter 2943 / 4823 | Time 0.0s | Iter Loss 0.1730 | Iter Mean Loss 0.2665\n",
      "2025-11-10 13:48:58,080 - root - INFO - KG Training: Epoch 0001 Iter 2946 / 4823 | Time 0.0s | Iter Loss 0.1632 | Iter Mean Loss 0.2664\n",
      "2025-11-10 13:48:58,204 - root - INFO - KG Training: Epoch 0001 Iter 2949 / 4823 | Time 0.0s | Iter Loss 0.1579 | Iter Mean Loss 0.2663\n",
      "2025-11-10 13:48:58,326 - root - INFO - KG Training: Epoch 0001 Iter 2952 / 4823 | Time 0.0s | Iter Loss 0.1478 | Iter Mean Loss 0.2661\n",
      "2025-11-10 13:48:58,569 - root - INFO - KG Training: Epoch 0001 Iter 2955 / 4823 | Time 0.0s | Iter Loss 0.1621 | Iter Mean Loss 0.2660\n",
      "2025-11-10 13:48:58,694 - root - INFO - KG Training: Epoch 0001 Iter 2958 / 4823 | Time 0.0s | Iter Loss 0.1380 | Iter Mean Loss 0.2659\n",
      "2025-11-10 13:48:58,817 - root - INFO - KG Training: Epoch 0001 Iter 2961 / 4823 | Time 0.0s | Iter Loss 0.1538 | Iter Mean Loss 0.2658\n",
      "2025-11-10 13:48:59,008 - root - INFO - KG Training: Epoch 0001 Iter 2964 / 4823 | Time 0.0s | Iter Loss 0.1445 | Iter Mean Loss 0.2657\n",
      "2025-11-10 13:48:59,379 - root - INFO - KG Training: Epoch 0001 Iter 2967 / 4823 | Time 0.0s | Iter Loss 0.1530 | Iter Mean Loss 0.2656\n",
      "2025-11-10 13:48:59,502 - root - INFO - KG Training: Epoch 0001 Iter 2970 / 4823 | Time 0.0s | Iter Loss 0.1453 | Iter Mean Loss 0.2655\n",
      "2025-11-10 13:48:59,626 - root - INFO - KG Training: Epoch 0001 Iter 2973 / 4823 | Time 0.0s | Iter Loss 0.1601 | Iter Mean Loss 0.2653\n",
      "2025-11-10 13:48:59,756 - root - INFO - KG Training: Epoch 0001 Iter 2976 / 4823 | Time 0.0s | Iter Loss 0.1570 | Iter Mean Loss 0.2652\n",
      "2025-11-10 13:48:59,880 - root - INFO - KG Training: Epoch 0001 Iter 2979 / 4823 | Time 0.0s | Iter Loss 0.1496 | Iter Mean Loss 0.2651\n",
      "2025-11-10 13:49:00,001 - root - INFO - KG Training: Epoch 0001 Iter 2982 / 4823 | Time 0.0s | Iter Loss 0.1563 | Iter Mean Loss 0.2650\n",
      "2025-11-10 13:49:00,123 - root - INFO - KG Training: Epoch 0001 Iter 2985 / 4823 | Time 0.0s | Iter Loss 0.1657 | Iter Mean Loss 0.2649\n",
      "2025-11-10 13:49:00,313 - root - INFO - KG Training: Epoch 0001 Iter 2988 / 4823 | Time 0.0s | Iter Loss 0.1595 | Iter Mean Loss 0.2648\n",
      "2025-11-10 13:49:00,435 - root - INFO - KG Training: Epoch 0001 Iter 2991 / 4823 | Time 0.0s | Iter Loss 0.1539 | Iter Mean Loss 0.2647\n",
      "2025-11-10 13:49:00,560 - root - INFO - KG Training: Epoch 0001 Iter 2994 / 4823 | Time 0.0s | Iter Loss 0.1622 | Iter Mean Loss 0.2646\n",
      "2025-11-10 13:49:00,686 - root - INFO - KG Training: Epoch 0001 Iter 2997 / 4823 | Time 0.0s | Iter Loss 0.1366 | Iter Mean Loss 0.2644\n",
      "2025-11-10 13:49:00,810 - root - INFO - KG Training: Epoch 0001 Iter 3000 / 4823 | Time 0.0s | Iter Loss 0.1555 | Iter Mean Loss 0.2643\n",
      "2025-11-10 13:49:00,935 - root - INFO - KG Training: Epoch 0001 Iter 3003 / 4823 | Time 0.0s | Iter Loss 0.1614 | Iter Mean Loss 0.2642\n",
      "2025-11-10 13:49:01,065 - root - INFO - KG Training: Epoch 0001 Iter 3006 / 4823 | Time 0.0s | Iter Loss 0.1486 | Iter Mean Loss 0.2641\n",
      "2025-11-10 13:49:01,189 - root - INFO - KG Training: Epoch 0001 Iter 3009 / 4823 | Time 0.0s | Iter Loss 0.1693 | Iter Mean Loss 0.2640\n",
      "2025-11-10 13:49:01,314 - root - INFO - KG Training: Epoch 0001 Iter 3012 / 4823 | Time 0.0s | Iter Loss 0.1443 | Iter Mean Loss 0.2639\n",
      "2025-11-10 13:49:01,435 - root - INFO - KG Training: Epoch 0001 Iter 3015 / 4823 | Time 0.0s | Iter Loss 0.1649 | Iter Mean Loss 0.2638\n",
      "2025-11-10 13:49:01,643 - root - INFO - KG Training: Epoch 0001 Iter 3018 / 4823 | Time 0.0s | Iter Loss 0.1516 | Iter Mean Loss 0.2637\n",
      "2025-11-10 13:49:01,808 - root - INFO - KG Training: Epoch 0001 Iter 3021 / 4823 | Time 0.0s | Iter Loss 0.1576 | Iter Mean Loss 0.2636\n",
      "2025-11-10 13:49:02,013 - root - INFO - KG Training: Epoch 0001 Iter 3024 / 4823 | Time 0.1s | Iter Loss 0.1421 | Iter Mean Loss 0.2635\n",
      "2025-11-10 13:49:02,207 - root - INFO - KG Training: Epoch 0001 Iter 3027 / 4823 | Time 0.0s | Iter Loss 0.1413 | Iter Mean Loss 0.2633\n",
      "2025-11-10 13:49:02,332 - root - INFO - KG Training: Epoch 0001 Iter 3030 / 4823 | Time 0.0s | Iter Loss 0.1513 | Iter Mean Loss 0.2632\n",
      "2025-11-10 13:49:02,457 - root - INFO - KG Training: Epoch 0001 Iter 3033 / 4823 | Time 0.0s | Iter Loss 0.1635 | Iter Mean Loss 0.2631\n",
      "2025-11-10 13:49:02,904 - root - INFO - KG Training: Epoch 0001 Iter 3036 / 4823 | Time 0.2s | Iter Loss 0.1468 | Iter Mean Loss 0.2630\n",
      "2025-11-10 13:49:03,032 - root - INFO - KG Training: Epoch 0001 Iter 3039 / 4823 | Time 0.0s | Iter Loss 0.1541 | Iter Mean Loss 0.2629\n",
      "2025-11-10 13:49:03,157 - root - INFO - KG Training: Epoch 0001 Iter 3042 / 4823 | Time 0.0s | Iter Loss 0.1455 | Iter Mean Loss 0.2628\n",
      "2025-11-10 13:49:03,279 - root - INFO - KG Training: Epoch 0001 Iter 3045 / 4823 | Time 0.0s | Iter Loss 0.1539 | Iter Mean Loss 0.2627\n",
      "2025-11-10 13:49:03,406 - root - INFO - KG Training: Epoch 0001 Iter 3048 / 4823 | Time 0.0s | Iter Loss 0.1520 | Iter Mean Loss 0.2626\n",
      "2025-11-10 13:49:03,529 - root - INFO - KG Training: Epoch 0001 Iter 3051 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2625\n",
      "2025-11-10 13:49:03,653 - root - INFO - KG Training: Epoch 0001 Iter 3054 / 4823 | Time 0.0s | Iter Loss 0.1590 | Iter Mean Loss 0.2624\n",
      "2025-11-10 13:49:03,975 - root - INFO - KG Training: Epoch 0001 Iter 3057 / 4823 | Time 0.2s | Iter Loss 0.1437 | Iter Mean Loss 0.2623\n",
      "2025-11-10 13:49:04,104 - root - INFO - KG Training: Epoch 0001 Iter 3060 / 4823 | Time 0.0s | Iter Loss 0.1552 | Iter Mean Loss 0.2621\n",
      "2025-11-10 13:49:04,234 - root - INFO - KG Training: Epoch 0001 Iter 3063 / 4823 | Time 0.0s | Iter Loss 0.1669 | Iter Mean Loss 0.2620\n",
      "2025-11-10 13:49:04,358 - root - INFO - KG Training: Epoch 0001 Iter 3066 / 4823 | Time 0.0s | Iter Loss 0.1499 | Iter Mean Loss 0.2619\n",
      "2025-11-10 13:49:04,478 - root - INFO - KG Training: Epoch 0001 Iter 3069 / 4823 | Time 0.0s | Iter Loss 0.1466 | Iter Mean Loss 0.2618\n",
      "2025-11-10 13:49:04,670 - root - INFO - KG Training: Epoch 0001 Iter 3072 / 4823 | Time 0.0s | Iter Loss 0.1580 | Iter Mean Loss 0.2617\n",
      "2025-11-10 13:49:04,793 - root - INFO - KG Training: Epoch 0001 Iter 3075 / 4823 | Time 0.0s | Iter Loss 0.1431 | Iter Mean Loss 0.2616\n",
      "2025-11-10 13:49:04,916 - root - INFO - KG Training: Epoch 0001 Iter 3078 / 4823 | Time 0.0s | Iter Loss 0.1451 | Iter Mean Loss 0.2615\n",
      "2025-11-10 13:49:05,046 - root - INFO - KG Training: Epoch 0001 Iter 3081 / 4823 | Time 0.0s | Iter Loss 0.1435 | Iter Mean Loss 0.2614\n",
      "2025-11-10 13:49:05,166 - root - INFO - KG Training: Epoch 0001 Iter 3084 / 4823 | Time 0.0s | Iter Loss 0.1458 | Iter Mean Loss 0.2613\n",
      "2025-11-10 13:49:05,290 - root - INFO - KG Training: Epoch 0001 Iter 3087 / 4823 | Time 0.0s | Iter Loss 0.1456 | Iter Mean Loss 0.2612\n",
      "2025-11-10 13:49:05,412 - root - INFO - KG Training: Epoch 0001 Iter 3090 / 4823 | Time 0.0s | Iter Loss 0.1547 | Iter Mean Loss 0.2610\n",
      "2025-11-10 13:49:05,533 - root - INFO - KG Training: Epoch 0001 Iter 3093 / 4823 | Time 0.0s | Iter Loss 0.1555 | Iter Mean Loss 0.2609\n",
      "2025-11-10 13:49:05,661 - root - INFO - KG Training: Epoch 0001 Iter 3096 / 4823 | Time 0.0s | Iter Loss 0.1553 | Iter Mean Loss 0.2608\n",
      "2025-11-10 13:49:05,870 - root - INFO - KG Training: Epoch 0001 Iter 3099 / 4823 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.2607\n",
      "2025-11-10 13:49:06,253 - root - INFO - KG Training: Epoch 0001 Iter 3102 / 4823 | Time 0.3s | Iter Loss 0.1401 | Iter Mean Loss 0.2606\n",
      "2025-11-10 13:49:06,384 - root - INFO - KG Training: Epoch 0001 Iter 3105 / 4823 | Time 0.0s | Iter Loss 0.1526 | Iter Mean Loss 0.2605\n",
      "2025-11-10 13:49:06,508 - root - INFO - KG Training: Epoch 0001 Iter 3108 / 4823 | Time 0.0s | Iter Loss 0.1469 | Iter Mean Loss 0.2604\n",
      "2025-11-10 13:49:06,633 - root - INFO - KG Training: Epoch 0001 Iter 3111 / 4823 | Time 0.0s | Iter Loss 0.1437 | Iter Mean Loss 0.2603\n",
      "2025-11-10 13:49:06,754 - root - INFO - KG Training: Epoch 0001 Iter 3114 / 4823 | Time 0.0s | Iter Loss 0.1505 | Iter Mean Loss 0.2602\n",
      "2025-11-10 13:49:06,881 - root - INFO - KG Training: Epoch 0001 Iter 3117 / 4823 | Time 0.0s | Iter Loss 0.1567 | Iter Mean Loss 0.2601\n",
      "2025-11-10 13:49:07,389 - root - INFO - KG Training: Epoch 0001 Iter 3120 / 4823 | Time 0.0s | Iter Loss 0.1473 | Iter Mean Loss 0.2600\n",
      "2025-11-10 13:49:07,517 - root - INFO - KG Training: Epoch 0001 Iter 3123 / 4823 | Time 0.0s | Iter Loss 0.1543 | Iter Mean Loss 0.2599\n",
      "2025-11-10 13:49:07,643 - root - INFO - KG Training: Epoch 0001 Iter 3126 / 4823 | Time 0.0s | Iter Loss 0.1576 | Iter Mean Loss 0.2598\n",
      "2025-11-10 13:49:07,765 - root - INFO - KG Training: Epoch 0001 Iter 3129 / 4823 | Time 0.0s | Iter Loss 0.1422 | Iter Mean Loss 0.2597\n",
      "2025-11-10 13:49:07,889 - root - INFO - KG Training: Epoch 0001 Iter 3132 / 4823 | Time 0.0s | Iter Loss 0.1528 | Iter Mean Loss 0.2596\n",
      "2025-11-10 13:49:08,015 - root - INFO - KG Training: Epoch 0001 Iter 3135 / 4823 | Time 0.0s | Iter Loss 0.1556 | Iter Mean Loss 0.2595\n",
      "2025-11-10 13:49:08,139 - root - INFO - KG Training: Epoch 0001 Iter 3138 / 4823 | Time 0.0s | Iter Loss 0.1511 | Iter Mean Loss 0.2594\n",
      "2025-11-10 13:49:08,262 - root - INFO - KG Training: Epoch 0001 Iter 3141 / 4823 | Time 0.0s | Iter Loss 0.1415 | Iter Mean Loss 0.2593\n",
      "2025-11-10 13:49:08,386 - root - INFO - KG Training: Epoch 0001 Iter 3144 / 4823 | Time 0.0s | Iter Loss 0.1529 | Iter Mean Loss 0.2592\n",
      "2025-11-10 13:49:08,510 - root - INFO - KG Training: Epoch 0001 Iter 3147 / 4823 | Time 0.0s | Iter Loss 0.1447 | Iter Mean Loss 0.2591\n",
      "2025-11-10 13:49:08,634 - root - INFO - KG Training: Epoch 0001 Iter 3150 / 4823 | Time 0.0s | Iter Loss 0.1463 | Iter Mean Loss 0.2590\n",
      "2025-11-10 13:49:08,763 - root - INFO - KG Training: Epoch 0001 Iter 3153 / 4823 | Time 0.0s | Iter Loss 0.1390 | Iter Mean Loss 0.2588\n",
      "2025-11-10 13:49:08,952 - root - INFO - KG Training: Epoch 0001 Iter 3156 / 4823 | Time 0.0s | Iter Loss 0.1310 | Iter Mean Loss 0.2587\n",
      "2025-11-10 13:49:09,080 - root - INFO - KG Training: Epoch 0001 Iter 3159 / 4823 | Time 0.0s | Iter Loss 0.1480 | Iter Mean Loss 0.2586\n",
      "2025-11-10 13:49:09,207 - root - INFO - KG Training: Epoch 0001 Iter 3162 / 4823 | Time 0.0s | Iter Loss 0.1360 | Iter Mean Loss 0.2585\n",
      "2025-11-10 13:49:09,331 - root - INFO - KG Training: Epoch 0001 Iter 3165 / 4823 | Time 0.0s | Iter Loss 0.1626 | Iter Mean Loss 0.2584\n",
      "2025-11-10 13:49:09,456 - root - INFO - KG Training: Epoch 0001 Iter 3168 / 4823 | Time 0.0s | Iter Loss 0.1482 | Iter Mean Loss 0.2583\n",
      "2025-11-10 13:49:09,581 - root - INFO - KG Training: Epoch 0001 Iter 3171 / 4823 | Time 0.0s | Iter Loss 0.1504 | Iter Mean Loss 0.2582\n",
      "2025-11-10 13:49:09,708 - root - INFO - KG Training: Epoch 0001 Iter 3174 / 4823 | Time 0.0s | Iter Loss 0.1511 | Iter Mean Loss 0.2581\n",
      "2025-11-10 13:49:09,899 - root - INFO - KG Training: Epoch 0001 Iter 3177 / 4823 | Time 0.0s | Iter Loss 0.1566 | Iter Mean Loss 0.2580\n",
      "2025-11-10 13:49:10,022 - root - INFO - KG Training: Epoch 0001 Iter 3180 / 4823 | Time 0.0s | Iter Loss 0.1587 | Iter Mean Loss 0.2579\n",
      "2025-11-10 13:49:10,149 - root - INFO - KG Training: Epoch 0001 Iter 3183 / 4823 | Time 0.0s | Iter Loss 0.1495 | Iter Mean Loss 0.2578\n",
      "2025-11-10 13:49:10,401 - root - INFO - KG Training: Epoch 0001 Iter 3186 / 4823 | Time 0.0s | Iter Loss 0.1484 | Iter Mean Loss 0.2577\n",
      "2025-11-10 13:49:10,531 - root - INFO - KG Training: Epoch 0001 Iter 3189 / 4823 | Time 0.0s | Iter Loss 0.1399 | Iter Mean Loss 0.2576\n",
      "2025-11-10 13:49:10,658 - root - INFO - KG Training: Epoch 0001 Iter 3192 / 4823 | Time 0.0s | Iter Loss 0.1428 | Iter Mean Loss 0.2575\n",
      "2025-11-10 13:49:10,781 - root - INFO - KG Training: Epoch 0001 Iter 3195 / 4823 | Time 0.0s | Iter Loss 0.1530 | Iter Mean Loss 0.2574\n",
      "2025-11-10 13:49:10,987 - root - INFO - KG Training: Epoch 0001 Iter 3198 / 4823 | Time 0.0s | Iter Loss 0.1455 | Iter Mean Loss 0.2573\n",
      "2025-11-10 13:49:11,113 - root - INFO - KG Training: Epoch 0001 Iter 3201 / 4823 | Time 0.0s | Iter Loss 0.1465 | Iter Mean Loss 0.2572\n",
      "2025-11-10 13:49:11,235 - root - INFO - KG Training: Epoch 0001 Iter 3204 / 4823 | Time 0.0s | Iter Loss 0.1559 | Iter Mean Loss 0.2571\n",
      "2025-11-10 13:49:11,360 - root - INFO - KG Training: Epoch 0001 Iter 3207 / 4823 | Time 0.0s | Iter Loss 0.1524 | Iter Mean Loss 0.2570\n",
      "2025-11-10 13:49:11,481 - root - INFO - KG Training: Epoch 0001 Iter 3210 / 4823 | Time 0.0s | Iter Loss 0.1440 | Iter Mean Loss 0.2569\n",
      "2025-11-10 13:49:11,606 - root - INFO - KG Training: Epoch 0001 Iter 3213 / 4823 | Time 0.0s | Iter Loss 0.1383 | Iter Mean Loss 0.2568\n",
      "2025-11-10 13:49:11,799 - root - INFO - KG Training: Epoch 0001 Iter 3216 / 4823 | Time 0.0s | Iter Loss 0.1484 | Iter Mean Loss 0.2567\n",
      "2025-11-10 13:49:11,922 - root - INFO - KG Training: Epoch 0001 Iter 3219 / 4823 | Time 0.0s | Iter Loss 0.1702 | Iter Mean Loss 0.2566\n",
      "2025-11-10 13:49:12,201 - root - INFO - KG Training: Epoch 0001 Iter 3222 / 4823 | Time 0.1s | Iter Loss 0.1434 | Iter Mean Loss 0.2565\n",
      "2025-11-10 13:49:12,331 - root - INFO - KG Training: Epoch 0001 Iter 3225 / 4823 | Time 0.0s | Iter Loss 0.1468 | Iter Mean Loss 0.2564\n",
      "2025-11-10 13:49:12,456 - root - INFO - KG Training: Epoch 0001 Iter 3228 / 4823 | Time 0.0s | Iter Loss 0.1556 | Iter Mean Loss 0.2563\n",
      "2025-11-10 13:49:12,654 - root - INFO - KG Training: Epoch 0001 Iter 3231 / 4823 | Time 0.1s | Iter Loss 0.1607 | Iter Mean Loss 0.2562\n",
      "2025-11-10 13:49:12,783 - root - INFO - KG Training: Epoch 0001 Iter 3234 / 4823 | Time 0.0s | Iter Loss 0.1552 | Iter Mean Loss 0.2561\n",
      "2025-11-10 13:49:12,906 - root - INFO - KG Training: Epoch 0001 Iter 3237 / 4823 | Time 0.0s | Iter Loss 0.1506 | Iter Mean Loss 0.2560\n",
      "2025-11-10 13:49:13,030 - root - INFO - KG Training: Epoch 0001 Iter 3240 / 4823 | Time 0.0s | Iter Loss 0.1630 | Iter Mean Loss 0.2559\n",
      "2025-11-10 13:49:13,222 - root - INFO - KG Training: Epoch 0001 Iter 3243 / 4823 | Time 0.0s | Iter Loss 0.1366 | Iter Mean Loss 0.2558\n",
      "2025-11-10 13:49:13,538 - root - INFO - KG Training: Epoch 0001 Iter 3246 / 4823 | Time 0.0s | Iter Loss 0.1498 | Iter Mean Loss 0.2557\n",
      "2025-11-10 13:49:13,666 - root - INFO - KG Training: Epoch 0001 Iter 3249 / 4823 | Time 0.0s | Iter Loss 0.1605 | Iter Mean Loss 0.2556\n",
      "2025-11-10 13:49:13,789 - root - INFO - KG Training: Epoch 0001 Iter 3252 / 4823 | Time 0.0s | Iter Loss 0.1578 | Iter Mean Loss 0.2555\n",
      "2025-11-10 13:49:13,914 - root - INFO - KG Training: Epoch 0001 Iter 3255 / 4823 | Time 0.0s | Iter Loss 0.1424 | Iter Mean Loss 0.2554\n",
      "2025-11-10 13:49:14,035 - root - INFO - KG Training: Epoch 0001 Iter 3258 / 4823 | Time 0.0s | Iter Loss 0.1526 | Iter Mean Loss 0.2553\n",
      "2025-11-10 13:49:14,285 - root - INFO - KG Training: Epoch 0001 Iter 3261 / 4823 | Time 0.0s | Iter Loss 0.1604 | Iter Mean Loss 0.2552\n",
      "2025-11-10 13:49:14,412 - root - INFO - KG Training: Epoch 0001 Iter 3264 / 4823 | Time 0.0s | Iter Loss 0.1482 | Iter Mean Loss 0.2551\n",
      "2025-11-10 13:49:14,534 - root - INFO - KG Training: Epoch 0001 Iter 3267 / 4823 | Time 0.0s | Iter Loss 0.1435 | Iter Mean Loss 0.2550\n",
      "2025-11-10 13:49:14,660 - root - INFO - KG Training: Epoch 0001 Iter 3270 / 4823 | Time 0.0s | Iter Loss 0.1376 | Iter Mean Loss 0.2549\n",
      "2025-11-10 13:49:14,785 - root - INFO - KG Training: Epoch 0001 Iter 3273 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2548\n",
      "2025-11-10 13:49:14,910 - root - INFO - KG Training: Epoch 0001 Iter 3276 / 4823 | Time 0.0s | Iter Loss 0.1398 | Iter Mean Loss 0.2547\n",
      "2025-11-10 13:49:15,036 - root - INFO - KG Training: Epoch 0001 Iter 3279 / 4823 | Time 0.0s | Iter Loss 0.1464 | Iter Mean Loss 0.2546\n",
      "2025-11-10 13:49:15,165 - root - INFO - KG Training: Epoch 0001 Iter 3282 / 4823 | Time 0.0s | Iter Loss 0.1528 | Iter Mean Loss 0.2545\n",
      "2025-11-10 13:49:15,288 - root - INFO - KG Training: Epoch 0001 Iter 3285 / 4823 | Time 0.0s | Iter Loss 0.1378 | Iter Mean Loss 0.2544\n",
      "2025-11-10 13:49:15,417 - root - INFO - KG Training: Epoch 0001 Iter 3288 / 4823 | Time 0.0s | Iter Loss 0.1475 | Iter Mean Loss 0.2543\n",
      "2025-11-10 13:49:15,672 - root - INFO - KG Training: Epoch 0001 Iter 3291 / 4823 | Time 0.0s | Iter Loss 0.1488 | Iter Mean Loss 0.2542\n",
      "2025-11-10 13:49:15,908 - root - INFO - KG Training: Epoch 0001 Iter 3294 / 4823 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.2541\n",
      "2025-11-10 13:49:16,253 - root - INFO - KG Training: Epoch 0001 Iter 3297 / 4823 | Time 0.3s | Iter Loss 0.1417 | Iter Mean Loss 0.2540\n",
      "2025-11-10 13:49:16,380 - root - INFO - KG Training: Epoch 0001 Iter 3300 / 4823 | Time 0.0s | Iter Loss 0.1436 | Iter Mean Loss 0.2539\n",
      "2025-11-10 13:49:16,507 - root - INFO - KG Training: Epoch 0001 Iter 3303 / 4823 | Time 0.0s | Iter Loss 0.1402 | Iter Mean Loss 0.2538\n",
      "2025-11-10 13:49:16,634 - root - INFO - KG Training: Epoch 0001 Iter 3306 / 4823 | Time 0.0s | Iter Loss 0.1476 | Iter Mean Loss 0.2538\n",
      "2025-11-10 13:49:16,757 - root - INFO - KG Training: Epoch 0001 Iter 3309 / 4823 | Time 0.0s | Iter Loss 0.1540 | Iter Mean Loss 0.2537\n",
      "2025-11-10 13:49:16,882 - root - INFO - KG Training: Epoch 0001 Iter 3312 / 4823 | Time 0.0s | Iter Loss 0.1655 | Iter Mean Loss 0.2536\n",
      "2025-11-10 13:49:17,285 - root - INFO - KG Training: Epoch 0001 Iter 3315 / 4823 | Time 0.3s | Iter Loss 0.1586 | Iter Mean Loss 0.2535\n",
      "2025-11-10 13:49:17,416 - root - INFO - KG Training: Epoch 0001 Iter 3318 / 4823 | Time 0.0s | Iter Loss 0.1408 | Iter Mean Loss 0.2534\n",
      "2025-11-10 13:49:17,545 - root - INFO - KG Training: Epoch 0001 Iter 3321 / 4823 | Time 0.0s | Iter Loss 0.1520 | Iter Mean Loss 0.2533\n",
      "2025-11-10 13:49:17,667 - root - INFO - KG Training: Epoch 0001 Iter 3324 / 4823 | Time 0.0s | Iter Loss 0.1443 | Iter Mean Loss 0.2532\n",
      "2025-11-10 13:49:17,790 - root - INFO - KG Training: Epoch 0001 Iter 3327 / 4823 | Time 0.0s | Iter Loss 0.1387 | Iter Mean Loss 0.2531\n",
      "2025-11-10 13:49:17,914 - root - INFO - KG Training: Epoch 0001 Iter 3330 / 4823 | Time 0.0s | Iter Loss 0.1588 | Iter Mean Loss 0.2530\n",
      "2025-11-10 13:49:18,405 - root - INFO - KG Training: Epoch 0001 Iter 3333 / 4823 | Time 0.4s | Iter Loss 0.1587 | Iter Mean Loss 0.2529\n",
      "2025-11-10 13:49:18,532 - root - INFO - KG Training: Epoch 0001 Iter 3336 / 4823 | Time 0.0s | Iter Loss 0.1524 | Iter Mean Loss 0.2528\n",
      "2025-11-10 13:49:18,661 - root - INFO - KG Training: Epoch 0001 Iter 3339 / 4823 | Time 0.0s | Iter Loss 0.1568 | Iter Mean Loss 0.2527\n",
      "2025-11-10 13:49:18,788 - root - INFO - KG Training: Epoch 0001 Iter 3342 / 4823 | Time 0.0s | Iter Loss 0.1511 | Iter Mean Loss 0.2526\n",
      "2025-11-10 13:49:18,912 - root - INFO - KG Training: Epoch 0001 Iter 3345 / 4823 | Time 0.0s | Iter Loss 0.1611 | Iter Mean Loss 0.2526\n",
      "2025-11-10 13:49:19,037 - root - INFO - KG Training: Epoch 0001 Iter 3348 / 4823 | Time 0.0s | Iter Loss 0.1407 | Iter Mean Loss 0.2525\n",
      "2025-11-10 13:49:19,160 - root - INFO - KG Training: Epoch 0001 Iter 3351 / 4823 | Time 0.0s | Iter Loss 0.1521 | Iter Mean Loss 0.2524\n",
      "2025-11-10 13:49:19,283 - root - INFO - KG Training: Epoch 0001 Iter 3354 / 4823 | Time 0.0s | Iter Loss 0.1444 | Iter Mean Loss 0.2523\n",
      "2025-11-10 13:49:19,482 - root - INFO - KG Training: Epoch 0001 Iter 3357 / 4823 | Time 0.1s | Iter Loss 0.1596 | Iter Mean Loss 0.2522\n",
      "2025-11-10 13:49:19,617 - root - INFO - KG Training: Epoch 0001 Iter 3360 / 4823 | Time 0.0s | Iter Loss 0.1446 | Iter Mean Loss 0.2521\n",
      "2025-11-10 13:49:19,847 - root - INFO - KG Training: Epoch 0001 Iter 3363 / 4823 | Time 0.0s | Iter Loss 0.1460 | Iter Mean Loss 0.2520\n",
      "2025-11-10 13:49:20,145 - root - INFO - KG Training: Epoch 0001 Iter 3366 / 4823 | Time 0.0s | Iter Loss 0.1459 | Iter Mean Loss 0.2519\n",
      "2025-11-10 13:49:20,292 - root - INFO - KG Training: Epoch 0001 Iter 3369 / 4823 | Time 0.1s | Iter Loss 0.1401 | Iter Mean Loss 0.2518\n",
      "2025-11-10 13:49:20,424 - root - INFO - KG Training: Epoch 0001 Iter 3372 / 4823 | Time 0.0s | Iter Loss 0.1446 | Iter Mean Loss 0.2517\n",
      "2025-11-10 13:49:20,548 - root - INFO - KG Training: Epoch 0001 Iter 3375 / 4823 | Time 0.0s | Iter Loss 0.1521 | Iter Mean Loss 0.2516\n",
      "2025-11-10 13:49:20,934 - root - INFO - KG Training: Epoch 0001 Iter 3378 / 4823 | Time 0.1s | Iter Loss 0.1395 | Iter Mean Loss 0.2515\n",
      "2025-11-10 13:49:21,066 - root - INFO - KG Training: Epoch 0001 Iter 3381 / 4823 | Time 0.0s | Iter Loss 0.1535 | Iter Mean Loss 0.2514\n",
      "2025-11-10 13:49:21,194 - root - INFO - KG Training: Epoch 0001 Iter 3384 / 4823 | Time 0.0s | Iter Loss 0.1414 | Iter Mean Loss 0.2513\n",
      "2025-11-10 13:49:21,322 - root - INFO - KG Training: Epoch 0001 Iter 3387 / 4823 | Time 0.0s | Iter Loss 0.1462 | Iter Mean Loss 0.2513\n",
      "2025-11-10 13:49:21,446 - root - INFO - KG Training: Epoch 0001 Iter 3390 / 4823 | Time 0.0s | Iter Loss 0.1459 | Iter Mean Loss 0.2512\n",
      "2025-11-10 13:49:21,576 - root - INFO - KG Training: Epoch 0001 Iter 3393 / 4823 | Time 0.0s | Iter Loss 0.1454 | Iter Mean Loss 0.2511\n",
      "2025-11-10 13:49:21,698 - root - INFO - KG Training: Epoch 0001 Iter 3396 / 4823 | Time 0.0s | Iter Loss 0.1444 | Iter Mean Loss 0.2510\n",
      "2025-11-10 13:49:21,821 - root - INFO - KG Training: Epoch 0001 Iter 3399 / 4823 | Time 0.0s | Iter Loss 0.1643 | Iter Mean Loss 0.2509\n",
      "2025-11-10 13:49:21,947 - root - INFO - KG Training: Epoch 0001 Iter 3402 / 4823 | Time 0.0s | Iter Loss 0.1592 | Iter Mean Loss 0.2508\n",
      "2025-11-10 13:49:22,403 - root - INFO - KG Training: Epoch 0001 Iter 3405 / 4823 | Time 0.4s | Iter Loss 0.1344 | Iter Mean Loss 0.2507\n",
      "2025-11-10 13:49:22,560 - root - INFO - KG Training: Epoch 0001 Iter 3408 / 4823 | Time 0.1s | Iter Loss 0.1334 | Iter Mean Loss 0.2506\n",
      "2025-11-10 13:49:22,694 - root - INFO - KG Training: Epoch 0001 Iter 3411 / 4823 | Time 0.0s | Iter Loss 0.1414 | Iter Mean Loss 0.2505\n",
      "2025-11-10 13:49:22,816 - root - INFO - KG Training: Epoch 0001 Iter 3414 / 4823 | Time 0.0s | Iter Loss 0.1350 | Iter Mean Loss 0.2504\n",
      "2025-11-10 13:49:22,943 - root - INFO - KG Training: Epoch 0001 Iter 3417 / 4823 | Time 0.0s | Iter Loss 0.1647 | Iter Mean Loss 0.2504\n",
      "2025-11-10 13:49:23,065 - root - INFO - KG Training: Epoch 0001 Iter 3420 / 4823 | Time 0.0s | Iter Loss 0.1472 | Iter Mean Loss 0.2503\n",
      "2025-11-10 13:49:23,196 - root - INFO - KG Training: Epoch 0001 Iter 3423 / 4823 | Time 0.0s | Iter Loss 0.1477 | Iter Mean Loss 0.2502\n",
      "2025-11-10 13:49:23,321 - root - INFO - KG Training: Epoch 0001 Iter 3426 / 4823 | Time 0.0s | Iter Loss 0.1351 | Iter Mean Loss 0.2501\n",
      "2025-11-10 13:49:23,446 - root - INFO - KG Training: Epoch 0001 Iter 3429 / 4823 | Time 0.0s | Iter Loss 0.1410 | Iter Mean Loss 0.2500\n",
      "2025-11-10 13:49:23,576 - root - INFO - KG Training: Epoch 0001 Iter 3432 / 4823 | Time 0.0s | Iter Loss 0.1472 | Iter Mean Loss 0.2499\n",
      "2025-11-10 13:49:23,700 - root - INFO - KG Training: Epoch 0001 Iter 3435 / 4823 | Time 0.0s | Iter Loss 0.1445 | Iter Mean Loss 0.2498\n",
      "2025-11-10 13:49:23,822 - root - INFO - KG Training: Epoch 0001 Iter 3438 / 4823 | Time 0.0s | Iter Loss 0.1484 | Iter Mean Loss 0.2497\n",
      "2025-11-10 13:49:23,953 - root - INFO - KG Training: Epoch 0001 Iter 3441 / 4823 | Time 0.0s | Iter Loss 0.1445 | Iter Mean Loss 0.2496\n",
      "2025-11-10 13:49:24,235 - root - INFO - KG Training: Epoch 0001 Iter 3444 / 4823 | Time 0.0s | Iter Loss 0.1317 | Iter Mean Loss 0.2495\n",
      "2025-11-10 13:49:24,617 - root - INFO - KG Training: Epoch 0001 Iter 3447 / 4823 | Time 0.0s | Iter Loss 0.1452 | Iter Mean Loss 0.2494\n",
      "2025-11-10 13:49:24,750 - root - INFO - KG Training: Epoch 0001 Iter 3450 / 4823 | Time 0.0s | Iter Loss 0.1378 | Iter Mean Loss 0.2493\n",
      "2025-11-10 13:49:25,026 - root - INFO - KG Training: Epoch 0001 Iter 3453 / 4823 | Time 0.0s | Iter Loss 0.1490 | Iter Mean Loss 0.2493\n",
      "2025-11-10 13:49:25,164 - root - INFO - KG Training: Epoch 0001 Iter 3456 / 4823 | Time 0.0s | Iter Loss 0.1464 | Iter Mean Loss 0.2492\n",
      "2025-11-10 13:49:25,353 - root - INFO - KG Training: Epoch 0001 Iter 3459 / 4823 | Time 0.0s | Iter Loss 0.1470 | Iter Mean Loss 0.2491\n",
      "2025-11-10 13:49:25,634 - root - INFO - KG Training: Epoch 0001 Iter 3462 / 4823 | Time 0.0s | Iter Loss 0.1470 | Iter Mean Loss 0.2490\n",
      "2025-11-10 13:49:25,782 - root - INFO - KG Training: Epoch 0001 Iter 3465 / 4823 | Time 0.1s | Iter Loss 0.1464 | Iter Mean Loss 0.2489\n",
      "2025-11-10 13:49:25,975 - root - INFO - KG Training: Epoch 0001 Iter 3468 / 4823 | Time 0.0s | Iter Loss 0.1435 | Iter Mean Loss 0.2488\n",
      "2025-11-10 13:49:26,102 - root - INFO - KG Training: Epoch 0001 Iter 3471 / 4823 | Time 0.0s | Iter Loss 0.1431 | Iter Mean Loss 0.2487\n",
      "2025-11-10 13:49:26,225 - root - INFO - KG Training: Epoch 0001 Iter 3474 / 4823 | Time 0.0s | Iter Loss 0.1477 | Iter Mean Loss 0.2486\n",
      "2025-11-10 13:49:26,433 - root - INFO - KG Training: Epoch 0001 Iter 3477 / 4823 | Time 0.1s | Iter Loss 0.1481 | Iter Mean Loss 0.2485\n",
      "2025-11-10 13:49:26,566 - root - INFO - KG Training: Epoch 0001 Iter 3480 / 4823 | Time 0.0s | Iter Loss 0.1517 | Iter Mean Loss 0.2484\n",
      "2025-11-10 13:49:26,826 - root - INFO - KG Training: Epoch 0001 Iter 3483 / 4823 | Time 0.0s | Iter Loss 0.1430 | Iter Mean Loss 0.2484\n",
      "2025-11-10 13:49:27,062 - root - INFO - KG Training: Epoch 0001 Iter 3486 / 4823 | Time 0.0s | Iter Loss 0.1430 | Iter Mean Loss 0.2483\n",
      "2025-11-10 13:49:27,195 - root - INFO - KG Training: Epoch 0001 Iter 3489 / 4823 | Time 0.0s | Iter Loss 0.1454 | Iter Mean Loss 0.2482\n",
      "2025-11-10 13:49:27,322 - root - INFO - KG Training: Epoch 0001 Iter 3492 / 4823 | Time 0.0s | Iter Loss 0.1494 | Iter Mean Loss 0.2481\n",
      "2025-11-10 13:49:27,448 - root - INFO - KG Training: Epoch 0001 Iter 3495 / 4823 | Time 0.0s | Iter Loss 0.1456 | Iter Mean Loss 0.2480\n",
      "2025-11-10 13:49:27,571 - root - INFO - KG Training: Epoch 0001 Iter 3498 / 4823 | Time 0.0s | Iter Loss 0.1530 | Iter Mean Loss 0.2479\n",
      "2025-11-10 13:49:27,697 - root - INFO - KG Training: Epoch 0001 Iter 3501 / 4823 | Time 0.0s | Iter Loss 0.1512 | Iter Mean Loss 0.2478\n",
      "2025-11-10 13:49:27,913 - root - INFO - KG Training: Epoch 0001 Iter 3504 / 4823 | Time 0.0s | Iter Loss 0.1417 | Iter Mean Loss 0.2477\n",
      "2025-11-10 13:49:28,044 - root - INFO - KG Training: Epoch 0001 Iter 3507 / 4823 | Time 0.0s | Iter Loss 0.1499 | Iter Mean Loss 0.2477\n",
      "2025-11-10 13:49:28,170 - root - INFO - KG Training: Epoch 0001 Iter 3510 / 4823 | Time 0.0s | Iter Loss 0.1566 | Iter Mean Loss 0.2476\n",
      "2025-11-10 13:49:28,292 - root - INFO - KG Training: Epoch 0001 Iter 3513 / 4823 | Time 0.0s | Iter Loss 0.1426 | Iter Mean Loss 0.2475\n",
      "2025-11-10 13:49:28,417 - root - INFO - KG Training: Epoch 0001 Iter 3516 / 4823 | Time 0.0s | Iter Loss 0.1345 | Iter Mean Loss 0.2474\n",
      "2025-11-10 13:49:28,545 - root - INFO - KG Training: Epoch 0001 Iter 3519 / 4823 | Time 0.0s | Iter Loss 0.1436 | Iter Mean Loss 0.2473\n",
      "2025-11-10 13:49:28,667 - root - INFO - KG Training: Epoch 0001 Iter 3522 / 4823 | Time 0.0s | Iter Loss 0.1427 | Iter Mean Loss 0.2472\n",
      "2025-11-10 13:49:28,789 - root - INFO - KG Training: Epoch 0001 Iter 3525 / 4823 | Time 0.0s | Iter Loss 0.1454 | Iter Mean Loss 0.2471\n",
      "2025-11-10 13:49:29,003 - root - INFO - KG Training: Epoch 0001 Iter 3528 / 4823 | Time 0.0s | Iter Loss 0.1491 | Iter Mean Loss 0.2470\n",
      "2025-11-10 13:49:29,315 - root - INFO - KG Training: Epoch 0001 Iter 3531 / 4823 | Time 0.2s | Iter Loss 0.1398 | Iter Mean Loss 0.2469\n",
      "2025-11-10 13:49:29,448 - root - INFO - KG Training: Epoch 0001 Iter 3534 / 4823 | Time 0.0s | Iter Loss 0.1410 | Iter Mean Loss 0.2469\n",
      "2025-11-10 13:49:29,576 - root - INFO - KG Training: Epoch 0001 Iter 3537 / 4823 | Time 0.0s | Iter Loss 0.1361 | Iter Mean Loss 0.2468\n",
      "2025-11-10 13:49:29,703 - root - INFO - KG Training: Epoch 0001 Iter 3540 / 4823 | Time 0.0s | Iter Loss 0.1561 | Iter Mean Loss 0.2467\n",
      "2025-11-10 13:49:29,826 - root - INFO - KG Training: Epoch 0001 Iter 3543 / 4823 | Time 0.0s | Iter Loss 0.1318 | Iter Mean Loss 0.2466\n",
      "2025-11-10 13:49:29,949 - root - INFO - KG Training: Epoch 0001 Iter 3546 / 4823 | Time 0.0s | Iter Loss 0.1487 | Iter Mean Loss 0.2465\n",
      "2025-11-10 13:49:30,072 - root - INFO - KG Training: Epoch 0001 Iter 3549 / 4823 | Time 0.0s | Iter Loss 0.1443 | Iter Mean Loss 0.2464\n",
      "2025-11-10 13:49:30,287 - root - INFO - KG Training: Epoch 0001 Iter 3552 / 4823 | Time 0.0s | Iter Loss 0.1550 | Iter Mean Loss 0.2463\n",
      "2025-11-10 13:49:30,409 - root - INFO - KG Training: Epoch 0001 Iter 3555 / 4823 | Time 0.0s | Iter Loss 0.1313 | Iter Mean Loss 0.2462\n",
      "2025-11-10 13:49:30,531 - root - INFO - KG Training: Epoch 0001 Iter 3558 / 4823 | Time 0.0s | Iter Loss 0.1435 | Iter Mean Loss 0.2462\n",
      "2025-11-10 13:49:30,652 - root - INFO - KG Training: Epoch 0001 Iter 3561 / 4823 | Time 0.0s | Iter Loss 0.1377 | Iter Mean Loss 0.2461\n",
      "2025-11-10 13:49:30,881 - root - INFO - KG Training: Epoch 0001 Iter 3564 / 4823 | Time 0.1s | Iter Loss 0.1599 | Iter Mean Loss 0.2460\n",
      "2025-11-10 13:49:31,011 - root - INFO - KG Training: Epoch 0001 Iter 3567 / 4823 | Time 0.0s | Iter Loss 0.1399 | Iter Mean Loss 0.2459\n",
      "2025-11-10 13:49:31,134 - root - INFO - KG Training: Epoch 0001 Iter 3570 / 4823 | Time 0.0s | Iter Loss 0.1371 | Iter Mean Loss 0.2458\n",
      "2025-11-10 13:49:31,259 - root - INFO - KG Training: Epoch 0001 Iter 3573 / 4823 | Time 0.0s | Iter Loss 0.1382 | Iter Mean Loss 0.2457\n",
      "2025-11-10 13:49:31,380 - root - INFO - KG Training: Epoch 0001 Iter 3576 / 4823 | Time 0.0s | Iter Loss 0.1349 | Iter Mean Loss 0.2456\n",
      "2025-11-10 13:49:31,509 - root - INFO - KG Training: Epoch 0001 Iter 3579 / 4823 | Time 0.0s | Iter Loss 0.1539 | Iter Mean Loss 0.2456\n",
      "2025-11-10 13:49:31,632 - root - INFO - KG Training: Epoch 0001 Iter 3582 / 4823 | Time 0.0s | Iter Loss 0.1624 | Iter Mean Loss 0.2455\n",
      "2025-11-10 13:49:31,756 - root - INFO - KG Training: Epoch 0001 Iter 3585 / 4823 | Time 0.0s | Iter Loss 0.1444 | Iter Mean Loss 0.2454\n",
      "2025-11-10 13:49:31,881 - root - INFO - KG Training: Epoch 0001 Iter 3588 / 4823 | Time 0.0s | Iter Loss 0.1440 | Iter Mean Loss 0.2453\n",
      "2025-11-10 13:49:32,006 - root - INFO - KG Training: Epoch 0001 Iter 3591 / 4823 | Time 0.0s | Iter Loss 0.1353 | Iter Mean Loss 0.2452\n",
      "2025-11-10 13:49:32,132 - root - INFO - KG Training: Epoch 0001 Iter 3594 / 4823 | Time 0.0s | Iter Loss 0.1313 | Iter Mean Loss 0.2451\n",
      "2025-11-10 13:49:32,257 - root - INFO - KG Training: Epoch 0001 Iter 3597 / 4823 | Time 0.0s | Iter Loss 0.1480 | Iter Mean Loss 0.2450\n",
      "2025-11-10 13:49:32,419 - root - INFO - KG Training: Epoch 0001 Iter 3600 / 4823 | Time 0.0s | Iter Loss 0.1396 | Iter Mean Loss 0.2449\n",
      "2025-11-10 13:49:32,547 - root - INFO - KG Training: Epoch 0001 Iter 3603 / 4823 | Time 0.0s | Iter Loss 0.1332 | Iter Mean Loss 0.2449\n",
      "2025-11-10 13:49:32,672 - root - INFO - KG Training: Epoch 0001 Iter 3606 / 4823 | Time 0.0s | Iter Loss 0.1411 | Iter Mean Loss 0.2448\n",
      "2025-11-10 13:49:32,796 - root - INFO - KG Training: Epoch 0001 Iter 3609 / 4823 | Time 0.0s | Iter Loss 0.1483 | Iter Mean Loss 0.2447\n",
      "2025-11-10 13:49:32,921 - root - INFO - KG Training: Epoch 0001 Iter 3612 / 4823 | Time 0.0s | Iter Loss 0.1446 | Iter Mean Loss 0.2446\n",
      "2025-11-10 13:49:33,046 - root - INFO - KG Training: Epoch 0001 Iter 3615 / 4823 | Time 0.0s | Iter Loss 0.1489 | Iter Mean Loss 0.2445\n",
      "2025-11-10 13:49:33,169 - root - INFO - KG Training: Epoch 0001 Iter 3618 / 4823 | Time 0.0s | Iter Loss 0.1389 | Iter Mean Loss 0.2444\n",
      "2025-11-10 13:49:33,300 - root - INFO - KG Training: Epoch 0001 Iter 3621 / 4823 | Time 0.0s | Iter Loss 0.1457 | Iter Mean Loss 0.2443\n",
      "2025-11-10 13:49:33,425 - root - INFO - KG Training: Epoch 0001 Iter 3624 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2443\n",
      "2025-11-10 13:49:33,548 - root - INFO - KG Training: Epoch 0001 Iter 3627 / 4823 | Time 0.0s | Iter Loss 0.1457 | Iter Mean Loss 0.2442\n",
      "2025-11-10 13:49:33,670 - root - INFO - KG Training: Epoch 0001 Iter 3630 / 4823 | Time 0.0s | Iter Loss 0.1335 | Iter Mean Loss 0.2441\n",
      "2025-11-10 13:49:33,815 - root - INFO - KG Training: Epoch 0001 Iter 3633 / 4823 | Time 0.0s | Iter Loss 0.1462 | Iter Mean Loss 0.2440\n",
      "2025-11-10 13:49:33,937 - root - INFO - KG Training: Epoch 0001 Iter 3636 / 4823 | Time 0.0s | Iter Loss 0.1489 | Iter Mean Loss 0.2439\n",
      "2025-11-10 13:49:34,060 - root - INFO - KG Training: Epoch 0001 Iter 3639 / 4823 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.2439\n",
      "2025-11-10 13:49:34,187 - root - INFO - KG Training: Epoch 0001 Iter 3642 / 4823 | Time 0.0s | Iter Loss 0.1381 | Iter Mean Loss 0.2438\n",
      "2025-11-10 13:49:34,315 - root - INFO - KG Training: Epoch 0001 Iter 3645 / 4823 | Time 0.0s | Iter Loss 0.1409 | Iter Mean Loss 0.2437\n",
      "2025-11-10 13:49:34,437 - root - INFO - KG Training: Epoch 0001 Iter 3648 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2436\n",
      "2025-11-10 13:49:34,565 - root - INFO - KG Training: Epoch 0001 Iter 3651 / 4823 | Time 0.0s | Iter Loss 0.1392 | Iter Mean Loss 0.2435\n",
      "2025-11-10 13:49:34,693 - root - INFO - KG Training: Epoch 0001 Iter 3654 / 4823 | Time 0.0s | Iter Loss 0.1557 | Iter Mean Loss 0.2434\n",
      "2025-11-10 13:49:34,885 - root - INFO - KG Training: Epoch 0001 Iter 3657 / 4823 | Time 0.0s | Iter Loss 0.1490 | Iter Mean Loss 0.2434\n",
      "2025-11-10 13:49:35,009 - root - INFO - KG Training: Epoch 0001 Iter 3660 / 4823 | Time 0.0s | Iter Loss 0.1303 | Iter Mean Loss 0.2433\n",
      "2025-11-10 13:49:35,133 - root - INFO - KG Training: Epoch 0001 Iter 3663 / 4823 | Time 0.0s | Iter Loss 0.1310 | Iter Mean Loss 0.2432\n",
      "2025-11-10 13:49:35,256 - root - INFO - KG Training: Epoch 0001 Iter 3666 / 4823 | Time 0.0s | Iter Loss 0.1363 | Iter Mean Loss 0.2431\n",
      "2025-11-10 13:49:35,378 - root - INFO - KG Training: Epoch 0001 Iter 3669 / 4823 | Time 0.0s | Iter Loss 0.1401 | Iter Mean Loss 0.2430\n",
      "2025-11-10 13:49:35,501 - root - INFO - KG Training: Epoch 0001 Iter 3672 / 4823 | Time 0.0s | Iter Loss 0.1399 | Iter Mean Loss 0.2429\n",
      "2025-11-10 13:49:35,625 - root - INFO - KG Training: Epoch 0001 Iter 3675 / 4823 | Time 0.0s | Iter Loss 0.1437 | Iter Mean Loss 0.2429\n",
      "2025-11-10 13:49:35,748 - root - INFO - KG Training: Epoch 0001 Iter 3678 / 4823 | Time 0.0s | Iter Loss 0.1343 | Iter Mean Loss 0.2428\n",
      "2025-11-10 13:49:35,873 - root - INFO - KG Training: Epoch 0001 Iter 3681 / 4823 | Time 0.0s | Iter Loss 0.1503 | Iter Mean Loss 0.2427\n",
      "2025-11-10 13:49:35,997 - root - INFO - KG Training: Epoch 0001 Iter 3684 / 4823 | Time 0.0s | Iter Loss 0.1425 | Iter Mean Loss 0.2426\n",
      "2025-11-10 13:49:36,122 - root - INFO - KG Training: Epoch 0001 Iter 3687 / 4823 | Time 0.0s | Iter Loss 0.1318 | Iter Mean Loss 0.2425\n",
      "2025-11-10 13:49:36,317 - root - INFO - KG Training: Epoch 0001 Iter 3690 / 4823 | Time 0.0s | Iter Loss 0.1515 | Iter Mean Loss 0.2424\n",
      "2025-11-10 13:49:36,445 - root - INFO - KG Training: Epoch 0001 Iter 3693 / 4823 | Time 0.0s | Iter Loss 0.1315 | Iter Mean Loss 0.2424\n",
      "2025-11-10 13:49:36,570 - root - INFO - KG Training: Epoch 0001 Iter 3696 / 4823 | Time 0.0s | Iter Loss 0.1334 | Iter Mean Loss 0.2423\n",
      "2025-11-10 13:49:36,693 - root - INFO - KG Training: Epoch 0001 Iter 3699 / 4823 | Time 0.0s | Iter Loss 0.1427 | Iter Mean Loss 0.2422\n",
      "2025-11-10 13:49:36,817 - root - INFO - KG Training: Epoch 0001 Iter 3702 / 4823 | Time 0.0s | Iter Loss 0.1405 | Iter Mean Loss 0.2421\n",
      "2025-11-10 13:49:36,941 - root - INFO - KG Training: Epoch 0001 Iter 3705 / 4823 | Time 0.0s | Iter Loss 0.1336 | Iter Mean Loss 0.2420\n",
      "2025-11-10 13:49:37,064 - root - INFO - KG Training: Epoch 0001 Iter 3708 / 4823 | Time 0.0s | Iter Loss 0.1518 | Iter Mean Loss 0.2420\n",
      "2025-11-10 13:49:37,187 - root - INFO - KG Training: Epoch 0001 Iter 3711 / 4823 | Time 0.0s | Iter Loss 0.1486 | Iter Mean Loss 0.2419\n",
      "2025-11-10 13:49:37,310 - root - INFO - KG Training: Epoch 0001 Iter 3714 / 4823 | Time 0.0s | Iter Loss 0.1289 | Iter Mean Loss 0.2418\n",
      "2025-11-10 13:49:37,553 - root - INFO - KG Training: Epoch 0001 Iter 3717 / 4823 | Time 0.0s | Iter Loss 0.1355 | Iter Mean Loss 0.2417\n",
      "2025-11-10 13:49:37,676 - root - INFO - KG Training: Epoch 0001 Iter 3720 / 4823 | Time 0.0s | Iter Loss 0.1453 | Iter Mean Loss 0.2416\n",
      "2025-11-10 13:49:37,803 - root - INFO - KG Training: Epoch 0001 Iter 3723 / 4823 | Time 0.0s | Iter Loss 0.1458 | Iter Mean Loss 0.2415\n",
      "2025-11-10 13:49:37,926 - root - INFO - KG Training: Epoch 0001 Iter 3726 / 4823 | Time 0.0s | Iter Loss 0.1405 | Iter Mean Loss 0.2415\n",
      "2025-11-10 13:49:38,048 - root - INFO - KG Training: Epoch 0001 Iter 3729 / 4823 | Time 0.0s | Iter Loss 0.1394 | Iter Mean Loss 0.2414\n",
      "2025-11-10 13:49:38,172 - root - INFO - KG Training: Epoch 0001 Iter 3732 / 4823 | Time 0.0s | Iter Loss 0.1462 | Iter Mean Loss 0.2413\n",
      "2025-11-10 13:49:38,297 - root - INFO - KG Training: Epoch 0001 Iter 3735 / 4823 | Time 0.0s | Iter Loss 0.1378 | Iter Mean Loss 0.2412\n",
      "2025-11-10 13:49:38,423 - root - INFO - KG Training: Epoch 0001 Iter 3738 / 4823 | Time 0.0s | Iter Loss 0.1487 | Iter Mean Loss 0.2411\n",
      "2025-11-10 13:49:38,545 - root - INFO - KG Training: Epoch 0001 Iter 3741 / 4823 | Time 0.0s | Iter Loss 0.1398 | Iter Mean Loss 0.2411\n",
      "2025-11-10 13:49:38,670 - root - INFO - KG Training: Epoch 0001 Iter 3744 / 4823 | Time 0.0s | Iter Loss 0.1433 | Iter Mean Loss 0.2410\n",
      "2025-11-10 13:49:38,799 - root - INFO - KG Training: Epoch 0001 Iter 3747 / 4823 | Time 0.0s | Iter Loss 0.1374 | Iter Mean Loss 0.2409\n",
      "2025-11-10 13:49:38,927 - root - INFO - KG Training: Epoch 0001 Iter 3750 / 4823 | Time 0.0s | Iter Loss 0.1532 | Iter Mean Loss 0.2408\n",
      "2025-11-10 13:49:39,050 - root - INFO - KG Training: Epoch 0001 Iter 3753 / 4823 | Time 0.0s | Iter Loss 0.1346 | Iter Mean Loss 0.2408\n",
      "2025-11-10 13:49:39,179 - root - INFO - KG Training: Epoch 0001 Iter 3756 / 4823 | Time 0.0s | Iter Loss 0.1296 | Iter Mean Loss 0.2407\n",
      "2025-11-10 13:49:39,307 - root - INFO - KG Training: Epoch 0001 Iter 3759 / 4823 | Time 0.0s | Iter Loss 0.1352 | Iter Mean Loss 0.2406\n",
      "2025-11-10 13:49:39,662 - root - INFO - KG Training: Epoch 0001 Iter 3762 / 4823 | Time 0.3s | Iter Loss 0.1346 | Iter Mean Loss 0.2405\n",
      "2025-11-10 13:49:39,978 - root - INFO - KG Training: Epoch 0001 Iter 3765 / 4823 | Time 0.2s | Iter Loss 0.1415 | Iter Mean Loss 0.2404\n",
      "2025-11-10 13:49:40,110 - root - INFO - KG Training: Epoch 0001 Iter 3768 / 4823 | Time 0.0s | Iter Loss 0.1516 | Iter Mean Loss 0.2403\n",
      "2025-11-10 13:49:40,233 - root - INFO - KG Training: Epoch 0001 Iter 3771 / 4823 | Time 0.0s | Iter Loss 0.1358 | Iter Mean Loss 0.2403\n",
      "2025-11-10 13:49:40,357 - root - INFO - KG Training: Epoch 0001 Iter 3774 / 4823 | Time 0.0s | Iter Loss 0.1520 | Iter Mean Loss 0.2402\n",
      "2025-11-10 13:49:40,618 - root - INFO - KG Training: Epoch 0001 Iter 3777 / 4823 | Time 0.0s | Iter Loss 0.1469 | Iter Mean Loss 0.2401\n",
      "2025-11-10 13:49:40,743 - root - INFO - KG Training: Epoch 0001 Iter 3780 / 4823 | Time 0.0s | Iter Loss 0.1291 | Iter Mean Loss 0.2400\n",
      "2025-11-10 13:49:40,969 - root - INFO - KG Training: Epoch 0001 Iter 3783 / 4823 | Time 0.0s | Iter Loss 0.1439 | Iter Mean Loss 0.2400\n",
      "2025-11-10 13:49:41,094 - root - INFO - KG Training: Epoch 0001 Iter 3786 / 4823 | Time 0.0s | Iter Loss 0.1395 | Iter Mean Loss 0.2399\n",
      "2025-11-10 13:49:41,215 - root - INFO - KG Training: Epoch 0001 Iter 3789 / 4823 | Time 0.0s | Iter Loss 0.1347 | Iter Mean Loss 0.2398\n",
      "2025-11-10 13:49:41,340 - root - INFO - KG Training: Epoch 0001 Iter 3792 / 4823 | Time 0.0s | Iter Loss 0.1202 | Iter Mean Loss 0.2397\n",
      "2025-11-10 13:49:41,571 - root - INFO - KG Training: Epoch 0001 Iter 3795 / 4823 | Time 0.0s | Iter Loss 0.1332 | Iter Mean Loss 0.2396\n",
      "2025-11-10 13:49:41,692 - root - INFO - KG Training: Epoch 0001 Iter 3798 / 4823 | Time 0.0s | Iter Loss 0.1354 | Iter Mean Loss 0.2395\n",
      "2025-11-10 13:49:41,815 - root - INFO - KG Training: Epoch 0001 Iter 3801 / 4823 | Time 0.0s | Iter Loss 0.1392 | Iter Mean Loss 0.2395\n",
      "2025-11-10 13:49:41,939 - root - INFO - KG Training: Epoch 0001 Iter 3804 / 4823 | Time 0.0s | Iter Loss 0.1489 | Iter Mean Loss 0.2394\n",
      "2025-11-10 13:49:42,087 - root - INFO - KG Training: Epoch 0001 Iter 3807 / 4823 | Time 0.0s | Iter Loss 0.1436 | Iter Mean Loss 0.2393\n",
      "2025-11-10 13:49:42,280 - root - INFO - KG Training: Epoch 0001 Iter 3810 / 4823 | Time 0.1s | Iter Loss 0.1458 | Iter Mean Loss 0.2392\n",
      "2025-11-10 13:49:42,404 - root - INFO - KG Training: Epoch 0001 Iter 3813 / 4823 | Time 0.0s | Iter Loss 0.1390 | Iter Mean Loss 0.2392\n",
      "2025-11-10 13:49:42,526 - root - INFO - KG Training: Epoch 0001 Iter 3816 / 4823 | Time 0.0s | Iter Loss 0.1497 | Iter Mean Loss 0.2391\n",
      "2025-11-10 13:49:42,657 - root - INFO - KG Training: Epoch 0001 Iter 3819 / 4823 | Time 0.0s | Iter Loss 0.1430 | Iter Mean Loss 0.2390\n",
      "2025-11-10 13:49:42,787 - root - INFO - KG Training: Epoch 0001 Iter 3822 / 4823 | Time 0.0s | Iter Loss 0.1302 | Iter Mean Loss 0.2389\n",
      "2025-11-10 13:49:42,909 - root - INFO - KG Training: Epoch 0001 Iter 3825 / 4823 | Time 0.0s | Iter Loss 0.1300 | Iter Mean Loss 0.2388\n",
      "2025-11-10 13:49:43,035 - root - INFO - KG Training: Epoch 0001 Iter 3828 / 4823 | Time 0.0s | Iter Loss 0.1389 | Iter Mean Loss 0.2388\n",
      "2025-11-10 13:49:43,224 - root - INFO - KG Training: Epoch 0001 Iter 3831 / 4823 | Time 0.1s | Iter Loss 0.1490 | Iter Mean Loss 0.2387\n",
      "2025-11-10 13:49:43,349 - root - INFO - KG Training: Epoch 0001 Iter 3834 / 4823 | Time 0.0s | Iter Loss 0.1499 | Iter Mean Loss 0.2386\n",
      "2025-11-10 13:49:43,613 - root - INFO - KG Training: Epoch 0001 Iter 3837 / 4823 | Time 0.0s | Iter Loss 0.1491 | Iter Mean Loss 0.2386\n",
      "2025-11-10 13:49:43,740 - root - INFO - KG Training: Epoch 0001 Iter 3840 / 4823 | Time 0.0s | Iter Loss 0.1366 | Iter Mean Loss 0.2385\n",
      "2025-11-10 13:49:43,862 - root - INFO - KG Training: Epoch 0001 Iter 3843 / 4823 | Time 0.0s | Iter Loss 0.1358 | Iter Mean Loss 0.2384\n",
      "2025-11-10 13:49:43,986 - root - INFO - KG Training: Epoch 0001 Iter 3846 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2383\n",
      "2025-11-10 13:49:44,109 - root - INFO - KG Training: Epoch 0001 Iter 3849 / 4823 | Time 0.0s | Iter Loss 0.1456 | Iter Mean Loss 0.2382\n",
      "2025-11-10 13:49:44,240 - root - INFO - KG Training: Epoch 0001 Iter 3852 / 4823 | Time 0.0s | Iter Loss 0.1420 | Iter Mean Loss 0.2382\n",
      "2025-11-10 13:49:44,366 - root - INFO - KG Training: Epoch 0001 Iter 3855 / 4823 | Time 0.0s | Iter Loss 0.1366 | Iter Mean Loss 0.2381\n",
      "2025-11-10 13:49:44,496 - root - INFO - KG Training: Epoch 0001 Iter 3858 / 4823 | Time 0.0s | Iter Loss 0.1380 | Iter Mean Loss 0.2380\n",
      "2025-11-10 13:49:44,669 - root - INFO - KG Training: Epoch 0001 Iter 3861 / 4823 | Time 0.1s | Iter Loss 0.1323 | Iter Mean Loss 0.2379\n",
      "2025-11-10 13:49:44,801 - root - INFO - KG Training: Epoch 0001 Iter 3864 / 4823 | Time 0.0s | Iter Loss 0.1444 | Iter Mean Loss 0.2379\n",
      "2025-11-10 13:49:44,923 - root - INFO - KG Training: Epoch 0001 Iter 3867 / 4823 | Time 0.0s | Iter Loss 0.1524 | Iter Mean Loss 0.2378\n",
      "2025-11-10 13:49:45,045 - root - INFO - KG Training: Epoch 0001 Iter 3870 / 4823 | Time 0.0s | Iter Loss 0.1445 | Iter Mean Loss 0.2377\n",
      "2025-11-10 13:49:45,174 - root - INFO - KG Training: Epoch 0001 Iter 3873 / 4823 | Time 0.0s | Iter Loss 0.1227 | Iter Mean Loss 0.2376\n",
      "2025-11-10 13:49:45,365 - root - INFO - KG Training: Epoch 0001 Iter 3876 / 4823 | Time 0.0s | Iter Loss 0.1403 | Iter Mean Loss 0.2376\n",
      "2025-11-10 13:49:45,493 - root - INFO - KG Training: Epoch 0001 Iter 3879 / 4823 | Time 0.0s | Iter Loss 0.1262 | Iter Mean Loss 0.2375\n",
      "2025-11-10 13:49:45,617 - root - INFO - KG Training: Epoch 0001 Iter 3882 / 4823 | Time 0.0s | Iter Loss 0.1332 | Iter Mean Loss 0.2374\n",
      "2025-11-10 13:49:45,809 - root - INFO - KG Training: Epoch 0001 Iter 3885 / 4823 | Time 0.0s | Iter Loss 0.1441 | Iter Mean Loss 0.2373\n",
      "2025-11-10 13:49:45,933 - root - INFO - KG Training: Epoch 0001 Iter 3888 / 4823 | Time 0.0s | Iter Loss 0.1366 | Iter Mean Loss 0.2372\n",
      "2025-11-10 13:49:46,058 - root - INFO - KG Training: Epoch 0001 Iter 3891 / 4823 | Time 0.0s | Iter Loss 0.1365 | Iter Mean Loss 0.2372\n",
      "2025-11-10 13:49:46,182 - root - INFO - KG Training: Epoch 0001 Iter 3894 / 4823 | Time 0.0s | Iter Loss 0.1487 | Iter Mean Loss 0.2371\n",
      "2025-11-10 13:49:46,305 - root - INFO - KG Training: Epoch 0001 Iter 3897 / 4823 | Time 0.0s | Iter Loss 0.1412 | Iter Mean Loss 0.2370\n",
      "2025-11-10 13:49:46,434 - root - INFO - KG Training: Epoch 0001 Iter 3900 / 4823 | Time 0.0s | Iter Loss 0.1457 | Iter Mean Loss 0.2369\n",
      "2025-11-10 13:49:46,558 - root - INFO - KG Training: Epoch 0001 Iter 3903 / 4823 | Time 0.0s | Iter Loss 0.1214 | Iter Mean Loss 0.2369\n",
      "2025-11-10 13:49:46,684 - root - INFO - KG Training: Epoch 0001 Iter 3906 / 4823 | Time 0.0s | Iter Loss 0.1628 | Iter Mean Loss 0.2368\n",
      "2025-11-10 13:49:46,876 - root - INFO - KG Training: Epoch 0001 Iter 3909 / 4823 | Time 0.0s | Iter Loss 0.1423 | Iter Mean Loss 0.2367\n",
      "2025-11-10 13:49:46,999 - root - INFO - KG Training: Epoch 0001 Iter 3912 / 4823 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.2366\n",
      "2025-11-10 13:49:47,122 - root - INFO - KG Training: Epoch 0001 Iter 3915 / 4823 | Time 0.0s | Iter Loss 0.1410 | Iter Mean Loss 0.2366\n",
      "2025-11-10 13:49:47,245 - root - INFO - KG Training: Epoch 0001 Iter 3918 / 4823 | Time 0.0s | Iter Loss 0.1371 | Iter Mean Loss 0.2365\n",
      "2025-11-10 13:49:47,540 - root - INFO - KG Training: Epoch 0001 Iter 3921 / 4823 | Time 0.0s | Iter Loss 0.1380 | Iter Mean Loss 0.2364\n",
      "2025-11-10 13:49:47,864 - root - INFO - KG Training: Epoch 0001 Iter 3924 / 4823 | Time 0.0s | Iter Loss 0.1202 | Iter Mean Loss 0.2363\n",
      "2025-11-10 13:49:47,989 - root - INFO - KG Training: Epoch 0001 Iter 3927 / 4823 | Time 0.0s | Iter Loss 0.1389 | Iter Mean Loss 0.2363\n",
      "2025-11-10 13:49:48,114 - root - INFO - KG Training: Epoch 0001 Iter 3930 / 4823 | Time 0.0s | Iter Loss 0.1347 | Iter Mean Loss 0.2362\n",
      "2025-11-10 13:49:48,362 - root - INFO - KG Training: Epoch 0001 Iter 3933 / 4823 | Time 0.0s | Iter Loss 0.1451 | Iter Mean Loss 0.2361\n",
      "2025-11-10 13:49:48,488 - root - INFO - KG Training: Epoch 0001 Iter 3936 / 4823 | Time 0.0s | Iter Loss 0.1393 | Iter Mean Loss 0.2360\n",
      "2025-11-10 13:49:48,614 - root - INFO - KG Training: Epoch 0001 Iter 3939 / 4823 | Time 0.0s | Iter Loss 0.1498 | Iter Mean Loss 0.2360\n",
      "2025-11-10 13:49:48,741 - root - INFO - KG Training: Epoch 0001 Iter 3942 / 4823 | Time 0.0s | Iter Loss 0.1346 | Iter Mean Loss 0.2359\n",
      "2025-11-10 13:49:48,868 - root - INFO - KG Training: Epoch 0001 Iter 3945 / 4823 | Time 0.0s | Iter Loss 0.1318 | Iter Mean Loss 0.2358\n",
      "2025-11-10 13:49:48,991 - root - INFO - KG Training: Epoch 0001 Iter 3948 / 4823 | Time 0.0s | Iter Loss 0.1372 | Iter Mean Loss 0.2357\n",
      "2025-11-10 13:49:49,127 - root - INFO - KG Training: Epoch 0001 Iter 3951 / 4823 | Time 0.0s | Iter Loss 0.1390 | Iter Mean Loss 0.2357\n",
      "2025-11-10 13:49:49,250 - root - INFO - KG Training: Epoch 0001 Iter 3954 / 4823 | Time 0.0s | Iter Loss 0.1389 | Iter Mean Loss 0.2356\n",
      "2025-11-10 13:49:49,376 - root - INFO - KG Training: Epoch 0001 Iter 3957 / 4823 | Time 0.0s | Iter Loss 0.1320 | Iter Mean Loss 0.2355\n",
      "2025-11-10 13:49:49,502 - root - INFO - KG Training: Epoch 0001 Iter 3960 / 4823 | Time 0.0s | Iter Loss 0.1355 | Iter Mean Loss 0.2354\n",
      "2025-11-10 13:49:49,702 - root - INFO - KG Training: Epoch 0001 Iter 3963 / 4823 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.2354\n",
      "2025-11-10 13:49:49,829 - root - INFO - KG Training: Epoch 0001 Iter 3966 / 4823 | Time 0.0s | Iter Loss 0.1320 | Iter Mean Loss 0.2353\n",
      "2025-11-10 13:49:50,116 - root - INFO - KG Training: Epoch 0001 Iter 3969 / 4823 | Time 0.0s | Iter Loss 0.1445 | Iter Mean Loss 0.2352\n",
      "2025-11-10 13:49:50,245 - root - INFO - KG Training: Epoch 0001 Iter 3972 / 4823 | Time 0.0s | Iter Loss 0.1303 | Iter Mean Loss 0.2351\n",
      "2025-11-10 13:49:50,391 - root - INFO - KG Training: Epoch 0001 Iter 3975 / 4823 | Time 0.0s | Iter Loss 0.1305 | Iter Mean Loss 0.2351\n",
      "2025-11-10 13:49:50,516 - root - INFO - KG Training: Epoch 0001 Iter 3978 / 4823 | Time 0.0s | Iter Loss 0.1337 | Iter Mean Loss 0.2350\n",
      "2025-11-10 13:49:50,638 - root - INFO - KG Training: Epoch 0001 Iter 3981 / 4823 | Time 0.0s | Iter Loss 0.1231 | Iter Mean Loss 0.2349\n",
      "2025-11-10 13:49:50,765 - root - INFO - KG Training: Epoch 0001 Iter 3984 / 4823 | Time 0.0s | Iter Loss 0.1390 | Iter Mean Loss 0.2348\n",
      "2025-11-10 13:49:50,976 - root - INFO - KG Training: Epoch 0001 Iter 3987 / 4823 | Time 0.0s | Iter Loss 0.1368 | Iter Mean Loss 0.2348\n",
      "2025-11-10 13:49:51,100 - root - INFO - KG Training: Epoch 0001 Iter 3990 / 4823 | Time 0.0s | Iter Loss 0.1440 | Iter Mean Loss 0.2347\n",
      "2025-11-10 13:49:51,222 - root - INFO - KG Training: Epoch 0001 Iter 3993 / 4823 | Time 0.0s | Iter Loss 0.1358 | Iter Mean Loss 0.2346\n",
      "2025-11-10 13:49:51,347 - root - INFO - KG Training: Epoch 0001 Iter 3996 / 4823 | Time 0.0s | Iter Loss 0.1367 | Iter Mean Loss 0.2345\n",
      "2025-11-10 13:49:51,471 - root - INFO - KG Training: Epoch 0001 Iter 3999 / 4823 | Time 0.0s | Iter Loss 0.1220 | Iter Mean Loss 0.2345\n",
      "2025-11-10 13:49:51,663 - root - INFO - KG Training: Epoch 0001 Iter 4002 / 4823 | Time 0.1s | Iter Loss 0.1261 | Iter Mean Loss 0.2344\n",
      "2025-11-10 13:49:51,789 - root - INFO - KG Training: Epoch 0001 Iter 4005 / 4823 | Time 0.0s | Iter Loss 0.1297 | Iter Mean Loss 0.2343\n",
      "2025-11-10 13:49:51,911 - root - INFO - KG Training: Epoch 0001 Iter 4008 / 4823 | Time 0.0s | Iter Loss 0.1289 | Iter Mean Loss 0.2342\n",
      "2025-11-10 13:49:52,032 - root - INFO - KG Training: Epoch 0001 Iter 4011 / 4823 | Time 0.0s | Iter Loss 0.1339 | Iter Mean Loss 0.2342\n",
      "2025-11-10 13:49:52,158 - root - INFO - KG Training: Epoch 0001 Iter 4014 / 4823 | Time 0.0s | Iter Loss 0.1320 | Iter Mean Loss 0.2341\n",
      "2025-11-10 13:49:52,282 - root - INFO - KG Training: Epoch 0001 Iter 4017 / 4823 | Time 0.0s | Iter Loss 0.1380 | Iter Mean Loss 0.2340\n",
      "2025-11-10 13:49:52,475 - root - INFO - KG Training: Epoch 0001 Iter 4020 / 4823 | Time 0.0s | Iter Loss 0.1383 | Iter Mean Loss 0.2339\n",
      "2025-11-10 13:49:52,720 - root - INFO - KG Training: Epoch 0001 Iter 4023 / 4823 | Time 0.0s | Iter Loss 0.1487 | Iter Mean Loss 0.2339\n",
      "2025-11-10 13:49:52,937 - root - INFO - KG Training: Epoch 0001 Iter 4026 / 4823 | Time 0.0s | Iter Loss 0.1271 | Iter Mean Loss 0.2338\n",
      "2025-11-10 13:49:53,065 - root - INFO - KG Training: Epoch 0001 Iter 4029 / 4823 | Time 0.0s | Iter Loss 0.1413 | Iter Mean Loss 0.2337\n",
      "2025-11-10 13:49:53,258 - root - INFO - KG Training: Epoch 0001 Iter 4032 / 4823 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.2336\n",
      "2025-11-10 13:49:53,406 - root - INFO - KG Training: Epoch 0001 Iter 4035 / 4823 | Time 0.1s | Iter Loss 0.1398 | Iter Mean Loss 0.2336\n",
      "2025-11-10 13:49:53,533 - root - INFO - KG Training: Epoch 0001 Iter 4038 / 4823 | Time 0.0s | Iter Loss 0.1384 | Iter Mean Loss 0.2335\n",
      "2025-11-10 13:49:53,812 - root - INFO - KG Training: Epoch 0001 Iter 4041 / 4823 | Time 0.0s | Iter Loss 0.1409 | Iter Mean Loss 0.2334\n",
      "2025-11-10 13:49:53,940 - root - INFO - KG Training: Epoch 0001 Iter 4044 / 4823 | Time 0.0s | Iter Loss 0.1342 | Iter Mean Loss 0.2334\n",
      "2025-11-10 13:49:54,066 - root - INFO - KG Training: Epoch 0001 Iter 4047 / 4823 | Time 0.0s | Iter Loss 0.1329 | Iter Mean Loss 0.2333\n",
      "2025-11-10 13:49:54,197 - root - INFO - KG Training: Epoch 0001 Iter 4050 / 4823 | Time 0.0s | Iter Loss 0.1343 | Iter Mean Loss 0.2332\n",
      "2025-11-10 13:49:54,321 - root - INFO - KG Training: Epoch 0001 Iter 4053 / 4823 | Time 0.0s | Iter Loss 0.1359 | Iter Mean Loss 0.2331\n",
      "2025-11-10 13:49:54,450 - root - INFO - KG Training: Epoch 0001 Iter 4056 / 4823 | Time 0.0s | Iter Loss 0.1431 | Iter Mean Loss 0.2331\n",
      "2025-11-10 13:49:54,574 - root - INFO - KG Training: Epoch 0001 Iter 4059 / 4823 | Time 0.0s | Iter Loss 0.1526 | Iter Mean Loss 0.2330\n",
      "2025-11-10 13:49:54,697 - root - INFO - KG Training: Epoch 0001 Iter 4062 / 4823 | Time 0.0s | Iter Loss 0.1391 | Iter Mean Loss 0.2329\n",
      "2025-11-10 13:49:54,820 - root - INFO - KG Training: Epoch 0001 Iter 4065 / 4823 | Time 0.0s | Iter Loss 0.1355 | Iter Mean Loss 0.2329\n",
      "2025-11-10 13:49:54,989 - root - INFO - KG Training: Epoch 0001 Iter 4068 / 4823 | Time 0.0s | Iter Loss 0.1355 | Iter Mean Loss 0.2328\n",
      "2025-11-10 13:49:55,115 - root - INFO - KG Training: Epoch 0001 Iter 4071 / 4823 | Time 0.0s | Iter Loss 0.1417 | Iter Mean Loss 0.2327\n",
      "2025-11-10 13:49:55,246 - root - INFO - KG Training: Epoch 0001 Iter 4074 / 4823 | Time 0.0s | Iter Loss 0.1437 | Iter Mean Loss 0.2326\n",
      "2025-11-10 13:49:55,468 - root - INFO - KG Training: Epoch 0001 Iter 4077 / 4823 | Time 0.0s | Iter Loss 0.1299 | Iter Mean Loss 0.2326\n",
      "2025-11-10 13:49:55,596 - root - INFO - KG Training: Epoch 0001 Iter 4080 / 4823 | Time 0.0s | Iter Loss 0.1328 | Iter Mean Loss 0.2325\n",
      "2025-11-10 13:49:55,719 - root - INFO - KG Training: Epoch 0001 Iter 4083 / 4823 | Time 0.0s | Iter Loss 0.1432 | Iter Mean Loss 0.2324\n",
      "2025-11-10 13:49:55,909 - root - INFO - KG Training: Epoch 0001 Iter 4086 / 4823 | Time 0.1s | Iter Loss 0.1203 | Iter Mean Loss 0.2323\n",
      "2025-11-10 13:49:56,035 - root - INFO - KG Training: Epoch 0001 Iter 4089 / 4823 | Time 0.0s | Iter Loss 0.1328 | Iter Mean Loss 0.2323\n",
      "2025-11-10 13:49:56,160 - root - INFO - KG Training: Epoch 0001 Iter 4092 / 4823 | Time 0.0s | Iter Loss 0.1276 | Iter Mean Loss 0.2322\n",
      "2025-11-10 13:49:56,282 - root - INFO - KG Training: Epoch 0001 Iter 4095 / 4823 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.2321\n",
      "2025-11-10 13:49:56,404 - root - INFO - KG Training: Epoch 0001 Iter 4098 / 4823 | Time 0.0s | Iter Loss 0.1238 | Iter Mean Loss 0.2321\n",
      "2025-11-10 13:49:56,533 - root - INFO - KG Training: Epoch 0001 Iter 4101 / 4823 | Time 0.0s | Iter Loss 0.1523 | Iter Mean Loss 0.2320\n",
      "2025-11-10 13:49:56,659 - root - INFO - KG Training: Epoch 0001 Iter 4104 / 4823 | Time 0.0s | Iter Loss 0.1333 | Iter Mean Loss 0.2319\n",
      "2025-11-10 13:49:56,783 - root - INFO - KG Training: Epoch 0001 Iter 4107 / 4823 | Time 0.0s | Iter Loss 0.1263 | Iter Mean Loss 0.2318\n",
      "2025-11-10 13:49:56,905 - root - INFO - KG Training: Epoch 0001 Iter 4110 / 4823 | Time 0.0s | Iter Loss 0.1335 | Iter Mean Loss 0.2318\n",
      "2025-11-10 13:49:57,112 - root - INFO - KG Training: Epoch 0001 Iter 4113 / 4823 | Time 0.1s | Iter Loss 0.1435 | Iter Mean Loss 0.2317\n",
      "2025-11-10 13:49:57,236 - root - INFO - KG Training: Epoch 0001 Iter 4116 / 4823 | Time 0.0s | Iter Loss 0.1330 | Iter Mean Loss 0.2316\n",
      "2025-11-10 13:49:57,425 - root - INFO - KG Training: Epoch 0001 Iter 4119 / 4823 | Time 0.0s | Iter Loss 0.1537 | Iter Mean Loss 0.2316\n",
      "2025-11-10 13:49:57,549 - root - INFO - KG Training: Epoch 0001 Iter 4122 / 4823 | Time 0.0s | Iter Loss 0.1288 | Iter Mean Loss 0.2315\n",
      "2025-11-10 13:49:57,676 - root - INFO - KG Training: Epoch 0001 Iter 4125 / 4823 | Time 0.0s | Iter Loss 0.1289 | Iter Mean Loss 0.2314\n",
      "2025-11-10 13:49:57,800 - root - INFO - KG Training: Epoch 0001 Iter 4128 / 4823 | Time 0.0s | Iter Loss 0.1147 | Iter Mean Loss 0.2313\n",
      "2025-11-10 13:49:57,994 - root - INFO - KG Training: Epoch 0001 Iter 4131 / 4823 | Time 0.1s | Iter Loss 0.1379 | Iter Mean Loss 0.2313\n",
      "2025-11-10 13:49:58,120 - root - INFO - KG Training: Epoch 0001 Iter 4134 / 4823 | Time 0.0s | Iter Loss 0.1375 | Iter Mean Loss 0.2312\n",
      "2025-11-10 13:49:58,309 - root - INFO - KG Training: Epoch 0001 Iter 4137 / 4823 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.2311\n",
      "2025-11-10 13:49:58,432 - root - INFO - KG Training: Epoch 0001 Iter 4140 / 4823 | Time 0.0s | Iter Loss 0.1338 | Iter Mean Loss 0.2311\n",
      "2025-11-10 13:49:58,554 - root - INFO - KG Training: Epoch 0001 Iter 4143 / 4823 | Time 0.0s | Iter Loss 0.1405 | Iter Mean Loss 0.2310\n",
      "2025-11-10 13:49:58,676 - root - INFO - KG Training: Epoch 0001 Iter 4146 / 4823 | Time 0.0s | Iter Loss 0.1277 | Iter Mean Loss 0.2309\n",
      "2025-11-10 13:49:58,798 - root - INFO - KG Training: Epoch 0001 Iter 4149 / 4823 | Time 0.0s | Iter Loss 0.1202 | Iter Mean Loss 0.2309\n",
      "2025-11-10 13:49:58,927 - root - INFO - KG Training: Epoch 0001 Iter 4152 / 4823 | Time 0.0s | Iter Loss 0.1383 | Iter Mean Loss 0.2308\n",
      "2025-11-10 13:49:59,059 - root - INFO - KG Training: Epoch 0001 Iter 4155 / 4823 | Time 0.0s | Iter Loss 0.1255 | Iter Mean Loss 0.2307\n",
      "2025-11-10 13:49:59,187 - root - INFO - KG Training: Epoch 0001 Iter 4158 / 4823 | Time 0.0s | Iter Loss 0.1266 | Iter Mean Loss 0.2307\n",
      "2025-11-10 13:49:59,317 - root - INFO - KG Training: Epoch 0001 Iter 4161 / 4823 | Time 0.0s | Iter Loss 0.1384 | Iter Mean Loss 0.2306\n",
      "2025-11-10 13:49:59,439 - root - INFO - KG Training: Epoch 0001 Iter 4164 / 4823 | Time 0.0s | Iter Loss 0.1548 | Iter Mean Loss 0.2305\n",
      "2025-11-10 13:49:59,563 - root - INFO - KG Training: Epoch 0001 Iter 4167 / 4823 | Time 0.0s | Iter Loss 0.1284 | Iter Mean Loss 0.2305\n",
      "2025-11-10 13:49:59,690 - root - INFO - KG Training: Epoch 0001 Iter 4170 / 4823 | Time 0.0s | Iter Loss 0.1354 | Iter Mean Loss 0.2304\n",
      "2025-11-10 13:50:00,164 - root - INFO - KG Training: Epoch 0001 Iter 4173 / 4823 | Time 0.0s | Iter Loss 0.1293 | Iter Mean Loss 0.2303\n",
      "2025-11-10 13:50:00,287 - root - INFO - KG Training: Epoch 0001 Iter 4176 / 4823 | Time 0.0s | Iter Loss 0.1392 | Iter Mean Loss 0.2303\n",
      "2025-11-10 13:50:00,413 - root - INFO - KG Training: Epoch 0001 Iter 4179 / 4823 | Time 0.0s | Iter Loss 0.1160 | Iter Mean Loss 0.2302\n",
      "2025-11-10 13:50:00,536 - root - INFO - KG Training: Epoch 0001 Iter 4182 / 4823 | Time 0.0s | Iter Loss 0.1360 | Iter Mean Loss 0.2301\n",
      "2025-11-10 13:50:00,663 - root - INFO - KG Training: Epoch 0001 Iter 4185 / 4823 | Time 0.0s | Iter Loss 0.1345 | Iter Mean Loss 0.2300\n",
      "2025-11-10 13:50:00,977 - root - INFO - KG Training: Epoch 0001 Iter 4188 / 4823 | Time 0.0s | Iter Loss 0.1326 | Iter Mean Loss 0.2300\n",
      "2025-11-10 13:50:01,101 - root - INFO - KG Training: Epoch 0001 Iter 4191 / 4823 | Time 0.0s | Iter Loss 0.1417 | Iter Mean Loss 0.2299\n",
      "2025-11-10 13:50:01,315 - root - INFO - KG Training: Epoch 0001 Iter 4194 / 4823 | Time 0.0s | Iter Loss 0.1321 | Iter Mean Loss 0.2298\n",
      "2025-11-10 13:50:01,564 - root - INFO - KG Training: Epoch 0001 Iter 4197 / 4823 | Time 0.2s | Iter Loss 0.1347 | Iter Mean Loss 0.2298\n",
      "2025-11-10 13:50:01,691 - root - INFO - KG Training: Epoch 0001 Iter 4200 / 4823 | Time 0.0s | Iter Loss 0.1152 | Iter Mean Loss 0.2297\n",
      "2025-11-10 13:50:01,905 - root - INFO - KG Training: Epoch 0001 Iter 4203 / 4823 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.2296\n",
      "2025-11-10 13:50:02,030 - root - INFO - KG Training: Epoch 0001 Iter 4206 / 4823 | Time 0.0s | Iter Loss 0.1332 | Iter Mean Loss 0.2296\n",
      "2025-11-10 13:50:02,155 - root - INFO - KG Training: Epoch 0001 Iter 4209 / 4823 | Time 0.0s | Iter Loss 0.1396 | Iter Mean Loss 0.2295\n",
      "2025-11-10 13:50:02,395 - root - INFO - KG Training: Epoch 0001 Iter 4212 / 4823 | Time 0.0s | Iter Loss 0.1348 | Iter Mean Loss 0.2294\n",
      "2025-11-10 13:50:02,519 - root - INFO - KG Training: Epoch 0001 Iter 4215 / 4823 | Time 0.0s | Iter Loss 0.1404 | Iter Mean Loss 0.2294\n",
      "2025-11-10 13:50:02,644 - root - INFO - KG Training: Epoch 0001 Iter 4218 / 4823 | Time 0.0s | Iter Loss 0.1214 | Iter Mean Loss 0.2293\n",
      "2025-11-10 13:50:02,770 - root - INFO - KG Training: Epoch 0001 Iter 4221 / 4823 | Time 0.0s | Iter Loss 0.1204 | Iter Mean Loss 0.2292\n",
      "2025-11-10 13:50:02,961 - root - INFO - KG Training: Epoch 0001 Iter 4224 / 4823 | Time 0.1s | Iter Loss 0.1335 | Iter Mean Loss 0.2291\n",
      "2025-11-10 13:50:03,086 - root - INFO - KG Training: Epoch 0001 Iter 4227 / 4823 | Time 0.0s | Iter Loss 0.1354 | Iter Mean Loss 0.2291\n",
      "2025-11-10 13:50:03,212 - root - INFO - KG Training: Epoch 0001 Iter 4230 / 4823 | Time 0.0s | Iter Loss 0.1443 | Iter Mean Loss 0.2290\n",
      "2025-11-10 13:50:03,400 - root - INFO - KG Training: Epoch 0001 Iter 4233 / 4823 | Time 0.1s | Iter Loss 0.1402 | Iter Mean Loss 0.2289\n",
      "2025-11-10 13:50:03,524 - root - INFO - KG Training: Epoch 0001 Iter 4236 / 4823 | Time 0.0s | Iter Loss 0.1374 | Iter Mean Loss 0.2289\n",
      "2025-11-10 13:50:03,647 - root - INFO - KG Training: Epoch 0001 Iter 4239 / 4823 | Time 0.0s | Iter Loss 0.1287 | Iter Mean Loss 0.2288\n",
      "2025-11-10 13:50:03,771 - root - INFO - KG Training: Epoch 0001 Iter 4242 / 4823 | Time 0.0s | Iter Loss 0.1182 | Iter Mean Loss 0.2287\n",
      "2025-11-10 13:50:03,894 - root - INFO - KG Training: Epoch 0001 Iter 4245 / 4823 | Time 0.0s | Iter Loss 0.1400 | Iter Mean Loss 0.2287\n",
      "2025-11-10 13:50:04,085 - root - INFO - KG Training: Epoch 0001 Iter 4248 / 4823 | Time 0.0s | Iter Loss 0.1333 | Iter Mean Loss 0.2286\n",
      "2025-11-10 13:50:04,210 - root - INFO - KG Training: Epoch 0001 Iter 4251 / 4823 | Time 0.0s | Iter Loss 0.1208 | Iter Mean Loss 0.2285\n",
      "2025-11-10 13:50:04,334 - root - INFO - KG Training: Epoch 0001 Iter 4254 / 4823 | Time 0.0s | Iter Loss 0.1218 | Iter Mean Loss 0.2285\n",
      "2025-11-10 13:50:04,720 - root - INFO - KG Training: Epoch 0001 Iter 4257 / 4823 | Time 0.3s | Iter Loss 0.1297 | Iter Mean Loss 0.2284\n",
      "2025-11-10 13:50:04,853 - root - INFO - KG Training: Epoch 0001 Iter 4260 / 4823 | Time 0.0s | Iter Loss 0.1453 | Iter Mean Loss 0.2283\n",
      "2025-11-10 13:50:04,980 - root - INFO - KG Training: Epoch 0001 Iter 4263 / 4823 | Time 0.0s | Iter Loss 0.1354 | Iter Mean Loss 0.2283\n",
      "2025-11-10 13:50:05,216 - root - INFO - KG Training: Epoch 0001 Iter 4266 / 4823 | Time 0.0s | Iter Loss 0.1321 | Iter Mean Loss 0.2282\n",
      "2025-11-10 13:50:05,344 - root - INFO - KG Training: Epoch 0001 Iter 4269 / 4823 | Time 0.0s | Iter Loss 0.1241 | Iter Mean Loss 0.2281\n",
      "2025-11-10 13:50:05,469 - root - INFO - KG Training: Epoch 0001 Iter 4272 / 4823 | Time 0.0s | Iter Loss 0.1306 | Iter Mean Loss 0.2280\n",
      "2025-11-10 13:50:05,592 - root - INFO - KG Training: Epoch 0001 Iter 4275 / 4823 | Time 0.0s | Iter Loss 0.1237 | Iter Mean Loss 0.2280\n",
      "2025-11-10 13:50:05,716 - root - INFO - KG Training: Epoch 0001 Iter 4278 / 4823 | Time 0.0s | Iter Loss 0.1439 | Iter Mean Loss 0.2279\n",
      "2025-11-10 13:50:05,841 - root - INFO - KG Training: Epoch 0001 Iter 4281 / 4823 | Time 0.0s | Iter Loss 0.1276 | Iter Mean Loss 0.2278\n",
      "2025-11-10 13:50:05,967 - root - INFO - KG Training: Epoch 0001 Iter 4284 / 4823 | Time 0.0s | Iter Loss 0.1364 | Iter Mean Loss 0.2278\n",
      "2025-11-10 13:50:06,092 - root - INFO - KG Training: Epoch 0001 Iter 4287 / 4823 | Time 0.0s | Iter Loss 0.1390 | Iter Mean Loss 0.2277\n",
      "2025-11-10 13:50:06,215 - root - INFO - KG Training: Epoch 0001 Iter 4290 / 4823 | Time 0.0s | Iter Loss 0.1362 | Iter Mean Loss 0.2276\n",
      "2025-11-10 13:50:06,340 - root - INFO - KG Training: Epoch 0001 Iter 4293 / 4823 | Time 0.0s | Iter Loss 0.1223 | Iter Mean Loss 0.2276\n",
      "2025-11-10 13:50:06,464 - root - INFO - KG Training: Epoch 0001 Iter 4296 / 4823 | Time 0.0s | Iter Loss 0.1243 | Iter Mean Loss 0.2275\n",
      "2025-11-10 13:50:06,592 - root - INFO - KG Training: Epoch 0001 Iter 4299 / 4823 | Time 0.0s | Iter Loss 0.1279 | Iter Mean Loss 0.2274\n",
      "2025-11-10 13:50:06,718 - root - INFO - KG Training: Epoch 0001 Iter 4302 / 4823 | Time 0.0s | Iter Loss 0.1249 | Iter Mean Loss 0.2274\n",
      "2025-11-10 13:50:06,949 - root - INFO - KG Training: Epoch 0001 Iter 4305 / 4823 | Time 0.0s | Iter Loss 0.1265 | Iter Mean Loss 0.2273\n",
      "2025-11-10 13:50:07,144 - root - INFO - KG Training: Epoch 0001 Iter 4308 / 4823 | Time 0.0s | Iter Loss 0.1342 | Iter Mean Loss 0.2272\n",
      "2025-11-10 13:50:07,489 - root - INFO - KG Training: Epoch 0001 Iter 4311 / 4823 | Time 0.3s | Iter Loss 0.1461 | Iter Mean Loss 0.2272\n",
      "2025-11-10 13:50:07,625 - root - INFO - KG Training: Epoch 0001 Iter 4314 / 4823 | Time 0.0s | Iter Loss 0.1325 | Iter Mean Loss 0.2271\n",
      "2025-11-10 13:50:07,751 - root - INFO - KG Training: Epoch 0001 Iter 4317 / 4823 | Time 0.0s | Iter Loss 0.1342 | Iter Mean Loss 0.2270\n",
      "2025-11-10 13:50:07,877 - root - INFO - KG Training: Epoch 0001 Iter 4320 / 4823 | Time 0.0s | Iter Loss 0.1212 | Iter Mean Loss 0.2270\n",
      "2025-11-10 13:50:08,002 - root - INFO - KG Training: Epoch 0001 Iter 4323 / 4823 | Time 0.0s | Iter Loss 0.1288 | Iter Mean Loss 0.2269\n",
      "2025-11-10 13:50:08,125 - root - INFO - KG Training: Epoch 0001 Iter 4326 / 4823 | Time 0.0s | Iter Loss 0.1302 | Iter Mean Loss 0.2268\n",
      "2025-11-10 13:50:08,253 - root - INFO - KG Training: Epoch 0001 Iter 4329 / 4823 | Time 0.0s | Iter Loss 0.1353 | Iter Mean Loss 0.2268\n",
      "2025-11-10 13:50:08,381 - root - INFO - KG Training: Epoch 0001 Iter 4332 / 4823 | Time 0.0s | Iter Loss 0.1389 | Iter Mean Loss 0.2267\n",
      "2025-11-10 13:50:08,505 - root - INFO - KG Training: Epoch 0001 Iter 4335 / 4823 | Time 0.0s | Iter Loss 0.1091 | Iter Mean Loss 0.2266\n",
      "2025-11-10 13:50:08,633 - root - INFO - KG Training: Epoch 0001 Iter 4338 / 4823 | Time 0.0s | Iter Loss 0.1321 | Iter Mean Loss 0.2266\n",
      "2025-11-10 13:50:08,758 - root - INFO - KG Training: Epoch 0001 Iter 4341 / 4823 | Time 0.0s | Iter Loss 0.1356 | Iter Mean Loss 0.2265\n",
      "2025-11-10 13:50:08,884 - root - INFO - KG Training: Epoch 0001 Iter 4344 / 4823 | Time 0.0s | Iter Loss 0.1311 | Iter Mean Loss 0.2264\n",
      "2025-11-10 13:50:09,119 - root - INFO - KG Training: Epoch 0001 Iter 4347 / 4823 | Time 0.2s | Iter Loss 0.1364 | Iter Mean Loss 0.2264\n",
      "2025-11-10 13:50:09,295 - root - INFO - KG Training: Epoch 0001 Iter 4350 / 4823 | Time 0.0s | Iter Loss 0.1301 | Iter Mean Loss 0.2263\n",
      "2025-11-10 13:50:09,426 - root - INFO - KG Training: Epoch 0001 Iter 4353 / 4823 | Time 0.0s | Iter Loss 0.1266 | Iter Mean Loss 0.2263\n",
      "2025-11-10 13:50:09,547 - root - INFO - KG Training: Epoch 0001 Iter 4356 / 4823 | Time 0.0s | Iter Loss 0.1317 | Iter Mean Loss 0.2262\n",
      "2025-11-10 13:50:09,846 - root - INFO - KG Training: Epoch 0001 Iter 4359 / 4823 | Time 0.0s | Iter Loss 0.1392 | Iter Mean Loss 0.2261\n",
      "2025-11-10 13:50:09,988 - root - INFO - KG Training: Epoch 0001 Iter 4362 / 4823 | Time 0.0s | Iter Loss 0.1362 | Iter Mean Loss 0.2260\n",
      "2025-11-10 13:50:10,232 - root - INFO - KG Training: Epoch 0001 Iter 4365 / 4823 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.2260\n",
      "2025-11-10 13:50:10,366 - root - INFO - KG Training: Epoch 0001 Iter 4368 / 4823 | Time 0.0s | Iter Loss 0.1281 | Iter Mean Loss 0.2259\n",
      "2025-11-10 13:50:10,501 - root - INFO - KG Training: Epoch 0001 Iter 4371 / 4823 | Time 0.0s | Iter Loss 0.1451 | Iter Mean Loss 0.2259\n",
      "2025-11-10 13:50:10,636 - root - INFO - KG Training: Epoch 0001 Iter 4374 / 4823 | Time 0.0s | Iter Loss 0.1315 | Iter Mean Loss 0.2258\n",
      "2025-11-10 13:50:10,780 - root - INFO - KG Training: Epoch 0001 Iter 4377 / 4823 | Time 0.0s | Iter Loss 0.1393 | Iter Mean Loss 0.2257\n",
      "2025-11-10 13:50:10,913 - root - INFO - KG Training: Epoch 0001 Iter 4380 / 4823 | Time 0.0s | Iter Loss 0.1272 | Iter Mean Loss 0.2257\n",
      "2025-11-10 13:50:11,087 - root - INFO - KG Training: Epoch 0001 Iter 4383 / 4823 | Time 0.0s | Iter Loss 0.1374 | Iter Mean Loss 0.2256\n",
      "2025-11-10 13:50:11,217 - root - INFO - KG Training: Epoch 0001 Iter 4386 / 4823 | Time 0.0s | Iter Loss 0.1247 | Iter Mean Loss 0.2255\n",
      "2025-11-10 13:50:11,439 - root - INFO - KG Training: Epoch 0001 Iter 4389 / 4823 | Time 0.0s | Iter Loss 0.1242 | Iter Mean Loss 0.2255\n",
      "2025-11-10 13:50:11,578 - root - INFO - KG Training: Epoch 0001 Iter 4392 / 4823 | Time 0.0s | Iter Loss 0.1292 | Iter Mean Loss 0.2254\n",
      "2025-11-10 13:50:11,702 - root - INFO - KG Training: Epoch 0001 Iter 4395 / 4823 | Time 0.0s | Iter Loss 0.1277 | Iter Mean Loss 0.2253\n",
      "2025-11-10 13:50:11,833 - root - INFO - KG Training: Epoch 0001 Iter 4398 / 4823 | Time 0.0s | Iter Loss 0.1252 | Iter Mean Loss 0.2253\n",
      "2025-11-10 13:50:12,028 - root - INFO - KG Training: Epoch 0001 Iter 4401 / 4823 | Time 0.0s | Iter Loss 0.1268 | Iter Mean Loss 0.2252\n",
      "2025-11-10 13:50:12,153 - root - INFO - KG Training: Epoch 0001 Iter 4404 / 4823 | Time 0.0s | Iter Loss 0.1120 | Iter Mean Loss 0.2251\n",
      "2025-11-10 13:50:12,284 - root - INFO - KG Training: Epoch 0001 Iter 4407 / 4823 | Time 0.0s | Iter Loss 0.1317 | Iter Mean Loss 0.2251\n",
      "2025-11-10 13:50:12,412 - root - INFO - KG Training: Epoch 0001 Iter 4410 / 4823 | Time 0.0s | Iter Loss 0.1319 | Iter Mean Loss 0.2250\n",
      "2025-11-10 13:50:12,543 - root - INFO - KG Training: Epoch 0001 Iter 4413 / 4823 | Time 0.0s | Iter Loss 0.1378 | Iter Mean Loss 0.2249\n",
      "2025-11-10 13:50:12,671 - root - INFO - KG Training: Epoch 0001 Iter 4416 / 4823 | Time 0.0s | Iter Loss 0.1393 | Iter Mean Loss 0.2249\n",
      "2025-11-10 13:50:12,799 - root - INFO - KG Training: Epoch 0001 Iter 4419 / 4823 | Time 0.0s | Iter Loss 0.1493 | Iter Mean Loss 0.2248\n",
      "2025-11-10 13:50:12,933 - root - INFO - KG Training: Epoch 0001 Iter 4422 / 4823 | Time 0.0s | Iter Loss 0.1315 | Iter Mean Loss 0.2247\n",
      "2025-11-10 13:50:13,057 - root - INFO - KG Training: Epoch 0001 Iter 4425 / 4823 | Time 0.0s | Iter Loss 0.1374 | Iter Mean Loss 0.2247\n",
      "2025-11-10 13:50:13,287 - root - INFO - KG Training: Epoch 0001 Iter 4428 / 4823 | Time 0.0s | Iter Loss 0.1225 | Iter Mean Loss 0.2246\n",
      "2025-11-10 13:50:13,415 - root - INFO - KG Training: Epoch 0001 Iter 4431 / 4823 | Time 0.0s | Iter Loss 0.1152 | Iter Mean Loss 0.2246\n",
      "2025-11-10 13:50:13,541 - root - INFO - KG Training: Epoch 0001 Iter 4434 / 4823 | Time 0.0s | Iter Loss 0.1183 | Iter Mean Loss 0.2245\n",
      "2025-11-10 13:50:13,669 - root - INFO - KG Training: Epoch 0001 Iter 4437 / 4823 | Time 0.0s | Iter Loss 0.1146 | Iter Mean Loss 0.2244\n",
      "2025-11-10 13:50:13,796 - root - INFO - KG Training: Epoch 0001 Iter 4440 / 4823 | Time 0.0s | Iter Loss 0.1326 | Iter Mean Loss 0.2244\n",
      "2025-11-10 13:50:13,923 - root - INFO - KG Training: Epoch 0001 Iter 4443 / 4823 | Time 0.0s | Iter Loss 0.1355 | Iter Mean Loss 0.2243\n",
      "2025-11-10 13:50:14,052 - root - INFO - KG Training: Epoch 0001 Iter 4446 / 4823 | Time 0.0s | Iter Loss 0.1471 | Iter Mean Loss 0.2242\n",
      "2025-11-10 13:50:14,181 - root - INFO - KG Training: Epoch 0001 Iter 4449 / 4823 | Time 0.0s | Iter Loss 0.1176 | Iter Mean Loss 0.2242\n",
      "2025-11-10 13:50:14,546 - root - INFO - KG Training: Epoch 0001 Iter 4452 / 4823 | Time 0.3s | Iter Loss 0.1285 | Iter Mean Loss 0.2241\n",
      "2025-11-10 13:50:14,679 - root - INFO - KG Training: Epoch 0001 Iter 4455 / 4823 | Time 0.0s | Iter Loss 0.1161 | Iter Mean Loss 0.2240\n",
      "2025-11-10 13:50:14,811 - root - INFO - KG Training: Epoch 0001 Iter 4458 / 4823 | Time 0.0s | Iter Loss 0.1214 | Iter Mean Loss 0.2240\n",
      "2025-11-10 13:50:14,933 - root - INFO - KG Training: Epoch 0001 Iter 4461 / 4823 | Time 0.0s | Iter Loss 0.1254 | Iter Mean Loss 0.2239\n",
      "2025-11-10 13:50:15,292 - root - INFO - KG Training: Epoch 0001 Iter 4464 / 4823 | Time 0.0s | Iter Loss 0.1348 | Iter Mean Loss 0.2238\n",
      "2025-11-10 13:50:15,837 - root - INFO - KG Training: Epoch 0001 Iter 4467 / 4823 | Time 0.0s | Iter Loss 0.1204 | Iter Mean Loss 0.2238\n",
      "2025-11-10 13:50:15,961 - root - INFO - KG Training: Epoch 0001 Iter 4470 / 4823 | Time 0.0s | Iter Loss 0.1282 | Iter Mean Loss 0.2237\n",
      "2025-11-10 13:50:16,091 - root - INFO - KG Training: Epoch 0001 Iter 4473 / 4823 | Time 0.0s | Iter Loss 0.1372 | Iter Mean Loss 0.2237\n",
      "2025-11-10 13:50:16,219 - root - INFO - KG Training: Epoch 0001 Iter 4476 / 4823 | Time 0.0s | Iter Loss 0.1144 | Iter Mean Loss 0.2236\n",
      "2025-11-10 13:50:16,343 - root - INFO - KG Training: Epoch 0001 Iter 4479 / 4823 | Time 0.0s | Iter Loss 0.1315 | Iter Mean Loss 0.2235\n",
      "2025-11-10 13:50:16,465 - root - INFO - KG Training: Epoch 0001 Iter 4482 / 4823 | Time 0.0s | Iter Loss 0.1233 | Iter Mean Loss 0.2235\n",
      "2025-11-10 13:50:16,674 - root - INFO - KG Training: Epoch 0001 Iter 4485 / 4823 | Time 0.0s | Iter Loss 0.1352 | Iter Mean Loss 0.2234\n",
      "2025-11-10 13:50:16,797 - root - INFO - KG Training: Epoch 0001 Iter 4488 / 4823 | Time 0.0s | Iter Loss 0.1276 | Iter Mean Loss 0.2233\n",
      "2025-11-10 13:50:17,043 - root - INFO - KG Training: Epoch 0001 Iter 4491 / 4823 | Time 0.0s | Iter Loss 0.1212 | Iter Mean Loss 0.2233\n",
      "2025-11-10 13:50:17,172 - root - INFO - KG Training: Epoch 0001 Iter 4494 / 4823 | Time 0.0s | Iter Loss 0.1442 | Iter Mean Loss 0.2232\n",
      "2025-11-10 13:50:17,298 - root - INFO - KG Training: Epoch 0001 Iter 4497 / 4823 | Time 0.0s | Iter Loss 0.1220 | Iter Mean Loss 0.2231\n",
      "2025-11-10 13:50:17,425 - root - INFO - KG Training: Epoch 0001 Iter 4500 / 4823 | Time 0.0s | Iter Loss 0.1175 | Iter Mean Loss 0.2231\n",
      "2025-11-10 13:50:17,753 - root - INFO - KG Training: Epoch 0001 Iter 4503 / 4823 | Time 0.2s | Iter Loss 0.1395 | Iter Mean Loss 0.2230\n",
      "2025-11-10 13:50:18,040 - root - INFO - KG Training: Epoch 0001 Iter 4506 / 4823 | Time 0.0s | Iter Loss 0.1271 | Iter Mean Loss 0.2230\n",
      "2025-11-10 13:50:18,221 - root - INFO - KG Training: Epoch 0001 Iter 4509 / 4823 | Time 0.0s | Iter Loss 0.1197 | Iter Mean Loss 0.2229\n",
      "2025-11-10 13:50:18,639 - root - INFO - KG Training: Epoch 0001 Iter 4512 / 4823 | Time 0.0s | Iter Loss 0.1265 | Iter Mean Loss 0.2228\n",
      "2025-11-10 13:50:19,006 - root - INFO - KG Training: Epoch 0001 Iter 4515 / 4823 | Time 0.0s | Iter Loss 0.1109 | Iter Mean Loss 0.2228\n",
      "2025-11-10 13:50:19,201 - root - INFO - KG Training: Epoch 0001 Iter 4518 / 4823 | Time 0.0s | Iter Loss 0.1295 | Iter Mean Loss 0.2227\n",
      "2025-11-10 13:50:19,328 - root - INFO - KG Training: Epoch 0001 Iter 4521 / 4823 | Time 0.0s | Iter Loss 0.1198 | Iter Mean Loss 0.2226\n",
      "2025-11-10 13:50:19,451 - root - INFO - KG Training: Epoch 0001 Iter 4524 / 4823 | Time 0.0s | Iter Loss 0.1293 | Iter Mean Loss 0.2226\n",
      "2025-11-10 13:50:19,576 - root - INFO - KG Training: Epoch 0001 Iter 4527 / 4823 | Time 0.0s | Iter Loss 0.1200 | Iter Mean Loss 0.2225\n",
      "2025-11-10 13:50:19,843 - root - INFO - KG Training: Epoch 0001 Iter 4530 / 4823 | Time 0.0s | Iter Loss 0.1403 | Iter Mean Loss 0.2224\n",
      "2025-11-10 13:50:19,968 - root - INFO - KG Training: Epoch 0001 Iter 4533 / 4823 | Time 0.0s | Iter Loss 0.1304 | Iter Mean Loss 0.2224\n",
      "2025-11-10 13:50:20,100 - root - INFO - KG Training: Epoch 0001 Iter 4536 / 4823 | Time 0.0s | Iter Loss 0.1333 | Iter Mean Loss 0.2223\n",
      "2025-11-10 13:50:20,220 - root - INFO - KG Training: Epoch 0001 Iter 4539 / 4823 | Time 0.0s | Iter Loss 0.1214 | Iter Mean Loss 0.2222\n",
      "2025-11-10 13:50:20,516 - root - INFO - KG Training: Epoch 0001 Iter 4542 / 4823 | Time 0.0s | Iter Loss 0.1340 | Iter Mean Loss 0.2222\n",
      "2025-11-10 13:50:20,646 - root - INFO - KG Training: Epoch 0001 Iter 4545 / 4823 | Time 0.0s | Iter Loss 0.1345 | Iter Mean Loss 0.2221\n",
      "2025-11-10 13:50:20,768 - root - INFO - KG Training: Epoch 0001 Iter 4548 / 4823 | Time 0.0s | Iter Loss 0.1161 | Iter Mean Loss 0.2221\n",
      "2025-11-10 13:50:20,994 - root - INFO - KG Training: Epoch 0001 Iter 4551 / 4823 | Time 0.0s | Iter Loss 0.1228 | Iter Mean Loss 0.2220\n",
      "2025-11-10 13:50:21,123 - root - INFO - KG Training: Epoch 0001 Iter 4554 / 4823 | Time 0.0s | Iter Loss 0.1215 | Iter Mean Loss 0.2219\n",
      "2025-11-10 13:50:21,248 - root - INFO - KG Training: Epoch 0001 Iter 4557 / 4823 | Time 0.0s | Iter Loss 0.1211 | Iter Mean Loss 0.2219\n",
      "2025-11-10 13:50:21,375 - root - INFO - KG Training: Epoch 0001 Iter 4560 / 4823 | Time 0.0s | Iter Loss 0.1142 | Iter Mean Loss 0.2218\n",
      "2025-11-10 13:50:21,508 - root - INFO - KG Training: Epoch 0001 Iter 4563 / 4823 | Time 0.0s | Iter Loss 0.1261 | Iter Mean Loss 0.2217\n",
      "2025-11-10 13:50:21,892 - root - INFO - KG Training: Epoch 0001 Iter 4566 / 4823 | Time 0.0s | Iter Loss 0.1265 | Iter Mean Loss 0.2217\n",
      "2025-11-10 13:50:22,021 - root - INFO - KG Training: Epoch 0001 Iter 4569 / 4823 | Time 0.0s | Iter Loss 0.1297 | Iter Mean Loss 0.2216\n",
      "2025-11-10 13:50:22,145 - root - INFO - KG Training: Epoch 0001 Iter 4572 / 4823 | Time 0.0s | Iter Loss 0.1224 | Iter Mean Loss 0.2216\n",
      "2025-11-10 13:50:22,266 - root - INFO - KG Training: Epoch 0001 Iter 4575 / 4823 | Time 0.0s | Iter Loss 0.1255 | Iter Mean Loss 0.2215\n",
      "2025-11-10 13:50:22,396 - root - INFO - KG Training: Epoch 0001 Iter 4578 / 4823 | Time 0.0s | Iter Loss 0.1414 | Iter Mean Loss 0.2214\n",
      "2025-11-10 13:50:22,522 - root - INFO - KG Training: Epoch 0001 Iter 4581 / 4823 | Time 0.0s | Iter Loss 0.1179 | Iter Mean Loss 0.2214\n",
      "2025-11-10 13:50:22,649 - root - INFO - KG Training: Epoch 0001 Iter 4584 / 4823 | Time 0.0s | Iter Loss 0.1190 | Iter Mean Loss 0.2213\n",
      "2025-11-10 13:50:22,776 - root - INFO - KG Training: Epoch 0001 Iter 4587 / 4823 | Time 0.0s | Iter Loss 0.1279 | Iter Mean Loss 0.2213\n",
      "2025-11-10 13:50:22,905 - root - INFO - KG Training: Epoch 0001 Iter 4590 / 4823 | Time 0.0s | Iter Loss 0.1288 | Iter Mean Loss 0.2212\n",
      "2025-11-10 13:50:23,028 - root - INFO - KG Training: Epoch 0001 Iter 4593 / 4823 | Time 0.0s | Iter Loss 0.1200 | Iter Mean Loss 0.2211\n",
      "2025-11-10 13:50:23,153 - root - INFO - KG Training: Epoch 0001 Iter 4596 / 4823 | Time 0.0s | Iter Loss 0.1300 | Iter Mean Loss 0.2211\n",
      "2025-11-10 13:50:23,275 - root - INFO - KG Training: Epoch 0001 Iter 4599 / 4823 | Time 0.0s | Iter Loss 0.1252 | Iter Mean Loss 0.2210\n",
      "2025-11-10 13:50:23,399 - root - INFO - KG Training: Epoch 0001 Iter 4602 / 4823 | Time 0.0s | Iter Loss 0.1365 | Iter Mean Loss 0.2210\n",
      "2025-11-10 13:50:23,526 - root - INFO - KG Training: Epoch 0001 Iter 4605 / 4823 | Time 0.0s | Iter Loss 0.1219 | Iter Mean Loss 0.2209\n",
      "2025-11-10 13:50:23,767 - root - INFO - KG Training: Epoch 0001 Iter 4608 / 4823 | Time 0.0s | Iter Loss 0.1316 | Iter Mean Loss 0.2208\n",
      "2025-11-10 13:50:23,889 - root - INFO - KG Training: Epoch 0001 Iter 4611 / 4823 | Time 0.0s | Iter Loss 0.1370 | Iter Mean Loss 0.2208\n",
      "2025-11-10 13:50:24,013 - root - INFO - KG Training: Epoch 0001 Iter 4614 / 4823 | Time 0.0s | Iter Loss 0.1351 | Iter Mean Loss 0.2207\n",
      "2025-11-10 13:50:24,246 - root - INFO - KG Training: Epoch 0001 Iter 4617 / 4823 | Time 0.1s | Iter Loss 0.1377 | Iter Mean Loss 0.2207\n",
      "2025-11-10 13:50:24,374 - root - INFO - KG Training: Epoch 0001 Iter 4620 / 4823 | Time 0.0s | Iter Loss 0.1310 | Iter Mean Loss 0.2206\n",
      "2025-11-10 13:50:24,499 - root - INFO - KG Training: Epoch 0001 Iter 4623 / 4823 | Time 0.0s | Iter Loss 0.1169 | Iter Mean Loss 0.2205\n",
      "2025-11-10 13:50:24,621 - root - INFO - KG Training: Epoch 0001 Iter 4626 / 4823 | Time 0.0s | Iter Loss 0.1189 | Iter Mean Loss 0.2205\n",
      "2025-11-10 13:50:24,752 - root - INFO - KG Training: Epoch 0001 Iter 4629 / 4823 | Time 0.0s | Iter Loss 0.1313 | Iter Mean Loss 0.2204\n",
      "2025-11-10 13:50:24,879 - root - INFO - KG Training: Epoch 0001 Iter 4632 / 4823 | Time 0.0s | Iter Loss 0.1331 | Iter Mean Loss 0.2204\n",
      "2025-11-10 13:50:25,003 - root - INFO - KG Training: Epoch 0001 Iter 4635 / 4823 | Time 0.0s | Iter Loss 0.1202 | Iter Mean Loss 0.2203\n",
      "2025-11-10 13:50:25,134 - root - INFO - KG Training: Epoch 0001 Iter 4638 / 4823 | Time 0.0s | Iter Loss 0.1237 | Iter Mean Loss 0.2202\n",
      "2025-11-10 13:50:25,261 - root - INFO - KG Training: Epoch 0001 Iter 4641 / 4823 | Time 0.0s | Iter Loss 0.1247 | Iter Mean Loss 0.2202\n",
      "2025-11-10 13:50:25,387 - root - INFO - KG Training: Epoch 0001 Iter 4644 / 4823 | Time 0.0s | Iter Loss 0.1235 | Iter Mean Loss 0.2201\n",
      "2025-11-10 13:50:25,514 - root - INFO - KG Training: Epoch 0001 Iter 4647 / 4823 | Time 0.0s | Iter Loss 0.1182 | Iter Mean Loss 0.2200\n",
      "2025-11-10 13:50:25,788 - root - INFO - KG Training: Epoch 0001 Iter 4650 / 4823 | Time 0.2s | Iter Loss 0.1301 | Iter Mean Loss 0.2200\n",
      "2025-11-10 13:50:25,920 - root - INFO - KG Training: Epoch 0001 Iter 4653 / 4823 | Time 0.0s | Iter Loss 0.1269 | Iter Mean Loss 0.2199\n",
      "2025-11-10 13:50:26,048 - root - INFO - KG Training: Epoch 0001 Iter 4656 / 4823 | Time 0.0s | Iter Loss 0.1208 | Iter Mean Loss 0.2199\n",
      "2025-11-10 13:50:26,243 - root - INFO - KG Training: Epoch 0001 Iter 4659 / 4823 | Time 0.0s | Iter Loss 0.1291 | Iter Mean Loss 0.2198\n",
      "2025-11-10 13:50:26,370 - root - INFO - KG Training: Epoch 0001 Iter 4662 / 4823 | Time 0.0s | Iter Loss 0.1461 | Iter Mean Loss 0.2198\n",
      "2025-11-10 13:50:26,659 - root - INFO - KG Training: Epoch 0001 Iter 4665 / 4823 | Time 0.2s | Iter Loss 0.1198 | Iter Mean Loss 0.2197\n",
      "2025-11-10 13:50:26,826 - root - INFO - KG Training: Epoch 0001 Iter 4668 / 4823 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.2196\n",
      "2025-11-10 13:50:26,951 - root - INFO - KG Training: Epoch 0001 Iter 4671 / 4823 | Time 0.0s | Iter Loss 0.1250 | Iter Mean Loss 0.2196\n",
      "2025-11-10 13:50:27,076 - root - INFO - KG Training: Epoch 0001 Iter 4674 / 4823 | Time 0.0s | Iter Loss 0.1213 | Iter Mean Loss 0.2195\n",
      "2025-11-10 13:50:27,204 - root - INFO - KG Training: Epoch 0001 Iter 4677 / 4823 | Time 0.0s | Iter Loss 0.1270 | Iter Mean Loss 0.2194\n",
      "2025-11-10 13:50:27,325 - root - INFO - KG Training: Epoch 0001 Iter 4680 / 4823 | Time 0.0s | Iter Loss 0.1302 | Iter Mean Loss 0.2194\n",
      "2025-11-10 13:50:27,453 - root - INFO - KG Training: Epoch 0001 Iter 4683 / 4823 | Time 0.0s | Iter Loss 0.1294 | Iter Mean Loss 0.2193\n",
      "2025-11-10 13:50:27,577 - root - INFO - KG Training: Epoch 0001 Iter 4686 / 4823 | Time 0.0s | Iter Loss 0.1275 | Iter Mean Loss 0.2193\n",
      "2025-11-10 13:50:27,701 - root - INFO - KG Training: Epoch 0001 Iter 4689 / 4823 | Time 0.0s | Iter Loss 0.1210 | Iter Mean Loss 0.2192\n",
      "2025-11-10 13:50:27,825 - root - INFO - KG Training: Epoch 0001 Iter 4692 / 4823 | Time 0.0s | Iter Loss 0.1222 | Iter Mean Loss 0.2191\n",
      "2025-11-10 13:50:27,948 - root - INFO - KG Training: Epoch 0001 Iter 4695 / 4823 | Time 0.0s | Iter Loss 0.1230 | Iter Mean Loss 0.2191\n",
      "2025-11-10 13:50:28,072 - root - INFO - KG Training: Epoch 0001 Iter 4698 / 4823 | Time 0.0s | Iter Loss 0.1276 | Iter Mean Loss 0.2190\n",
      "2025-11-10 13:50:28,198 - root - INFO - KG Training: Epoch 0001 Iter 4701 / 4823 | Time 0.0s | Iter Loss 0.1320 | Iter Mean Loss 0.2190\n",
      "2025-11-10 13:50:28,396 - root - INFO - KG Training: Epoch 0001 Iter 4704 / 4823 | Time 0.1s | Iter Loss 0.1127 | Iter Mean Loss 0.2189\n",
      "2025-11-10 13:50:28,519 - root - INFO - KG Training: Epoch 0001 Iter 4707 / 4823 | Time 0.0s | Iter Loss 0.1139 | Iter Mean Loss 0.2188\n",
      "2025-11-10 13:50:28,647 - root - INFO - KG Training: Epoch 0001 Iter 4710 / 4823 | Time 0.0s | Iter Loss 0.1210 | Iter Mean Loss 0.2188\n",
      "2025-11-10 13:50:28,770 - root - INFO - KG Training: Epoch 0001 Iter 4713 / 4823 | Time 0.0s | Iter Loss 0.1238 | Iter Mean Loss 0.2187\n",
      "2025-11-10 13:50:28,895 - root - INFO - KG Training: Epoch 0001 Iter 4716 / 4823 | Time 0.0s | Iter Loss 0.1247 | Iter Mean Loss 0.2187\n",
      "2025-11-10 13:50:29,018 - root - INFO - KG Training: Epoch 0001 Iter 4719 / 4823 | Time 0.0s | Iter Loss 0.1283 | Iter Mean Loss 0.2186\n",
      "2025-11-10 13:50:29,564 - root - INFO - KG Training: Epoch 0001 Iter 4722 / 4823 | Time 0.0s | Iter Loss 0.1213 | Iter Mean Loss 0.2185\n",
      "2025-11-10 13:50:29,688 - root - INFO - KG Training: Epoch 0001 Iter 4725 / 4823 | Time 0.0s | Iter Loss 0.1235 | Iter Mean Loss 0.2185\n",
      "2025-11-10 13:50:29,815 - root - INFO - KG Training: Epoch 0001 Iter 4728 / 4823 | Time 0.0s | Iter Loss 0.1221 | Iter Mean Loss 0.2184\n",
      "2025-11-10 13:50:29,944 - root - INFO - KG Training: Epoch 0001 Iter 4731 / 4823 | Time 0.0s | Iter Loss 0.1209 | Iter Mean Loss 0.2184\n",
      "2025-11-10 13:50:30,067 - root - INFO - KG Training: Epoch 0001 Iter 4734 / 4823 | Time 0.0s | Iter Loss 0.1127 | Iter Mean Loss 0.2183\n",
      "2025-11-10 13:50:30,194 - root - INFO - KG Training: Epoch 0001 Iter 4737 / 4823 | Time 0.0s | Iter Loss 0.1351 | Iter Mean Loss 0.2182\n",
      "2025-11-10 13:50:30,321 - root - INFO - KG Training: Epoch 0001 Iter 4740 / 4823 | Time 0.0s | Iter Loss 0.1307 | Iter Mean Loss 0.2182\n",
      "2025-11-10 13:50:30,445 - root - INFO - KG Training: Epoch 0001 Iter 4743 / 4823 | Time 0.0s | Iter Loss 0.1294 | Iter Mean Loss 0.2181\n",
      "2025-11-10 13:50:30,777 - root - INFO - KG Training: Epoch 0001 Iter 4746 / 4823 | Time 0.2s | Iter Loss 0.1238 | Iter Mean Loss 0.2181\n",
      "2025-11-10 13:50:30,904 - root - INFO - KG Training: Epoch 0001 Iter 4749 / 4823 | Time 0.0s | Iter Loss 0.1265 | Iter Mean Loss 0.2180\n",
      "2025-11-10 13:50:31,033 - root - INFO - KG Training: Epoch 0001 Iter 4752 / 4823 | Time 0.0s | Iter Loss 0.1241 | Iter Mean Loss 0.2179\n",
      "2025-11-10 13:50:31,395 - root - INFO - KG Training: Epoch 0001 Iter 4755 / 4823 | Time 0.3s | Iter Loss 0.1204 | Iter Mean Loss 0.2179\n",
      "2025-11-10 13:50:31,526 - root - INFO - KG Training: Epoch 0001 Iter 4758 / 4823 | Time 0.0s | Iter Loss 0.1121 | Iter Mean Loss 0.2178\n",
      "2025-11-10 13:50:31,662 - root - INFO - KG Training: Epoch 0001 Iter 4761 / 4823 | Time 0.0s | Iter Loss 0.1314 | Iter Mean Loss 0.2178\n",
      "2025-11-10 13:50:31,785 - root - INFO - KG Training: Epoch 0001 Iter 4764 / 4823 | Time 0.0s | Iter Loss 0.1133 | Iter Mean Loss 0.2177\n",
      "2025-11-10 13:50:31,910 - root - INFO - KG Training: Epoch 0001 Iter 4767 / 4823 | Time 0.0s | Iter Loss 0.1138 | Iter Mean Loss 0.2176\n",
      "2025-11-10 13:50:32,031 - root - INFO - KG Training: Epoch 0001 Iter 4770 / 4823 | Time 0.0s | Iter Loss 0.1264 | Iter Mean Loss 0.2176\n",
      "2025-11-10 13:50:32,155 - root - INFO - KG Training: Epoch 0001 Iter 4773 / 4823 | Time 0.0s | Iter Loss 0.1184 | Iter Mean Loss 0.2175\n",
      "2025-11-10 13:50:32,281 - root - INFO - KG Training: Epoch 0001 Iter 4776 / 4823 | Time 0.0s | Iter Loss 0.1207 | Iter Mean Loss 0.2175\n",
      "2025-11-10 13:50:32,449 - root - INFO - KG Training: Epoch 0001 Iter 4779 / 4823 | Time 0.1s | Iter Loss 0.1165 | Iter Mean Loss 0.2174\n",
      "2025-11-10 13:50:32,670 - root - INFO - KG Training: Epoch 0001 Iter 4782 / 4823 | Time 0.0s | Iter Loss 0.1253 | Iter Mean Loss 0.2173\n",
      "2025-11-10 13:50:32,797 - root - INFO - KG Training: Epoch 0001 Iter 4785 / 4823 | Time 0.0s | Iter Loss 0.1209 | Iter Mean Loss 0.2173\n",
      "2025-11-10 13:50:32,919 - root - INFO - KG Training: Epoch 0001 Iter 4788 / 4823 | Time 0.0s | Iter Loss 0.1014 | Iter Mean Loss 0.2172\n",
      "2025-11-10 13:50:33,042 - root - INFO - KG Training: Epoch 0001 Iter 4791 / 4823 | Time 0.0s | Iter Loss 0.1261 | Iter Mean Loss 0.2172\n",
      "2025-11-10 13:50:33,164 - root - INFO - KG Training: Epoch 0001 Iter 4794 / 4823 | Time 0.0s | Iter Loss 0.1156 | Iter Mean Loss 0.2171\n",
      "2025-11-10 13:50:33,290 - root - INFO - KG Training: Epoch 0001 Iter 4797 / 4823 | Time 0.0s | Iter Loss 0.1260 | Iter Mean Loss 0.2170\n",
      "2025-11-10 13:50:33,412 - root - INFO - KG Training: Epoch 0001 Iter 4800 / 4823 | Time 0.0s | Iter Loss 0.1184 | Iter Mean Loss 0.2170\n",
      "2025-11-10 13:50:33,557 - root - INFO - KG Training: Epoch 0001 Iter 4803 / 4823 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.2169\n",
      "2025-11-10 13:50:33,682 - root - INFO - KG Training: Epoch 0001 Iter 4806 / 4823 | Time 0.0s | Iter Loss 0.1062 | Iter Mean Loss 0.2169\n",
      "2025-11-10 13:50:33,803 - root - INFO - KG Training: Epoch 0001 Iter 4809 / 4823 | Time 0.0s | Iter Loss 0.1306 | Iter Mean Loss 0.2168\n",
      "2025-11-10 13:50:34,055 - root - INFO - KG Training: Epoch 0001 Iter 4812 / 4823 | Time 0.0s | Iter Loss 0.1259 | Iter Mean Loss 0.2167\n",
      "2025-11-10 13:50:34,324 - root - INFO - KG Training: Epoch 0001 Iter 4815 / 4823 | Time 0.2s | Iter Loss 0.1276 | Iter Mean Loss 0.2167\n",
      "2025-11-10 13:50:34,452 - root - INFO - KG Training: Epoch 0001 Iter 4818 / 4823 | Time 0.0s | Iter Loss 0.1143 | Iter Mean Loss 0.2166\n",
      "2025-11-10 13:50:34,648 - root - INFO - KG Training: Epoch 0001 Iter 4821 / 4823 | Time 0.0s | Iter Loss 0.1181 | Iter Mean Loss 0.2166\n",
      "2025-11-10 13:50:34,758 - root - INFO - KG Training: Epoch 0001 Total Iter 4823 | Total Time 250.1s | Iter Mean Loss 0.2165\n",
      "2025-11-10 13:50:35,915 - root - INFO - Update Attention: Epoch 0001 | Total Time 1.2s\n",
      "2025-11-10 13:50:35,916 - root - INFO - CF + KG Training: Epoch 0001 | Total Time 403.1s\n",
      "Evaluating Iteration: 100%|| 1/1 [02:06<00:00, 126.11s/it]\n",
      "2025-11-10 13:52:45,054 - root - INFO - CF Evaluation: Epoch 0001 | Total Time 129.1s | Precision [0.0014, 0.0010], Recall [0.0028, 0.0098], NDCG [0.0022, 0.0047]\n",
      "2025-11-10 13:52:45,250 - root - INFO - Save model on epoch 0001!\n",
      "2025-11-10 13:52:45,265 - root - INFO - Best CF Evaluation: Epoch 0001 | Precision [0.0014, 0.0010], Recall [0.0028, 0.0098], NDCG [0.0022, 0.0047]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    log_save_id = create_log_id(args.save_dir)\n",
    "    logging_config(folder=args.save_dir, name='log{:d}'.format(log_save_id), no_console=False)\n",
    "    logging.info(args)\n",
    "\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    if args.use_pretrain == 1:\n",
    "        user_pre_embed = torch.tensor(data.user_pre_embed)\n",
    "        item_pre_embed = torch.tensor(data.item_pre_embed)\n",
    "    else:\n",
    "        user_pre_embed, item_pre_embed = None, None\n",
    "\n",
    "    # construct model & optimizer\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    if args.use_pretrain == 2:\n",
    "        model = load_model(model, args.pretrain_model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(model)\n",
    "\n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.to(device)\n",
    "            cf_batch_pos_item = cf_batch_pos_item.to(device)\n",
    "            cf_batch_neg_item = cf_batch_neg_item.to(device)\n",
    "\n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            if (iter % args.kg_print_every) == 0:\n",
    "                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            logging.info('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                logging.info('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.tsv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    logging.info('Best CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n",
    "\n",
    "\n",
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_kgat_args()\n",
    "    train(args)\n",
    "    # predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7333650",
   "metadata": {},
   "source": [
    "# 3. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66dd7d",
   "metadata": {},
   "source": [
    "## 3.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045ee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585aaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/DM/lib/python3.11/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8b005",
   "metadata": {},
   "source": [
    "## 3.2. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbbfc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ngannkim/bxdump?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24.6M/24.6M [00:01<00:00, 19.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/user/.cache/kagglehub/datasets/ngannkim/bxdump/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ngannkim/bxdump\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743485bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the dataset directory: ['BX-Books.csv', 'BX-Users.csv', 'BX-Book-Ratings.csv']\n",
      "First 5 lines of BX-Books.csv:\n",
      "\"ISBN\";\"Book-Title\";\"Book-Author\";\"Year-Of-Publication\";\"Publisher\";\"Image-URL-S\";\"Image-URL-M\";\"Image-URL-L\"\n",
      "\"0195153448\";\"Classical Mythology\";\"Mark P. O. Morford\";\"2002\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg\"\n",
      "\"0002005018\";\"Clara Callan\";\"Richard Bruce Wright\";\"2001\";\"HarperFlamingo Canada\";\"http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg\"\n",
      "\"0060973129\";\"Decision in Normandy\";\"Carlo D'Este\";\"1991\";\"HarperPerennial\";\"http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg\"\n",
      "\"0374157065\";\"Flu: The Story of the Great Influenza Pandemic of 1918 and the Search for the Virus That Caused It\";\"Gina Bari Kolata\";\"1999\";\"Farrar Straus Giroux\";\"http://images.amazon.com/images/P/0374157065.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.LZZZZZZZ.jpg\"\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Path to the extracted dataset\n",
    "dataset_path = '/home/user/.cache/kagglehub/datasets/ngannkim/bxdump/versions/1'\n",
    "\n",
    "# Check if the directory exists and list the files\n",
    "if os.path.exists(dataset_path):\n",
    "    # List files in the dataset path\n",
    "    files = os.listdir(dataset_path)\n",
    "    print(\"Files in the dataset directory:\", files)\n",
    "\n",
    "    # Open a specific file if needed, for example 'BX-Books.csv'\n",
    "    books_file = os.path.join(dataset_path, 'BX-Books.csv')\n",
    "    with open(books_file, 'r', encoding='latin-1') as f:\n",
    "        # Process the file (e.g., read data, print the first few lines)\n",
    "        print(\"First 5 lines of BX-Books.csv:\")\n",
    "        for i in range(5):\n",
    "            print(f.readline().strip())\n",
    "else:\n",
    "    print(\"Dataset directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c690162",
   "metadata": {},
   "source": [
    "## 3.3. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1532e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3827158/1450481755.py:15: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(books_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path to your dataset\n",
    "base_path = '/home/user/.cache/kagglehub/datasets/ngannkim/bxdump/versions/1'\n",
    "\n",
    "# Define file paths\n",
    "ratings_file = os.path.join(base_path, 'BX-Book-Ratings.csv')\n",
    "users_file = os.path.join(base_path, 'BX-Users.csv')\n",
    "books_file = os.path.join(base_path, 'BX-Books.csv')\n",
    "\n",
    "# Load the datasets with appropriate options\n",
    "ratings = pd.read_csv(ratings_file, sep=';', encoding='latin-1')\n",
    "users = pd.read_csv(users_file, sep=';', encoding='latin-1')\n",
    "books = pd.read_csv(books_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc19176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0   276725  034545104X            0\n",
       "1   276726  0155061224            5\n",
       "2   276727  0446520802            0\n",
       "3   276729  052165615X            3\n",
       "4   276729  0521795028            6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4d5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5c175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297c3c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Book-Rating', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "sns.countplot(x=ratings['Book-Rating']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3370ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105283\n",
      "340556\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings['User-ID'].unique())) \n",
    "print(len(ratings['ISBN'].unique())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239ac07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isbn_counts = ratings.groupby('ISBN').size()\n",
    "count_occurrences = isbn_counts.value_counts()\n",
    "count_occurrences[:15].plot(kind='bar')\n",
    "plt.xlabel(\"Number of occurrences of an ISBN number\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "784f5c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "userid_counts = ratings.groupby('User-ID').size() \n",
    "count_occurrences = userid_counts.value_counts() \n",
    "count_occurrences[:15].plot(kind='bar')\n",
    "plt.xlabel(\"Number of occurrences of a User-ID\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519d575",
   "metadata": {},
   "source": [
    "## 3.4. Preprocessing the Book-Crossing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn import LGConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2a2a3",
   "metadata": {},
   "source": [
    "### 3.4.1. Use 100k highest ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06b2fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "ratings = ratings.loc[ratings['ISBN'].isin(books['ISBN'].unique()) & ratings['User-ID'].isin(users['User-ID'].unique())]\n",
    "\n",
    "# Keep the 100k highest ratings\n",
    "ratings = ratings[ratings['Book-Rating'] >= 8].iloc[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8f5c7",
   "metadata": {},
   "source": [
    "### 3.4.2. Create mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "474ba939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "user_mapping = {userid: i for i, userid in enumerate(ratings['User-ID'].unique())}\n",
    "item_mapping = {isbn: i for i, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "\n",
    "# Count users and items\n",
    "num_users = len(user_mapping)\n",
    "num_items = len(item_mapping)\n",
    "num_total = num_users + num_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de513e0",
   "metadata": {},
   "source": [
    "### 3.4.3. Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2afc2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the adjacency matrix based on user ratings\n",
    "user_ids = torch.LongTensor([user_mapping[i] for i in ratings['User-ID']])\n",
    "item_ids = torch.LongTensor([item_mapping[i] for i in ratings['ISBN']])\n",
    "edge_index = torch.stack((user_ids, item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d2a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation, and test adjacency matrices\n",
    "train_index, test_index = train_test_split(range(len(ratings)), test_size=0.2, random_state=0)\n",
    "val_index, test_index = train_test_split(test_index, test_size=0.5, random_state=0)\n",
    "\n",
    "train_edge_index = edge_index[:, train_index]\n",
    "val_edge_index = edge_index[:, val_index]\n",
    "test_edge_index = edge_index[:, test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ae4f6",
   "metadata": {},
   "source": [
    "## 3.5. Mini batch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc819e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mini_batch(edge_index):\n",
    "    # Generate BATCH_SIZE random indices\n",
    "    index = np.random.choice(range(edge_index.shape[1]), size=BATCH_SIZE)\n",
    "\n",
    "    # Generate negative sample indices\n",
    "    edge_index = structured_negative_sampling(edge_index)\n",
    "    edge_index = torch.stack(edge_index, dim=0)\n",
    "    \n",
    "    user_index = edge_index[0, index]\n",
    "    pos_item_index = edge_index[1, index]\n",
    "    neg_item_index = edge_index[2, index]\n",
    "    \n",
    "    return user_index, pos_item_index, neg_item_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298bdad",
   "metadata": {},
   "source": [
    "## 3.6. Implement LightGCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a47d3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_layers=4, dim_h=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_users = nn.Embedding(num_embeddings=self.num_users, embedding_dim=dim_h)\n",
    "        self.emb_items = nn.Embedding(num_embeddings=self.num_items, embedding_dim=dim_h)\n",
    "\n",
    "        self.convs = nn.ModuleList(LGConv() for _ in range(num_layers))\n",
    "\n",
    "        nn.init.normal_(self.emb_users.weight, std=0.01)\n",
    "        nn.init.normal_(self.emb_items.weight, std=0.01)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        emb = torch.cat([self.emb_users.weight, self.emb_items.weight])\n",
    "        embs = [emb]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            emb = conv(x=emb, edge_index=edge_index)\n",
    "            embs.append(emb)\n",
    "\n",
    "        emb_final = 1/(self.num_layers+1) * torch.mean(torch.stack(embs, dim=1), dim=1)\n",
    "\n",
    "        emb_users_final, emb_items_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
    "\n",
    "        return emb_users_final, self.emb_users.weight, emb_items_final, self.emb_items.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c8a20",
   "metadata": {},
   "source": [
    "## 3.7. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955934fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items):\n",
    "    reg_loss = LAMBDA * (emb_users.norm().pow(2) +\n",
    "                        emb_pos_items.norm().pow(2) +\n",
    "                        emb_neg_items.norm().pow(2))\n",
    "\n",
    "    pos_ratings = torch.mul(emb_users_final, emb_pos_items_final).sum(dim=-1)\n",
    "    neg_ratings = torch.mul(emb_users_final, emb_neg_items_final).sum(dim=-1)\n",
    "\n",
    "    bpr_loss = torch.mean(torch.nn.functional.softplus(pos_ratings - neg_ratings))\n",
    "    # bpr_loss = torch.mean(torch.nn.functional.logsigmoid(pos_ratings - neg_ratings))\n",
    "\n",
    "    return -bpr_loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a2b6e",
   "metadata": {},
   "source": [
    "## 3.8. Store user-item interaction in graph-based structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "791c1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_items(edge_index):\n",
    "    user_items = dict()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_items:\n",
    "            user_items[user] = []\n",
    "        user_items[user].append(item)\n",
    "    return user_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70384cf",
   "metadata": {},
   "source": [
    "## 3.9. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41689eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(items_ground_truth, items_predicted):\n",
    "    num_correct_pred = np.sum(items_predicted, axis=1)\n",
    "    num_total_pred = np.array([len(items_ground_truth[i]) for i in range(len(items_ground_truth))])\n",
    "\n",
    "    recall = np.mean(num_correct_pred / num_total_pred)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3392c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(items_ground_truth, items_predicted):\n",
    "    test_matrix = np.zeros((len(items_predicted), K))\n",
    "\n",
    "    for i, items in enumerate(items_ground_truth):\n",
    "        length = min(len(items), K)\n",
    "        test_matrix[i, :length] = 1\n",
    "    \n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1. / np.log2(np.arange(2, K + 2)), axis=1)\n",
    "    dcg = items_predicted * (1. / np.log2(np.arange(2, K + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg / idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    \n",
    "    return np.mean(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "978175dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to get evaluation metrics\n",
    "def get_metrics(model, edge_index, exclude_edge_indices):\n",
    "\n",
    "    ratings = torch.matmul(model.emb_users.weight, model.emb_items.weight.T)\n",
    "\n",
    "    for exclude_edge_index in exclude_edge_indices:\n",
    "        user_pos_items = get_user_items(exclude_edge_index)\n",
    "        exclude_users = []\n",
    "        exclude_items = []\n",
    "        for user, items in user_pos_items.items():\n",
    "            exclude_users.extend([user] * len(items))\n",
    "            exclude_items.extend(items)\n",
    "        ratings[exclude_users, exclude_items] = -1024\n",
    "\n",
    "    # get the top k recommended items for each user\n",
    "    _, top_K_items = torch.topk(ratings, k=K)\n",
    "\n",
    "    # get all unique users in evaluated split\n",
    "    users = edge_index[0].unique()\n",
    "\n",
    "    test_user_pos_items = get_user_items(edge_index)\n",
    "\n",
    "    # convert test user pos items dictionary into a list\n",
    "    test_user_pos_items_list = [test_user_pos_items[user.item()] for user in users]\n",
    "\n",
    "    # determine the correctness of topk predictions\n",
    "    items_predicted = []\n",
    "    for user in users:\n",
    "        ground_truth_items = test_user_pos_items[user.item()]\n",
    "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
    "        items_predicted.append(label)\n",
    "\n",
    "    recall = compute_recall_at_k(test_user_pos_items_list, items_predicted)\n",
    "    ndcg = compute_ndcg_at_k(test_user_pos_items_list, items_predicted)\n",
    "\n",
    "    return recall, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f35948",
   "metadata": {},
   "source": [
    "## 3.10. Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b70aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to evaluate model\n",
    "def test(model, edge_index, exclude_edge_indices):\n",
    "    emb_users_final, emb_users, emb_items_final, emb_items = model.forward(edge_index)\n",
    "    user_indices, pos_item_indices, neg_item_indices = structured_negative_sampling(edge_index, contains_neg_self_loops=False)\n",
    "\n",
    "    emb_users_final, emb_users = emb_users_final[user_indices], emb_users[user_indices]\n",
    "\n",
    "    emb_pos_items_final, emb_pos_items = emb_items_final[pos_item_indices], emb_items[pos_item_indices]\n",
    "    emb_neg_items_final, emb_neg_items = emb_items_final[neg_item_indices], emb_items[neg_item_indices]\n",
    "\n",
    "    loss = bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items).item()\n",
    "\n",
    "    recall, ndcg = get_metrics(model, edge_index, exclude_edge_indices)\n",
    "\n",
    "    return loss, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9af3d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "LAMBDA = 1e-6\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ca5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LightGCN(num_users, num_items)\n",
    "model = model.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "val_edge_index = val_edge_index.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8018fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: -0.69337 | Val loss: -0.69254 | Val recall@20: 0.01055 | Val ndcg@20: 0.00513\n",
      "Epoch 5 | Train loss: -0.72862 | Val loss: -0.63759 | Val recall@20: 0.01857 | Val ndcg@20: 0.00833\n",
      "Epoch 10 | Train loss: -0.92624 | Val loss: -0.43197 | Val recall@20: 0.01857 | Val ndcg@20: 0.00845\n",
      "Epoch 15 | Train loss: -1.55088 | Val loss: -0.04304 | Val recall@20: 0.01815 | Val ndcg@20: 0.00861\n",
      "Epoch 20 | Train loss: -2.12727 | Val loss: 0.49891 | Val recall@20: 0.01750 | Val ndcg@20: 0.00839\n",
      "Epoch 25 | Train loss: -3.06941 | Val loss: 1.16192 | Val recall@20: 0.01826 | Val ndcg@20: 0.00866\n",
      "Epoch 30 | Train loss: -3.97275 | Val loss: 1.92815 | Val recall@20: 0.01813 | Val ndcg@20: 0.00864\n"
     ]
    }
   ],
   "source": [
    "n_batch = int(len(train_index)/BATCH_SIZE)\n",
    "\n",
    "for epoch in range(31):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb_users_final, emb_users, emb_items_final, emb_items = model.forward(train_edge_index)\n",
    "\n",
    "        user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(train_edge_index)\n",
    "        \n",
    "        emb_users_final, emb_users = emb_users_final[user_indices], emb_users[user_indices]\n",
    "        emb_pos_items_final, emb_pos_items = emb_items_final[pos_item_indices], emb_items[pos_item_indices]\n",
    "        emb_neg_items_final, emb_neg_items = emb_items_final[neg_item_indices], emb_items[neg_item_indices]\n",
    "\n",
    "        train_loss = bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items)\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss, recall, ndcg = test(model, val_edge_index, [train_edge_index])\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss.item():.5f} | Val loss: {val_loss:.5f} | Val recall@{K}: {recall:.5f} | Val ndcg@{K}: {ndcg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03aa38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.90374 | Test recall@20: 0.01745 | Test ndcg@20: 0.00859\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_recall, test_ndcg = test(model, test_edge_index.to(device), [train_edge_index, val_edge_index])\n",
    "\n",
    "print(f\"Test loss: {test_loss:.5f} | Test recall@{K}: {test_recall:.5f} | Test ndcg@{K}: {test_ndcg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb19b30",
   "metadata": {},
   "source": [
    "## 3.11. Recommend books for a particular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0840f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookid_title = pd.Series(books['Book-Title'].values, index=books.ISBN).to_dict()\n",
    "bookid_author = pd.Series(books['Book-Author'].values, index=books.ISBN).to_dict()\n",
    "user_pos_items = get_user_items(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9929b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recommend(user_id, num_recs):\n",
    "    user = user_mapping[user_id]\n",
    "    emb_user = model.emb_users.weight[user]\n",
    "    ratings = model.emb_items.weight @ emb_user\n",
    "\n",
    "    values, indices = torch.topk(ratings, k=100)\n",
    "\n",
    "    # Fetching favorite books\n",
    "    ids = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n",
    "    item_isbns = [list(item_mapping.keys())[list(item_mapping.values()).index(book)] for book in ids]\n",
    "    titles = [bookid_title[id] for id in item_isbns]\n",
    "    authors = [bookid_author[id] for id in item_isbns]\n",
    "\n",
    "    print(f'Favorite books from user n{user_id}:')\n",
    "    for i in range(len(item_isbns)):\n",
    "        print(f'- {titles[i]}, by {authors[i]}')\n",
    "\n",
    "    # Fetching recommended books\n",
    "    ids = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n",
    "    item_isbns = [list(item_mapping.keys())[list(item_mapping.values()).index(book)] for book in ids]\n",
    "    titles = [bookid_title[id] for id in item_isbns]\n",
    "    authors = [bookid_author[id] for id in item_isbns]\n",
    "\n",
    "    print(f'\\nRecommended books for user n{user_id}')\n",
    "    for i in range(num_recs):\n",
    "        print(f'- {titles[i]}, by {authors[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a157fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Favorite books from user n277427:\n",
      "- The Da Vinci Code, by Dan Brown\n",
      "- Into the Wild, by Jon Krakauer\n",
      "- One for the Money (Stephanie Plum Novels (Paperback)), by Janet Evanovich\n",
      "\n",
      "Recommended books for user n277427\n",
      "- The Lovely Bones: A Novel, by Alice Sebold\n",
      "- The Red Tent (Bestselling Backlist), by Anita Diamant\n",
      "- To Kill a Mockingbird, by Harper Lee\n",
      "- Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback)), by J. K. Rowling\n",
      "- Angels &amp; Demons, by Dan Brown\n"
     ]
    }
   ],
   "source": [
    "recommend(277427, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
