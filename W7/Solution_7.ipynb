{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3dcbfb-8bae-48de-94b2-288f1c0b132d",
   "metadata": {},
   "source": [
    "### Assignment 1. Given the CiteSeer graph in Pytorch Geometric as shown in the following:\n",
    "\n",
    "Questions:\n",
    "\n",
    "1) Show some graph statistics of the CiteSeer graph (Number of nodes, Number of edges, and node feature length, ...)\n",
    "2) Running the GAT model on Node classification on the CiteSeer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8d2753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff64279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Features shape: torch.Size([3327, 3703])\n",
      "Labels shape: torch.Size([3327])\n",
      "Adjacency matrix shape: torch.Size([3327, 3327])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract necessary data and transfer to GPU\n",
    "features = data.x.to(device)  # Node feature matrix\n",
    "labels = data.y.to(device)  # Node labels\n",
    "edge_index = data.edge_index.to(device)  # Edge indices\n",
    "\n",
    "# Convert edge index to adjacency matrix\n",
    "num_nodes = features.shape[0]\n",
    "adj = torch.zeros((num_nodes, num_nodes)).to(device)\n",
    "adj[edge_index[0], edge_index[1]] = 1\n",
    "\n",
    "# Add self-loops\n",
    "adj += torch.eye(num_nodes).to(device)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Adjacency matrix shape: {adj.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d55c7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset Information:\n",
      "-------------------\n",
      "Dataset: CiteSeer()\n",
      "Number of graphs: 1\n",
      "Number of nodes: 3327\n",
      "Number of edges: 9104\n",
      "Node feature length: 3703\n",
      "Number of classes: 6\n",
      "\n",
      "Graph Structure:\n",
      "-------------------\n",
      "Edges are directed: False\n",
      "Graph has isolated nodes: True\n",
      "Graph has self-loops: False\n",
      "Number of isolated nodes = 48\n",
      "\n",
      "Matrix Shapes:\n",
      "-------------------\n",
      "Features shape: torch.Size([3327, 3703])\n",
      "Labels shape: torch.Size([3327])\n",
      "Adjacency matrix shape: torch.Size([3327, 3327])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import remove_isolated_nodes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print('-------------------')\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Node feature length: {data.num_node_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "print('-------------------')\n",
    "print(f\"Edges are directed: {data.is_directed()}\")\n",
    "print(f\"Graph has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Graph has self-loops: {data.has_self_loops()}\")\n",
    "\n",
    "isolated = (remove_isolated_nodes(data.edge_index)[2] == False).sum(dim=0).item()\n",
    "print(f\"Number of isolated nodes = {isolated}\")\n",
    "\n",
    "features = data.x.to(device) \n",
    "labels = data.y.to(device)  \n",
    "edge_index = data.edge_index.to(device)  \n",
    "\n",
    "num_nodes = features.shape[0]\n",
    "adj = torch.zeros((num_nodes, num_nodes)).to(device)\n",
    "adj[edge_index[0], edge_index[1]] = 1\n",
    "\n",
    "adj += torch.eye(num_nodes).to(device)\n",
    "\n",
    "print(f\"\\nMatrix Shapes:\")\n",
    "print('-------------------')\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Adjacency matrix shape: {adj.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874f80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        # Learnable weight matrix\n",
    "        self.W = torch.nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        torch.nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        # Attention mechanism weight\n",
    "        self.a = torch.nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        torch.nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        # LeakyReLU non-linearity\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(self.alpha)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # Linear transformation\n",
    "        Wh = torch.mm(h, self.W)  # (N, out_features)\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Aggregate node features\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        # If `concat` is True, apply activation function (ELU)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        Wh1 = torch.mm(Wh, self.a[:self.out_features, :])  # (N, 1)\n",
    "        Wh2 = torch.mm(Wh, self.a[self.out_features:, :])  # (N, 1)\n",
    "        e = Wh1 + Wh2.T  # Broadcasting (N, N)\n",
    "        return self.leakyrelu(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eda3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layer 1: Multi-head attention (input -> hidden dimension)\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # Layer 2: Single-head attention (concatenated hidden -> output classes)\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)  # Concatenate multiple heads\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.out_att(x, adj)  # Output layer\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38a81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "nfeat = features.shape[1]  # Number of input features\n",
    "nhid = 4  # Number of hidden units per attention head\n",
    "nclass = dataset.num_classes  # Number of output classes\n",
    "dropout = 0.6  # Dropout rate\n",
    "alpha = 0.2  # Alpha for the LeakyReLU\n",
    "nheads = 4  # Number of attention heads\n",
    "\n",
    "# Initialize the GAT model\n",
    "model = GAT(nfeat=nfeat, nhid=nhid, nclass=nclass, dropout=dropout, alpha=alpha, nheads=nheads).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Mask to split data into train, validation, and test\n",
    "idx_train = data.train_mask\n",
    "idx_val = data.val_mask\n",
    "idx_test = data.test_mask\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(), acc_train\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss = F.nll_loss(output[mask], labels[mask])\n",
    "        acc = accuracy(output[mask], labels[mask])\n",
    "    return loss.item(), acc\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd28927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 2.0495, Train Acc: 0.1667, Val Loss: 1.7662, Val Acc: 0.2660\n",
      "Epoch: 002, Train Loss: 1.8903, Train Acc: 0.1667, Val Loss: 1.7288, Val Acc: 0.3620\n",
      "Epoch: 003, Train Loss: 1.9907, Train Acc: 0.2500, Val Loss: 1.6905, Val Acc: 0.4060\n",
      "Epoch: 004, Train Loss: 1.6940, Train Acc: 0.2417, Val Loss: 1.6555, Val Acc: 0.4380\n",
      "Epoch: 005, Train Loss: 1.6932, Train Acc: 0.2917, Val Loss: 1.6223, Val Acc: 0.4660\n",
      "Epoch: 006, Train Loss: 1.6772, Train Acc: 0.3167, Val Loss: 1.5904, Val Acc: 0.5080\n",
      "Epoch: 007, Train Loss: 1.5484, Train Acc: 0.4083, Val Loss: 1.5603, Val Acc: 0.5120\n",
      "Epoch: 008, Train Loss: 1.5660, Train Acc: 0.3417, Val Loss: 1.5313, Val Acc: 0.5400\n",
      "Epoch: 009, Train Loss: 1.5344, Train Acc: 0.4167, Val Loss: 1.5032, Val Acc: 0.5480\n",
      "Epoch: 010, Train Loss: 1.4870, Train Acc: 0.4250, Val Loss: 1.4785, Val Acc: 0.5480\n",
      "Epoch: 011, Train Loss: 1.3781, Train Acc: 0.4833, Val Loss: 1.4553, Val Acc: 0.5680\n",
      "Epoch: 012, Train Loss: 1.3912, Train Acc: 0.5000, Val Loss: 1.4332, Val Acc: 0.5900\n",
      "Epoch: 013, Train Loss: 1.2514, Train Acc: 0.5667, Val Loss: 1.4122, Val Acc: 0.6040\n",
      "Epoch: 014, Train Loss: 1.2613, Train Acc: 0.5500, Val Loss: 1.3913, Val Acc: 0.6080\n",
      "Epoch: 015, Train Loss: 1.3802, Train Acc: 0.4750, Val Loss: 1.3721, Val Acc: 0.6160\n",
      "Epoch: 016, Train Loss: 1.2757, Train Acc: 0.5333, Val Loss: 1.3546, Val Acc: 0.6400\n",
      "Epoch: 017, Train Loss: 1.2796, Train Acc: 0.5000, Val Loss: 1.3386, Val Acc: 0.6420\n",
      "Epoch: 018, Train Loss: 1.3044, Train Acc: 0.5000, Val Loss: 1.3235, Val Acc: 0.6400\n",
      "Epoch: 019, Train Loss: 1.2469, Train Acc: 0.5250, Val Loss: 1.3097, Val Acc: 0.6380\n",
      "Epoch: 020, Train Loss: 1.2484, Train Acc: 0.5333, Val Loss: 1.2971, Val Acc: 0.6460\n",
      "Epoch: 021, Train Loss: 1.1917, Train Acc: 0.5500, Val Loss: 1.2854, Val Acc: 0.6500\n",
      "Epoch: 022, Train Loss: 1.1637, Train Acc: 0.6083, Val Loss: 1.2737, Val Acc: 0.6460\n",
      "Epoch: 023, Train Loss: 1.1960, Train Acc: 0.6417, Val Loss: 1.2636, Val Acc: 0.6500\n",
      "Epoch: 024, Train Loss: 1.0748, Train Acc: 0.6250, Val Loss: 1.2535, Val Acc: 0.6580\n",
      "Epoch: 025, Train Loss: 1.1340, Train Acc: 0.6417, Val Loss: 1.2439, Val Acc: 0.6540\n",
      "Epoch: 026, Train Loss: 1.1947, Train Acc: 0.5750, Val Loss: 1.2346, Val Acc: 0.6620\n",
      "Epoch: 027, Train Loss: 1.1974, Train Acc: 0.5917, Val Loss: 1.2259, Val Acc: 0.6640\n",
      "Epoch: 028, Train Loss: 1.0894, Train Acc: 0.6333, Val Loss: 1.2182, Val Acc: 0.6600\n",
      "Epoch: 029, Train Loss: 1.0615, Train Acc: 0.6500, Val Loss: 1.2106, Val Acc: 0.6580\n",
      "Epoch: 030, Train Loss: 1.2243, Train Acc: 0.5833, Val Loss: 1.2030, Val Acc: 0.6660\n",
      "Epoch: 031, Train Loss: 1.0123, Train Acc: 0.6250, Val Loss: 1.1962, Val Acc: 0.6660\n",
      "Epoch: 032, Train Loss: 1.0901, Train Acc: 0.5750, Val Loss: 1.1892, Val Acc: 0.6640\n",
      "Epoch: 033, Train Loss: 1.0869, Train Acc: 0.5750, Val Loss: 1.1832, Val Acc: 0.6660\n",
      "Epoch: 034, Train Loss: 1.0414, Train Acc: 0.6250, Val Loss: 1.1775, Val Acc: 0.6680\n",
      "Epoch: 035, Train Loss: 0.9871, Train Acc: 0.6417, Val Loss: 1.1728, Val Acc: 0.6660\n",
      "Epoch: 036, Train Loss: 0.9842, Train Acc: 0.6917, Val Loss: 1.1674, Val Acc: 0.6660\n",
      "Epoch: 037, Train Loss: 0.9494, Train Acc: 0.6333, Val Loss: 1.1619, Val Acc: 0.6660\n",
      "Epoch: 038, Train Loss: 0.9758, Train Acc: 0.7083, Val Loss: 1.1566, Val Acc: 0.6680\n",
      "Epoch: 039, Train Loss: 0.9361, Train Acc: 0.6667, Val Loss: 1.1507, Val Acc: 0.6680\n",
      "Epoch: 040, Train Loss: 0.9556, Train Acc: 0.6667, Val Loss: 1.1444, Val Acc: 0.6700\n",
      "Epoch: 041, Train Loss: 0.9355, Train Acc: 0.7333, Val Loss: 1.1383, Val Acc: 0.6700\n",
      "Epoch: 042, Train Loss: 0.8908, Train Acc: 0.7000, Val Loss: 1.1327, Val Acc: 0.6720\n",
      "Epoch: 043, Train Loss: 0.8653, Train Acc: 0.7083, Val Loss: 1.1271, Val Acc: 0.6740\n",
      "Epoch: 044, Train Loss: 0.9760, Train Acc: 0.6667, Val Loss: 1.1225, Val Acc: 0.6700\n",
      "Epoch: 045, Train Loss: 0.9392, Train Acc: 0.6583, Val Loss: 1.1182, Val Acc: 0.6680\n",
      "Epoch: 046, Train Loss: 0.9330, Train Acc: 0.6667, Val Loss: 1.1139, Val Acc: 0.6680\n",
      "Epoch: 047, Train Loss: 0.8940, Train Acc: 0.6917, Val Loss: 1.1099, Val Acc: 0.6700\n",
      "Epoch: 048, Train Loss: 0.8206, Train Acc: 0.7250, Val Loss: 1.1062, Val Acc: 0.6680\n",
      "Epoch: 049, Train Loss: 0.9326, Train Acc: 0.6333, Val Loss: 1.1033, Val Acc: 0.6680\n",
      "Epoch: 050, Train Loss: 0.9100, Train Acc: 0.6833, Val Loss: 1.0999, Val Acc: 0.6680\n",
      "Epoch: 051, Train Loss: 0.8217, Train Acc: 0.6750, Val Loss: 1.0967, Val Acc: 0.6680\n",
      "Epoch: 052, Train Loss: 0.9068, Train Acc: 0.6500, Val Loss: 1.0944, Val Acc: 0.6640\n",
      "Epoch: 053, Train Loss: 0.9413, Train Acc: 0.6167, Val Loss: 1.0919, Val Acc: 0.6640\n",
      "Epoch: 054, Train Loss: 0.9021, Train Acc: 0.6583, Val Loss: 1.0895, Val Acc: 0.6660\n",
      "Epoch: 055, Train Loss: 0.8835, Train Acc: 0.6833, Val Loss: 1.0875, Val Acc: 0.6620\n",
      "Epoch: 056, Train Loss: 0.9059, Train Acc: 0.6583, Val Loss: 1.0859, Val Acc: 0.6660\n",
      "Epoch: 057, Train Loss: 0.8952, Train Acc: 0.6833, Val Loss: 1.0845, Val Acc: 0.6660\n",
      "Epoch: 058, Train Loss: 0.9696, Train Acc: 0.6250, Val Loss: 1.0839, Val Acc: 0.6680\n",
      "Epoch: 059, Train Loss: 0.8198, Train Acc: 0.7333, Val Loss: 1.0834, Val Acc: 0.6700\n",
      "Epoch: 060, Train Loss: 0.8743, Train Acc: 0.6500, Val Loss: 1.0834, Val Acc: 0.6700\n",
      "Epoch: 061, Train Loss: 0.8694, Train Acc: 0.6500, Val Loss: 1.0839, Val Acc: 0.6700\n",
      "Epoch: 062, Train Loss: 0.9521, Train Acc: 0.6833, Val Loss: 1.0846, Val Acc: 0.6720\n",
      "Epoch: 063, Train Loss: 0.9456, Train Acc: 0.6250, Val Loss: 1.0848, Val Acc: 0.6700\n",
      "Epoch: 064, Train Loss: 1.0397, Train Acc: 0.6083, Val Loss: 1.0852, Val Acc: 0.6680\n",
      "Epoch: 065, Train Loss: 0.8652, Train Acc: 0.7083, Val Loss: 1.0851, Val Acc: 0.6700\n",
      "Epoch: 066, Train Loss: 0.9452, Train Acc: 0.6167, Val Loss: 1.0847, Val Acc: 0.6720\n",
      "Epoch: 067, Train Loss: 0.9032, Train Acc: 0.6667, Val Loss: 1.0840, Val Acc: 0.6700\n",
      "Epoch: 068, Train Loss: 0.7499, Train Acc: 0.7333, Val Loss: 1.0830, Val Acc: 0.6720\n",
      "Epoch: 069, Train Loss: 0.8777, Train Acc: 0.6667, Val Loss: 1.0820, Val Acc: 0.6700\n",
      "Epoch: 070, Train Loss: 0.7869, Train Acc: 0.7167, Val Loss: 1.0805, Val Acc: 0.6700\n",
      "Epoch: 071, Train Loss: 0.8648, Train Acc: 0.7000, Val Loss: 1.0789, Val Acc: 0.6700\n",
      "Epoch: 072, Train Loss: 0.7909, Train Acc: 0.7000, Val Loss: 1.0771, Val Acc: 0.6640\n",
      "Epoch: 073, Train Loss: 0.8137, Train Acc: 0.7250, Val Loss: 1.0761, Val Acc: 0.6620\n",
      "Epoch: 074, Train Loss: 0.8098, Train Acc: 0.7083, Val Loss: 1.0752, Val Acc: 0.6600\n",
      "Epoch: 075, Train Loss: 0.7959, Train Acc: 0.7000, Val Loss: 1.0750, Val Acc: 0.6640\n",
      "Epoch: 076, Train Loss: 0.7629, Train Acc: 0.7333, Val Loss: 1.0746, Val Acc: 0.6600\n",
      "Epoch: 077, Train Loss: 0.7019, Train Acc: 0.7750, Val Loss: 1.0739, Val Acc: 0.6600\n",
      "Epoch: 078, Train Loss: 0.9241, Train Acc: 0.6917, Val Loss: 1.0725, Val Acc: 0.6620\n",
      "Epoch: 079, Train Loss: 0.8227, Train Acc: 0.7083, Val Loss: 1.0708, Val Acc: 0.6620\n",
      "Epoch: 080, Train Loss: 0.7933, Train Acc: 0.7167, Val Loss: 1.0691, Val Acc: 0.6600\n",
      "Epoch: 081, Train Loss: 0.8716, Train Acc: 0.6583, Val Loss: 1.0675, Val Acc: 0.6640\n",
      "Epoch: 082, Train Loss: 0.8004, Train Acc: 0.6917, Val Loss: 1.0665, Val Acc: 0.6640\n",
      "Epoch: 083, Train Loss: 0.8109, Train Acc: 0.7167, Val Loss: 1.0653, Val Acc: 0.6640\n",
      "Epoch: 084, Train Loss: 0.8478, Train Acc: 0.6833, Val Loss: 1.0647, Val Acc: 0.6640\n",
      "Epoch: 085, Train Loss: 0.8704, Train Acc: 0.6750, Val Loss: 1.0623, Val Acc: 0.6640\n",
      "Epoch: 086, Train Loss: 0.7179, Train Acc: 0.7417, Val Loss: 1.0601, Val Acc: 0.6660\n",
      "Epoch: 087, Train Loss: 0.8111, Train Acc: 0.6917, Val Loss: 1.0583, Val Acc: 0.6660\n",
      "Epoch: 088, Train Loss: 0.8388, Train Acc: 0.6333, Val Loss: 1.0572, Val Acc: 0.6700\n",
      "Epoch: 089, Train Loss: 0.8585, Train Acc: 0.6583, Val Loss: 1.0560, Val Acc: 0.6720\n",
      "Epoch: 090, Train Loss: 0.8430, Train Acc: 0.6917, Val Loss: 1.0553, Val Acc: 0.6740\n",
      "Epoch: 091, Train Loss: 0.8228, Train Acc: 0.6417, Val Loss: 1.0553, Val Acc: 0.6720\n",
      "Epoch: 092, Train Loss: 0.8061, Train Acc: 0.7500, Val Loss: 1.0547, Val Acc: 0.6720\n",
      "Epoch: 093, Train Loss: 0.6964, Train Acc: 0.7333, Val Loss: 1.0543, Val Acc: 0.6720\n",
      "Epoch: 094, Train Loss: 0.7495, Train Acc: 0.7250, Val Loss: 1.0534, Val Acc: 0.6720\n",
      "Epoch: 095, Train Loss: 0.7510, Train Acc: 0.7250, Val Loss: 1.0529, Val Acc: 0.6720\n",
      "Epoch: 096, Train Loss: 0.6699, Train Acc: 0.8000, Val Loss: 1.0532, Val Acc: 0.6720\n",
      "Epoch: 097, Train Loss: 0.7979, Train Acc: 0.6583, Val Loss: 1.0540, Val Acc: 0.6660\n",
      "Epoch: 098, Train Loss: 0.8962, Train Acc: 0.6667, Val Loss: 1.0561, Val Acc: 0.6680\n",
      "Epoch: 099, Train Loss: 0.8249, Train Acc: 0.7083, Val Loss: 1.0589, Val Acc: 0.6700\n",
      "Epoch: 100, Train Loss: 0.7198, Train Acc: 0.7583, Val Loss: 1.0614, Val Acc: 0.6700\n",
      "Test Loss: 1.0506, Test Accuracy: 0.6570\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_losses, train_accuracies = [], []  # Initialize lists to store training metrics\n",
    "val_losses, val_accuracies = [], []  # Initialize lists to store validation metrics\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train()\n",
    "    val_loss, val_acc = evaluate(idx_val)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1:03d}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_loss, test_acc = evaluate(idx_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3bddb-c71c-44f5-a1f0-40671ea155ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Assignment 2. Load the  CiteSeer dataset from Torch Geometric and then do the node classification task by using the GATv2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b150809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GATv2Layer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, heads=1, concat=True):\n",
    "        super(GATv2Layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear transformation for node features\n",
    "        self.linear = nn.Linear(in_features, out_features * heads, bias=False)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Parameter(torch.Tensor(1, heads, 2 * out_features))\n",
    "        nn.init.xavier_uniform_(self.attention.data, gain=1.414)\n",
    "\n",
    "        # LeakyReLU for attention coefficients\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # Apply linear transformation: (N, in_features) -> (N, heads * out_features)\n",
    "        Wh = self.linear(h)\n",
    "        Wh = Wh.reshape(-1, self.heads, self.out_features)  # Using reshape instead of view\n",
    "\n",
    "        # Create attention scores for self and neighbors\n",
    "        Wh_i = Wh.unsqueeze(1).repeat(1, adj.size(1), 1, 1)  # Shape: (N, N, heads, out_features)\n",
    "        Wh_j = Wh.unsqueeze(0).repeat(adj.size(0), 1, 1, 1)  # Shape: (N, N, heads, out_features)\n",
    "\n",
    "        # Concatenate Wh_i and Wh_j for attention calculation\n",
    "        a_input = torch.cat([Wh_i, Wh_j], dim=-1)  # Shape: (N, N, heads, 2 * out_features)\n",
    "\n",
    "        # Compute attention scores using the learned parameter and LeakyReLU\n",
    "        e = self.leakyrelu((a_input * self.attention).sum(dim=-1))  # Shape: (N, N, heads)\n",
    "\n",
    "        # Masking: attention scores for unconnected nodes should be -infinity\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj.unsqueeze(-1) > 0, e, zero_vec)  # Shape: (N, N, heads)\n",
    "\n",
    "        # Softmax along neighbors\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "\n",
    "        # Compute the attention-weighted sum of node features\n",
    "        h_prime = torch.einsum('ijh,jhf->ihf', attention, Wh)  # Shape: (N, heads, out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            return h_prime.reshape(-1, self.heads * self.out_features)  # Use reshape instead of view\n",
    "        else:\n",
    "            return h_prime.mean(dim=1)  # Average heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7739e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATv2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, heads=1):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.gat1 = GATv2Layer(nfeat, nhid, heads=heads, concat=True)\n",
    "        self.gat2 = GATv2Layer(nhid * heads, nclass, heads=1, concat=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.gat1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Model and optimizer\n",
    "model = GATv2(nfeat=features.shape[1], nhid=4, nclass=int(labels.max().item()) + 1, dropout=0.6, heads=4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d442d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss = F.nll_loss(output[data.train_mask], labels[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct = preds[data.val_mask].eq(labels[data.val_mask]).sum().item()\n",
    "        return correct / data.val_mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46d214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.7923 | Val Accuracy: 0.5120\n",
      "Epoch 002 | Loss: 1.7324 | Val Accuracy: 0.6360\n",
      "Epoch 003 | Loss: 1.6561 | Val Accuracy: 0.6540\n",
      "Epoch 004 | Loss: 1.5935 | Val Accuracy: 0.6700\n",
      "Epoch 005 | Loss: 1.5264 | Val Accuracy: 0.6700\n",
      "Epoch 006 | Loss: 1.4514 | Val Accuracy: 0.6740\n",
      "Epoch 007 | Loss: 1.3754 | Val Accuracy: 0.6800\n",
      "Epoch 008 | Loss: 1.3063 | Val Accuracy: 0.6780\n",
      "Epoch 009 | Loss: 1.2154 | Val Accuracy: 0.6800\n",
      "Epoch 010 | Loss: 1.1376 | Val Accuracy: 0.6820\n",
      "Epoch 011 | Loss: 1.0736 | Val Accuracy: 0.6780\n",
      "Epoch 012 | Loss: 1.0486 | Val Accuracy: 0.6800\n",
      "Epoch 013 | Loss: 0.9495 | Val Accuracy: 0.6780\n",
      "Epoch 014 | Loss: 0.8877 | Val Accuracy: 0.6780\n",
      "Epoch 015 | Loss: 0.7989 | Val Accuracy: 0.6760\n",
      "Epoch 016 | Loss: 0.7576 | Val Accuracy: 0.6720\n",
      "Epoch 017 | Loss: 0.7031 | Val Accuracy: 0.6740\n",
      "Epoch 018 | Loss: 0.6740 | Val Accuracy: 0.6740\n",
      "Epoch 019 | Loss: 0.6077 | Val Accuracy: 0.6780\n",
      "Epoch 020 | Loss: 0.5738 | Val Accuracy: 0.6760\n",
      "Epoch 021 | Loss: 0.5351 | Val Accuracy: 0.6760\n",
      "Epoch 022 | Loss: 0.5114 | Val Accuracy: 0.6760\n",
      "Epoch 023 | Loss: 0.4456 | Val Accuracy: 0.6740\n",
      "Epoch 024 | Loss: 0.4684 | Val Accuracy: 0.6760\n",
      "Epoch 025 | Loss: 0.4539 | Val Accuracy: 0.6740\n",
      "Epoch 026 | Loss: 0.3575 | Val Accuracy: 0.6720\n",
      "Epoch 027 | Loss: 0.3658 | Val Accuracy: 0.6740\n",
      "Epoch 028 | Loss: 0.3377 | Val Accuracy: 0.6760\n",
      "Epoch 029 | Loss: 0.3157 | Val Accuracy: 0.6800\n",
      "Epoch 030 | Loss: 0.2694 | Val Accuracy: 0.6780\n",
      "Epoch 031 | Loss: 0.3188 | Val Accuracy: 0.6760\n",
      "Epoch 032 | Loss: 0.2956 | Val Accuracy: 0.6720\n",
      "Epoch 033 | Loss: 0.2619 | Val Accuracy: 0.6740\n",
      "Epoch 034 | Loss: 0.2628 | Val Accuracy: 0.6740\n",
      "Epoch 035 | Loss: 0.2225 | Val Accuracy: 0.6760\n",
      "Epoch 036 | Loss: 0.2012 | Val Accuracy: 0.6740\n",
      "Epoch 037 | Loss: 0.2039 | Val Accuracy: 0.6740\n",
      "Epoch 038 | Loss: 0.2513 | Val Accuracy: 0.6740\n",
      "Epoch 039 | Loss: 0.2074 | Val Accuracy: 0.6740\n",
      "Epoch 040 | Loss: 0.1919 | Val Accuracy: 0.6760\n",
      "Epoch 041 | Loss: 0.2103 | Val Accuracy: 0.6760\n",
      "Epoch 042 | Loss: 0.2195 | Val Accuracy: 0.6760\n",
      "Epoch 043 | Loss: 0.1528 | Val Accuracy: 0.6760\n",
      "Epoch 044 | Loss: 0.1648 | Val Accuracy: 0.6760\n",
      "Epoch 045 | Loss: 0.1560 | Val Accuracy: 0.6800\n",
      "Epoch 046 | Loss: 0.1546 | Val Accuracy: 0.6800\n",
      "Epoch 047 | Loss: 0.1465 | Val Accuracy: 0.6780\n",
      "Epoch 048 | Loss: 0.1244 | Val Accuracy: 0.6820\n",
      "Epoch 049 | Loss: 0.1534 | Val Accuracy: 0.6860\n",
      "Epoch 050 | Loss: 0.1524 | Val Accuracy: 0.6860\n",
      "Epoch 051 | Loss: 0.1313 | Val Accuracy: 0.6860\n",
      "Epoch 052 | Loss: 0.1202 | Val Accuracy: 0.6880\n",
      "Epoch 053 | Loss: 0.1442 | Val Accuracy: 0.6860\n",
      "Epoch 054 | Loss: 0.1080 | Val Accuracy: 0.6900\n",
      "Epoch 055 | Loss: 0.1062 | Val Accuracy: 0.6860\n",
      "Epoch 056 | Loss: 0.1352 | Val Accuracy: 0.6880\n",
      "Epoch 057 | Loss: 0.1425 | Val Accuracy: 0.6880\n",
      "Epoch 058 | Loss: 0.1075 | Val Accuracy: 0.6860\n",
      "Epoch 059 | Loss: 0.1608 | Val Accuracy: 0.6860\n",
      "Epoch 060 | Loss: 0.1213 | Val Accuracy: 0.6860\n",
      "Epoch 061 | Loss: 0.1004 | Val Accuracy: 0.6880\n",
      "Epoch 062 | Loss: 0.1080 | Val Accuracy: 0.6880\n",
      "Epoch 063 | Loss: 0.0886 | Val Accuracy: 0.6860\n",
      "Epoch 064 | Loss: 0.1482 | Val Accuracy: 0.6860\n",
      "Epoch 065 | Loss: 0.1123 | Val Accuracy: 0.6840\n",
      "Epoch 066 | Loss: 0.0998 | Val Accuracy: 0.6800\n",
      "Epoch 067 | Loss: 0.0729 | Val Accuracy: 0.6760\n",
      "Epoch 068 | Loss: 0.1191 | Val Accuracy: 0.6760\n",
      "Epoch 069 | Loss: 0.1160 | Val Accuracy: 0.6740\n",
      "Epoch 070 | Loss: 0.0962 | Val Accuracy: 0.6740\n",
      "Epoch 071 | Loss: 0.0918 | Val Accuracy: 0.6760\n",
      "Epoch 072 | Loss: 0.1037 | Val Accuracy: 0.6760\n",
      "Epoch 073 | Loss: 0.0715 | Val Accuracy: 0.6800\n",
      "Epoch 074 | Loss: 0.0766 | Val Accuracy: 0.6800\n",
      "Epoch 075 | Loss: 0.0765 | Val Accuracy: 0.6760\n",
      "Epoch 076 | Loss: 0.0896 | Val Accuracy: 0.6760\n",
      "Epoch 077 | Loss: 0.0743 | Val Accuracy: 0.6760\n",
      "Epoch 078 | Loss: 0.1153 | Val Accuracy: 0.6760\n",
      "Epoch 079 | Loss: 0.0850 | Val Accuracy: 0.6780\n",
      "Epoch 080 | Loss: 0.0699 | Val Accuracy: 0.6780\n",
      "Epoch 081 | Loss: 0.0796 | Val Accuracy: 0.6780\n",
      "Epoch 082 | Loss: 0.0924 | Val Accuracy: 0.6760\n",
      "Epoch 083 | Loss: 0.0771 | Val Accuracy: 0.6780\n",
      "Epoch 084 | Loss: 0.1135 | Val Accuracy: 0.6780\n",
      "Epoch 085 | Loss: 0.0951 | Val Accuracy: 0.6780\n",
      "Epoch 086 | Loss: 0.0890 | Val Accuracy: 0.6760\n",
      "Epoch 087 | Loss: 0.0798 | Val Accuracy: 0.6780\n",
      "Epoch 088 | Loss: 0.0606 | Val Accuracy: 0.6780\n",
      "Epoch 089 | Loss: 0.0547 | Val Accuracy: 0.6780\n",
      "Epoch 090 | Loss: 0.0781 | Val Accuracy: 0.6780\n",
      "Epoch 091 | Loss: 0.0766 | Val Accuracy: 0.6780\n",
      "Epoch 092 | Loss: 0.0738 | Val Accuracy: 0.6780\n",
      "Epoch 093 | Loss: 0.0758 | Val Accuracy: 0.6780\n",
      "Epoch 094 | Loss: 0.0942 | Val Accuracy: 0.6800\n",
      "Epoch 095 | Loss: 0.0966 | Val Accuracy: 0.6820\n",
      "Epoch 096 | Loss: 0.0796 | Val Accuracy: 0.6840\n",
      "Epoch 097 | Loss: 0.0739 | Val Accuracy: 0.6820\n",
      "Epoch 098 | Loss: 0.0679 | Val Accuracy: 0.6820\n",
      "Epoch 099 | Loss: 0.0573 | Val Accuracy: 0.6820\n",
      "Epoch 100 | Loss: 0.0617 | Val Accuracy: 0.6820\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    acc = evaluate()\n",
    "    train_loss.append(loss)\n",
    "    val_acc.append(acc)\n",
    "\n",
    "    # Print loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1:03d} | Loss: {loss:.4f} | Val Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GM_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
